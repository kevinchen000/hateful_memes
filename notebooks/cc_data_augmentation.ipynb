{"cells":[{"cell_type":"markdown","source":["# Prerequisites\n","\n","The following steps must be satisfied before running the rest of the notebook:\n","\n","- Run the `Augment_Data` notebook in order to generate the training files and images that are referred to throughout the notebook.\n","- Make changes to `hm_convert` to read the added files/folders when running `build_dataset`.\n","\n","Throughout the notebook, directory paths are used for references that depend on your local setup. There are comments that describe what each path should be pointing to so that should help set up the notebook to work for your hierarchy."],"metadata":{"id":"D_efaOP0V_8K"}},{"cell_type":"markdown","source":["# Setup"],"metadata":{"id":"Ccw-s21WV8kW"}},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3177,"status":"ok","timestamp":1714081415216,"user":{"displayName":"Patrick Prestridge","userId":"01452612983786479124"},"user_tz":420},"id":"oEAyXvI-d4UN","outputId":"5739da31-d613-46d6-c2d7-2aedd9cb3bc9"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["### Mount\n","from google.colab import drive\n","drive.mount('/content/drive', force_remount=True)"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"29fJJ1UEhi9D","outputId":"8b50bd9d-4949-4501-d53a-88eec2080a92","executionInfo":{"status":"ok","timestamp":1714081432763,"user_tz":420,"elapsed":17550,"user":{"displayName":"Patrick Prestridge","userId":"01452612983786479124"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/mmf-main\n","Obtaining file:///content/drive/MyDrive/mmf-main\n","  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Checking if build backend supports build_editable ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build editable ... \u001b[?25l\u001b[?25hdone\n","  Preparing editable metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: datasets<=2.18.0 in /usr/local/lib/python3.10/dist-packages (from mmf==1.0.0rc12) (2.18.0)\n","Requirement already satisfied: iopath==0.1.8 in /usr/local/lib/python3.10/dist-packages (from mmf==1.0.0rc12) (0.1.8)\n","Requirement already satisfied: editdistance==0.5.3 in /usr/local/lib/python3.10/dist-packages (from mmf==1.0.0rc12) (0.5.3)\n","Requirement already satisfied: fasttext==0.9.1 in /usr/local/lib/python3.10/dist-packages (from mmf==1.0.0rc12) (0.9.1)\n","Requirement already satisfied: ftfy==5.8 in /usr/local/lib/python3.10/dist-packages (from mmf==1.0.0rc12) (5.8)\n","Requirement already satisfied: GitPython==3.1.30 in /usr/local/lib/python3.10/dist-packages (from mmf==1.0.0rc12) (3.1.30)\n","Requirement already satisfied: lmdb==0.98 in /usr/local/lib/python3.10/dist-packages (from mmf==1.0.0rc12) (0.98)\n","Requirement already satisfied: omegaconf<=2.3,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from mmf==1.0.0rc12) (2.3.0)\n","Requirement already satisfied: matplotlib==3.8.0 in /usr/local/lib/python3.10/dist-packages (from mmf==1.0.0rc12) (3.8.0)\n","Requirement already satisfied: nltk==3.6.6 in /usr/local/lib/python3.10/dist-packages (from mmf==1.0.0rc12) (3.6.6)\n","Requirement already satisfied: numpy==1.26.4 in /usr/local/lib/python3.10/dist-packages (from mmf==1.0.0rc12) (1.26.4)\n","Requirement already satisfied: pillow==9.3.0 in /usr/local/lib/python3.10/dist-packages (from mmf==1.0.0rc12) (9.3.0)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from mmf==1.0.0rc12) (5.9.5)\n","Requirement already satisfied: pycocotools<=2.0.7 in /usr/local/lib/python3.10/dist-packages (from mmf==1.0.0rc12) (2.0.7)\n","Requirement already satisfied: pytorch-lightning<=1.6.0 in /usr/local/lib/python3.10/dist-packages (from mmf==1.0.0rc12) (1.6.0)\n","Requirement already satisfied: requests==2.23.0 in /usr/local/lib/python3.10/dist-packages (from mmf==1.0.0rc12) (2.23.0)\n","Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (from mmf==1.0.0rc12) (0.1.99)\n","Requirement already satisfied: sklearn==0.0 in /usr/local/lib/python3.10/dist-packages (from mmf==1.0.0rc12) (0.0)\n","Requirement already satisfied: termcolor==1.1.0 in /usr/local/lib/python3.10/dist-packages (from mmf==1.0.0rc12) (1.1.0)\n","Requirement already satisfied: torchaudio<=2.2.0 in /usr/local/lib/python3.10/dist-packages (from mmf==1.0.0rc12) (2.2.0)\n","Requirement already satisfied: torchtext<=0.17.0 in /usr/local/lib/python3.10/dist-packages (from mmf==1.0.0rc12) (0.17.0)\n","Requirement already satisfied: torchvision<=0.17.0 in /usr/local/lib/python3.10/dist-packages (from mmf==1.0.0rc12) (0.17.0)\n","Requirement already satisfied: tqdm>=4.43.0 in /usr/local/lib/python3.10/dist-packages (from mmf==1.0.0rc12) (4.66.2)\n","Requirement already satisfied: transformers<=4.39.2 in /usr/local/lib/python3.10/dist-packages (from mmf==1.0.0rc12) (4.39.2)\n","Requirement already satisfied: pybind11>=2.2 in /usr/local/lib/python3.10/dist-packages (from fasttext==0.9.1->mmf==1.0.0rc12) (2.12.0)\n","Requirement already satisfied: setuptools>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from fasttext==0.9.1->mmf==1.0.0rc12) (67.7.2)\n","Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from ftfy==5.8->mmf==1.0.0rc12) (0.2.13)\n","Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from GitPython==3.1.30->mmf==1.0.0rc12) (4.0.11)\n","Requirement already satisfied: portalocker in /usr/local/lib/python3.10/dist-packages (from iopath==0.1.8->mmf==1.0.0rc12) (2.8.2)\n","Requirement already satisfied: cffi>=0.8 in /usr/local/lib/python3.10/dist-packages (from lmdb==0.98->mmf==1.0.0rc12) (1.16.0)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib==3.8.0->mmf==1.0.0rc12) (1.2.1)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib==3.8.0->mmf==1.0.0rc12) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib==3.8.0->mmf==1.0.0rc12) (4.51.0)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib==3.8.0->mmf==1.0.0rc12) (1.4.5)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib==3.8.0->mmf==1.0.0rc12) (24.0)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib==3.8.0->mmf==1.0.0rc12) (3.1.2)\n","Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib==3.8.0->mmf==1.0.0rc12) (2.8.2)\n","Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk==3.6.6->mmf==1.0.0rc12) (8.1.7)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk==3.6.6->mmf==1.0.0rc12) (1.4.0)\n","Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk==3.6.6->mmf==1.0.0rc12) (2023.12.25)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from requests==2.23.0->mmf==1.0.0rc12) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests==2.23.0->mmf==1.0.0rc12) (2.10)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests==2.23.0->mmf==1.0.0rc12) (1.25.11)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests==2.23.0->mmf==1.0.0rc12) (2024.2.2)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sklearn==0.0->mmf==1.0.0rc12) (1.2.2)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets<=2.18.0->mmf==1.0.0rc12) (3.13.4)\n","Requirement already satisfied: pyarrow>=12.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets<=2.18.0->mmf==1.0.0rc12) (14.0.2)\n","Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets<=2.18.0->mmf==1.0.0rc12) (0.6)\n","Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets<=2.18.0->mmf==1.0.0rc12) (0.3.8)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets<=2.18.0->mmf==1.0.0rc12) (2.0.3)\n","Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets<=2.18.0->mmf==1.0.0rc12) (3.4.1)\n","Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets<=2.18.0->mmf==1.0.0rc12) (0.70.16)\n","Requirement already satisfied: fsspec[http]<=2024.2.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets<=2.18.0->mmf==1.0.0rc12) (2023.6.0)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets<=2.18.0->mmf==1.0.0rc12) (3.9.5)\n","Requirement already satisfied: huggingface-hub>=0.19.4 in /usr/local/lib/python3.10/dist-packages (from datasets<=2.18.0->mmf==1.0.0rc12) (0.20.3)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets<=2.18.0->mmf==1.0.0rc12) (6.0.1)\n","Requirement already satisfied: antlr4-python3-runtime==4.9.* in /usr/local/lib/python3.10/dist-packages (from omegaconf<=2.3,>=2.0.6->mmf==1.0.0rc12) (4.9.3)\n","Requirement already satisfied: torch>=1.8.* in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning<=1.6.0->mmf==1.0.0rc12) (2.2.0)\n","Requirement already satisfied: tensorboard>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning<=1.6.0->mmf==1.0.0rc12) (2.15.2)\n","Requirement already satisfied: torchmetrics>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning<=1.6.0->mmf==1.0.0rc12) (1.3.2)\n","Requirement already satisfied: pyDeprecate<0.4.0,>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning<=1.6.0->mmf==1.0.0rc12) (0.3.2)\n","Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning<=1.6.0->mmf==1.0.0rc12) (4.11.0)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.*->pytorch-lightning<=1.6.0->mmf==1.0.0rc12) (1.12)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.*->pytorch-lightning<=1.6.0->mmf==1.0.0rc12) (3.3)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.*->pytorch-lightning<=1.6.0->mmf==1.0.0rc12) (3.1.3)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.*->pytorch-lightning<=1.6.0->mmf==1.0.0rc12) (12.1.105)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.*->pytorch-lightning<=1.6.0->mmf==1.0.0rc12) (12.1.105)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.*->pytorch-lightning<=1.6.0->mmf==1.0.0rc12) (12.1.105)\n","Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.*->pytorch-lightning<=1.6.0->mmf==1.0.0rc12) (8.9.2.26)\n","Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.*->pytorch-lightning<=1.6.0->mmf==1.0.0rc12) (12.1.3.1)\n","Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.*->pytorch-lightning<=1.6.0->mmf==1.0.0rc12) (11.0.2.54)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.*->pytorch-lightning<=1.6.0->mmf==1.0.0rc12) (10.3.2.106)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.*->pytorch-lightning<=1.6.0->mmf==1.0.0rc12) (11.4.5.107)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.*->pytorch-lightning<=1.6.0->mmf==1.0.0rc12) (12.1.0.106)\n","Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.*->pytorch-lightning<=1.6.0->mmf==1.0.0rc12) (2.19.3)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.*->pytorch-lightning<=1.6.0->mmf==1.0.0rc12) (12.1.105)\n","Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.*->pytorch-lightning<=1.6.0->mmf==1.0.0rc12) (2.2.0)\n","Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.8.*->pytorch-lightning<=1.6.0->mmf==1.0.0rc12) (12.4.127)\n","Requirement already satisfied: torchdata==0.7.1 in /usr/local/lib/python3.10/dist-packages (from torchtext<=0.17.0->mmf==1.0.0rc12) (0.7.1)\n","Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers<=4.39.2->mmf==1.0.0rc12) (0.15.2)\n","Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers<=4.39.2->mmf==1.0.0rc12) (0.4.3)\n","Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=0.8->lmdb==0.98->mmf==1.0.0rc12) (2.22)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets<=2.18.0->mmf==1.0.0rc12) (1.3.1)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets<=2.18.0->mmf==1.0.0rc12) (23.2.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets<=2.18.0->mmf==1.0.0rc12) (1.4.1)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets<=2.18.0->mmf==1.0.0rc12) (6.0.5)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets<=2.18.0->mmf==1.0.0rc12) (1.9.4)\n","Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets<=2.18.0->mmf==1.0.0rc12) (4.0.3)\n","Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from gitdb<5,>=4.0.1->GitPython==3.1.30->mmf==1.0.0rc12) (5.0.1)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib==3.8.0->mmf==1.0.0rc12) (1.16.0)\n","Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.2.0->pytorch-lightning<=1.6.0->mmf==1.0.0rc12) (1.4.0)\n","Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.2.0->pytorch-lightning<=1.6.0->mmf==1.0.0rc12) (1.62.2)\n","Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.2.0->pytorch-lightning<=1.6.0->mmf==1.0.0rc12) (2.27.0)\n","Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.2.0->pytorch-lightning<=1.6.0->mmf==1.0.0rc12) (1.2.0)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.2.0->pytorch-lightning<=1.6.0->mmf==1.0.0rc12) (3.6)\n","Requirement already satisfied: protobuf!=4.24.0,>=3.19.6 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.2.0->pytorch-lightning<=1.6.0->mmf==1.0.0rc12) (3.20.3)\n","Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.2.0->pytorch-lightning<=1.6.0->mmf==1.0.0rc12) (0.7.2)\n","Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.2.0->pytorch-lightning<=1.6.0->mmf==1.0.0rc12) (3.0.2)\n","Requirement already satisfied: lightning-utilities>=0.8.0 in /usr/local/lib/python3.10/dist-packages (from torchmetrics>=0.4.1->pytorch-lightning<=1.6.0->mmf==1.0.0rc12) (0.11.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets<=2.18.0->mmf==1.0.0rc12) (2023.4)\n","Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets<=2.18.0->mmf==1.0.0rc12) (2024.1)\n","Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sklearn==0.0->mmf==1.0.0rc12) (1.11.4)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sklearn==0.0->mmf==1.0.0rc12) (3.4.0)\n","Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning<=1.6.0->mmf==1.0.0rc12) (5.3.3)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning<=1.6.0->mmf==1.0.0rc12) (0.4.0)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning<=1.6.0->mmf==1.0.0rc12) (4.9)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard>=2.2.0->pytorch-lightning<=1.6.0->mmf==1.0.0rc12) (1.3.1)\n","Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard>=2.2.0->pytorch-lightning<=1.6.0->mmf==1.0.0rc12) (2.1.5)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.8.*->pytorch-lightning<=1.6.0->mmf==1.0.0rc12) (1.3.0)\n","Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning<=1.6.0->mmf==1.0.0rc12) (0.6.0)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard>=2.2.0->pytorch-lightning<=1.6.0->mmf==1.0.0rc12) (3.2.2)\n","Building wheels for collected packages: mmf\n","  Building editable for mmf (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for mmf: filename=mmf-1.0.0rc12-0.editable-cp310-cp310-linux_x86_64.whl size=10743 sha256=e83c4fc145a61e52a89dab1f3d1bece658b4e007be246b6b0b9938e50a2fab93\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-zs1ovasu/wheels/e3/9f/fc/d49548515b10b7b3c02969460b19f70eb734152532b702074e\n","Successfully built mmf\n","Installing collected packages: mmf\n","  Attempting uninstall: mmf\n","    Found existing installation: mmf 1.0.0rc12\n","    Uninstalling mmf-1.0.0rc12:\n","      Successfully uninstalled mmf-1.0.0rc12\n","Successfully installed mmf-1.0.0rc12\n"]}],"source":["# local mmf installation\n","%cd '/content/drive/MyDrive/mmf-main'\n","!pip install -e ."]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":291305,"status":"ok","timestamp":1714081724059,"user":{"displayName":"Patrick Prestridge","userId":"01452612983786479124"},"user_tz":420},"id":"AQxnn1Tvhv9y","outputId":"44aad063-ddc6-46fe-fba4-de29b44ad7e8"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/transformers/deepspeed.py:23: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations\n","  warnings.warn(\n"]}],"source":["### Import General Modules, 3min\n","from mmf.common.registry import registry\n","from mmf.models.mmbt import MMBT\n","from mmf.utils.build import build_dataset\n","from mmf.utils.env import setup_imports\n","import matplotlib.pyplot as plt\n","from PIL import Image\n","import torch\n","\n","### Import Modules for Fusion Model\n","# All model using MMF need to inherit BaseModel\n","from mmf.models.base_model import BaseModel\n","# Builder methods for image encoder and classifier\n","from mmf.utils.build import build_classifier_layer, build_image_encoder,build_text_encoder\n","\n","setup_imports()\n","\n","# !mmf_convert_hm --zip_file=\"/content/drive/MyDrive/hateful-memes/hateful_memes.zip\" --bypass_checksum=1\n","# dataset = build_dataset(\"hateful_memes\")"]},{"cell_type":"code","source":["!ls ../hateful_memes/datasets"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gJWUXnvMPdes","executionInfo":{"status":"ok","timestamp":1714082715810,"user_tz":420,"elapsed":2,"user":{"displayName":"Patrick Prestridge","userId":"01452612983786479124"}},"outputId":"ae98998b-a1b1-47af-9fcd-e2134d0d71ac"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["coco  gqa  hateful_memes.zip  mmimdb  vizwiz\n"]}]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ffpJ8n9aiMDA","executionInfo":{"status":"ok","timestamp":1714082714905,"user_tz":420,"elapsed":819268,"user":{"displayName":"Patrick Prestridge","userId":"01452612983786479124"}},"outputId":"1789de2b-a6d5-4ee7-bc2e-83e8a40a1266"},"outputs":[{"output_type":"stream","name":"stdout","text":["2024-04-25 21:51:38.115248: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","2024-04-25 21:51:38.115299: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","2024-04-25 21:51:38.116894: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","2024-04-25 21:51:39.165749: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","2024-04-25 21:51:43.397195: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:47] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n","/usr/local/lib/python3.10/dist-packages/transformers/deepspeed.py:23: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/omegaconf/grammar_visitor.py:253: UserWarning: In the sequence `MMF_LOG_DIR,` some elements are missing: please replace them with empty quoted strings. See https://github.com/omry/omegaconf/issues/572 for details.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/omegaconf/grammar_visitor.py:253: UserWarning: In the sequence `MMF_REPORT_DIR,` some elements are missing: please replace them with empty quoted strings. See https://github.com/omry/omegaconf/issues/572 for details.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/omegaconf/grammar_visitor.py:253: UserWarning: In the sequence `MMF_TENSORBOARD_LOGDIR,` some elements are missing: please replace them with empty quoted strings. See https://github.com/omry/omegaconf/issues/572 for details.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/omegaconf/grammar_visitor.py:253: UserWarning: In the sequence `MMF_WANDB_LOGDIR,` some elements are missing: please replace them with empty quoted strings. See https://github.com/omry/omegaconf/issues/572 for details.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/omegaconf/grammar_visitor.py:253: UserWarning: In the sequence `MMF_USER_DIR,` some elements are missing: please replace them with empty quoted strings. See https://github.com/omry/omegaconf/issues/572 for details.\n","  warnings.warn(\n","Data folder is /root/.cache/torch/mmf/data\n","Zip path is ../hateful_memes/datasets/hateful_memes.zip\n","Copying ../hateful_memes/datasets/hateful_memes.zip\n","Unzipping ../hateful_memes/datasets/hateful_memes.zip\n","Extracting the zip can take time. Sit back and relax.\n","Moving train.jsonl\n","Moving dev_seen.jsonl\n","Moving test_seen.jsonl\n","Moving dev_unseen.jsonl\n","Moving test_unseen.jsonl\n","Moving img\n"]},{"output_type":"stream","name":"stderr","text":["/root/.cache/torch/mmf/glove.6B.zip: 862MB [02:41, 5.33MB/s]                           \n","100%|█████████▉| 399999/400000 [00:47<00:00, 8455.96it/s]\n"]}],"source":["### Build Dataset\n","# provide the path to the compressed dataset folder (should have hateful_memes as root folder when uncompressed)\n","!mmf_convert_hm --zip_file=\"../hateful_memes/datasets/hateful_memes.zip\" --bypass_checksum=1\n","dataset = build_dataset(\"hateful_memes\")"]},{"cell_type":"markdown","source":["# Training\n","\n","The sections below rely on training `json` files that were created in a separate notebook in the Prerequisites section. If you see a warning about these files not existing please make sure to follow the steps in that section."],"metadata":{"id":"TQDCamGAVzuk"}},{"cell_type":"markdown","source":["## Pre-trained with CC"],"metadata":{"id":"2-qj2OFtNpOO"}},{"cell_type":"code","source":["!ls projects/visual_bert/configs/hateful_memes/"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"m1Rio70FT-Ya","executionInfo":{"status":"ok","timestamp":1714083647067,"user_tz":420,"elapsed":3,"user":{"displayName":"Patrick Prestridge","userId":"01452612983786479124"}},"outputId":"0aa41b04-1a34-43f8-8082-2921ece829bb"},"execution_count":34,"outputs":[{"output_type":"stream","name":"stdout","text":["gqa\t\t      masked_coco\t\t  masked_sbu   nlvr2\t\t  vqa2\n","hateful_memes\t      masked_conceptual_captions  masked_vqa2  visual_entailment\n","localized_narratives  masked_gqa\t\t  mmimdb       vizwiz\n"]}]},{"cell_type":"markdown","source":["### Dataset: Default"],"metadata":{"id":"eAzNDbW5XwfB"}},{"cell_type":"code","source":["!mmf_run config=\"projects/visual_bert/configs/hateful_memes/defaults.yaml\" \\\n","  model=visual_bert \\\n","  dataset=hateful_memes \\\n","  run_type=train_val \\\n","  training.log_interval=200 \\\n","  training.max_updates=1500 \\\n","  training.batch_size=64 \\\n","  training.evaluation_interval=200 \\\n","  training.tensorboard=True \\\n","  training.checkpoint_interval=200 \\\n","  checkpoint.resume_pretrained=True \\\n","  checkpoint.resume_zoo=visual_bert.pretrained.cc \\\n","  dataset_config.hateful_memes.annotations.train[0]=\"hateful_memes/defaults/annotations/train.jsonl\" \\\n","  dataset_config.hateful_memes.annotations.val[0]=\"hateful_memes/defaults/annotations/dev_unseen.jsonl\" \\\n","  dataset_config.hateful_memes.annotations.test[0]=\"hateful_memes/defaults/annotations/test_unseen.jsonl\""],"metadata":{"id":"aT8WYzHNUop9","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1714085070951,"user_tz":420,"elapsed":1410650,"user":{"displayName":"Patrick Prestridge","userId":"01452612983786479124"}},"outputId":"b60eb5f9-49ca-4213-e2cf-b25782134250"},"execution_count":35,"outputs":[{"output_type":"stream","name":"stdout","text":["2024-04-25 22:21:02.827704: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","2024-04-25 22:21:02.827764: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","2024-04-25 22:21:02.829491: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","2024-04-25 22:21:03.872399: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","2024-04-25 22:21:07.923996: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:47] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n","/usr/local/lib/python3.10/dist-packages/transformers/deepspeed.py:23: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/omegaconf/grammar_visitor.py:253: UserWarning: In the sequence `MMF_USER_DIR,` some elements are missing: please replace them with empty quoted strings. See https://github.com/omry/omegaconf/issues/572 for details.\n","  warnings.warn(\n","\u001b[32m2024-04-25T22:21:14 | mmf.utils.configuration: \u001b[0mOverriding option config to projects/visual_bert/configs/hateful_memes/defaults.yaml\n","\u001b[32m2024-04-25T22:21:14 | mmf.utils.configuration: \u001b[0mOverriding option model to visual_bert\n","\u001b[32m2024-04-25T22:21:14 | mmf.utils.configuration: \u001b[0mOverriding option datasets to hateful_memes\n","\u001b[32m2024-04-25T22:21:14 | mmf.utils.configuration: \u001b[0mOverriding option run_type to train_val\n","\u001b[32m2024-04-25T22:21:14 | mmf.utils.configuration: \u001b[0mOverriding option training.log_interval to 200\n","\u001b[32m2024-04-25T22:21:14 | mmf.utils.configuration: \u001b[0mOverriding option training.max_updates to 1500\n","\u001b[32m2024-04-25T22:21:14 | mmf.utils.configuration: \u001b[0mOverriding option training.batch_size to 64\n","\u001b[32m2024-04-25T22:21:14 | mmf.utils.configuration: \u001b[0mOverriding option training.evaluation_interval to 200\n","\u001b[32m2024-04-25T22:21:14 | mmf.utils.configuration: \u001b[0mOverriding option training.tensorboard to True\n","\u001b[32m2024-04-25T22:21:14 | mmf.utils.configuration: \u001b[0mOverriding option training.checkpoint_interval to 200\n","\u001b[32m2024-04-25T22:21:14 | mmf.utils.configuration: \u001b[0mOverriding option checkpoint.resume_pretrained to True\n","\u001b[32m2024-04-25T22:21:14 | mmf.utils.configuration: \u001b[0mOverriding option checkpoint.resume_zoo to visual_bert.pretrained.cc\n","\u001b[32m2024-04-25T22:21:14 | mmf.utils.configuration: \u001b[0mOverriding option dataset_config.hateful_memes.annotations.train[0] to hateful_memes/defaults/annotations/train.jsonl\n","\u001b[32m2024-04-25T22:21:14 | mmf.utils.configuration: \u001b[0mOverriding option dataset_config.hateful_memes.annotations.val[0] to hateful_memes/defaults/annotations/dev_unseen.jsonl\n","\u001b[32m2024-04-25T22:21:14 | mmf.utils.configuration: \u001b[0mOverriding option dataset_config.hateful_memes.annotations.test[0] to hateful_memes/defaults/annotations/test_unseen.jsonl\n","/usr/local/lib/python3.10/dist-packages/omegaconf/grammar_visitor.py:253: UserWarning: In the sequence `MMF_LOG_DIR,` some elements are missing: please replace them with empty quoted strings. See https://github.com/omry/omegaconf/issues/572 for details.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/omegaconf/grammar_visitor.py:253: UserWarning: In the sequence `MMF_REPORT_DIR,` some elements are missing: please replace them with empty quoted strings. See https://github.com/omry/omegaconf/issues/572 for details.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/omegaconf/grammar_visitor.py:253: UserWarning: In the sequence `MMF_TENSORBOARD_LOGDIR,` some elements are missing: please replace them with empty quoted strings. See https://github.com/omry/omegaconf/issues/572 for details.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/omegaconf/grammar_visitor.py:253: UserWarning: In the sequence `MMF_WANDB_LOGDIR,` some elements are missing: please replace them with empty quoted strings. See https://github.com/omry/omegaconf/issues/572 for details.\n","  warnings.warn(\n","\u001b[32m2024-04-25T22:21:14 | mmf: \u001b[0mLogging to: ./save/train.log\n","\u001b[32m2024-04-25T22:21:14 | mmf_cli.run: \u001b[0mNamespace(config_override=None, local_rank=None, opts=['config=projects/visual_bert/configs/hateful_memes/defaults.yaml', 'model=visual_bert', 'dataset=hateful_memes', 'run_type=train_val', 'training.log_interval=200', 'training.max_updates=1500', 'training.batch_size=64', 'training.evaluation_interval=200', 'training.tensorboard=True', 'training.checkpoint_interval=200', 'checkpoint.resume_pretrained=True', 'checkpoint.resume_zoo=visual_bert.pretrained.cc', 'dataset_config.hateful_memes.annotations.train[0]=hateful_memes/defaults/annotations/train.jsonl', 'dataset_config.hateful_memes.annotations.val[0]=hateful_memes/defaults/annotations/dev_unseen.jsonl', 'dataset_config.hateful_memes.annotations.test[0]=hateful_memes/defaults/annotations/test_unseen.jsonl'])\n","\u001b[32m2024-04-25T22:21:14 | mmf_cli.run: \u001b[0mTorch version: 2.2.0+cu121\n","\u001b[32m2024-04-25T22:21:14 | mmf.utils.general: \u001b[0mCUDA Device 0 is: NVIDIA A100-SXM4-40GB\n","\u001b[32m2024-04-25T22:21:14 | mmf_cli.run: \u001b[0mUsing seed 14467709\n","\u001b[32m2024-04-25T22:21:14 | mmf.trainers.mmf_trainer: \u001b[0mLoading datasets\n","[ Downloading: https://dl.fbaipublicfiles.com/mmf/data/datasets/hateful_memes/defaults/features/features_2020_10_01.tar.gz to /root/.cache/torch/mmf/data/datasets/hateful_memes/defaults/features/features.tar.gz ]\n","Downloading features.tar.gz: 100% 10.3G/10.3G [00:32<00:00, 313MB/s]\n","[ Starting checksum for features.tar.gz]\n","[ Checksum successful for features.tar.gz]\n","Unpacking features.tar.gz\n","tokenizer_config.json: 100% 48.0/48.0 [00:00<00:00, 216kB/s]\n","config.json: 100% 570/570 [00:00<00:00, 3.31MB/s]\n","loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--bert-base-uncased/snapshots/86b5e0934494bd15c9632b12f734a8a67f723594/config.json\n","Model config BertConfig {\n","  \"_name_or_path\": \"bert-base-uncased\",\n","  \"architectures\": [\n","    \"BertForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.39.2\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n","vocab.txt: 100% 232k/232k [00:00<00:00, 505kB/s]\n","tokenizer.json: 100% 466k/466k [00:00<00:00, 39.4MB/s]\n","loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--bert-base-uncased/snapshots/86b5e0934494bd15c9632b12f734a8a67f723594/vocab.txt\n","loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--bert-base-uncased/snapshots/86b5e0934494bd15c9632b12f734a8a67f723594/tokenizer.json\n","loading file added_tokens.json from cache at None\n","loading file special_tokens_map.json from cache at None\n","loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--bert-base-uncased/snapshots/86b5e0934494bd15c9632b12f734a8a67f723594/tokenizer_config.json\n","loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--bert-base-uncased/snapshots/86b5e0934494bd15c9632b12f734a8a67f723594/config.json\n","Model config BertConfig {\n","  \"_name_or_path\": \"bert-base-uncased\",\n","  \"architectures\": [\n","    \"BertForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.39.2\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n","\u001b[32m2024-04-25T22:24:29 | mmf.datasets.multi_datamodule: \u001b[0mMultitasking disabled by default for single dataset training\n","\u001b[32m2024-04-25T22:24:29 | mmf.datasets.multi_datamodule: \u001b[0mMultitasking disabled by default for single dataset training\n","\u001b[32m2024-04-25T22:24:29 | mmf.datasets.multi_datamodule: \u001b[0mMultitasking disabled by default for single dataset training\n","\u001b[32m2024-04-25T22:24:29 | mmf.trainers.mmf_trainer: \u001b[0mLoading model\n","Model config BertConfig {\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bert_model_name\": \"bert-base-uncased\",\n","  \"bypass_transformer\": false,\n","  \"classifier_dropout\": null,\n","  \"embedding_strategy\": \"plain\",\n","  \"finetune_lr_multiplier\": 1,\n","  \"freeze_base\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"losses\": [\n","    \"cross_entropy\"\n","  ],\n","  \"max_position_embeddings\": 512,\n","  \"model\": \"visual_bert\",\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"pooler_strategy\": \"default\",\n","  \"position_embedding_type\": \"absolute\",\n","  \"random_initialize\": false,\n","  \"special_visual_initialize\": true,\n","  \"training_head_type\": \"classification\",\n","  \"transformers_version\": \"4.39.2\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"visual_embedding_dim\": 2048,\n","  \"vocab_size\": 30522,\n","  \"zerobias\": false\n","}\n","\n","loading weights file model.safetensors from cache at /root/.cache/torch/mmf/distributed_-1/models--bert-base-uncased/snapshots/86b5e0934494bd15c9632b12f734a8a67f723594/model.safetensors\n","Some weights of the model checkpoint at bert-base-uncased were not used when initializing VisualBERTBase: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n","- This IS expected if you are initializing VisualBERTBase from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing VisualBERTBase from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of VisualBERTBase were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['bert.embeddings.position_embeddings_visual.weight', 'bert.embeddings.projection.bias', 'bert.embeddings.projection.weight', 'bert.embeddings.token_type_embeddings_visual.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","\u001b[32m2024-04-25T22:24:29 | mmf.trainers.mmf_trainer: \u001b[0mLoading optimizer\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2024-04-25T22:24:29 | py.warnings: \u001b[0m/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:457: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n","\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2024-04-25T22:24:29 | py.warnings: \u001b[0m/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:457: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n","\n","\u001b[32m2024-04-25T22:24:29 | mmf.trainers.mmf_trainer: \u001b[0mLoading metrics\n","\u001b[32m2024-04-25T22:24:30 | mmf.utils.checkpoint: \u001b[0mLoading checkpoint\n","[ Downloading: https://dl.fbaipublicfiles.com/mmf/data/models/visual_bert/visual_bert.pretrained.cc_full.tar.gz to /root/.cache/torch/mmf/data/models/visual_bert.pretrained.cc.defaults/visual_bert.pretrained.cc_full.tar.gz ]\n","Downloading visual_bert.pretrained.cc_full.tar.gz: 100% 415M/415M [00:19<00:00, 21.2MB/s]\n","[ Starting checksum for visual_bert.pretrained.cc_full.tar.gz]\n","[ Checksum successful for visual_bert.pretrained.cc_full.tar.gz]\n","Unpacking visual_bert.pretrained.cc_full.tar.gz\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2024-04-25T22:24:56 | mmf: \u001b[0mKey data_parallel is not present in registry, returning default value of None\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2024-04-25T22:24:56 | mmf: \u001b[0mKey distributed is not present in registry, returning default value of None\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2024-04-25T22:24:56 | mmf: \u001b[0mKey data_parallel is not present in registry, returning default value of None\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2024-04-25T22:24:56 | mmf: \u001b[0mKey distributed is not present in registry, returning default value of None\n","\u001b[32m2024-04-25T22:24:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.word_embeddings.weight from model.bert.embeddings.word_embeddings.weight\n","\u001b[32m2024-04-25T22:24:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.position_embeddings.weight from model.bert.embeddings.position_embeddings.weight\n","\u001b[32m2024-04-25T22:24:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.token_type_embeddings.weight from model.bert.embeddings.token_type_embeddings.weight\n","\u001b[32m2024-04-25T22:24:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.LayerNorm.weight from model.bert.embeddings.LayerNorm.weight\n","\u001b[32m2024-04-25T22:24:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.LayerNorm.bias from model.bert.embeddings.LayerNorm.bias\n","\u001b[32m2024-04-25T22:24:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.token_type_embeddings_visual.weight from model.bert.embeddings.token_type_embeddings_visual.weight\n","\u001b[32m2024-04-25T22:24:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.position_embeddings_visual.weight from model.bert.embeddings.position_embeddings_visual.weight\n","\u001b[32m2024-04-25T22:24:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.projection.weight from model.bert.embeddings.projection.weight\n","\u001b[32m2024-04-25T22:24:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.projection.bias from model.bert.embeddings.projection.bias\n","\u001b[32m2024-04-25T22:24:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.self.query.weight from model.bert.encoder.layer.0.attention.self.query.weight\n","\u001b[32m2024-04-25T22:24:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.self.query.bias from model.bert.encoder.layer.0.attention.self.query.bias\n","\u001b[32m2024-04-25T22:24:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.self.key.weight from model.bert.encoder.layer.0.attention.self.key.weight\n","\u001b[32m2024-04-25T22:24:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.self.key.bias from model.bert.encoder.layer.0.attention.self.key.bias\n","\u001b[32m2024-04-25T22:24:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.self.value.weight from model.bert.encoder.layer.0.attention.self.value.weight\n","\u001b[32m2024-04-25T22:24:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.self.value.bias from model.bert.encoder.layer.0.attention.self.value.bias\n","\u001b[32m2024-04-25T22:24:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.output.dense.weight from model.bert.encoder.layer.0.attention.output.dense.weight\n","\u001b[32m2024-04-25T22:24:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.output.dense.bias from model.bert.encoder.layer.0.attention.output.dense.bias\n","\u001b[32m2024-04-25T22:24:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.output.LayerNorm.weight from model.bert.encoder.layer.0.attention.output.LayerNorm.weight\n","\u001b[32m2024-04-25T22:24:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.output.LayerNorm.bias from model.bert.encoder.layer.0.attention.output.LayerNorm.bias\n","\u001b[32m2024-04-25T22:24:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.intermediate.dense.weight from model.bert.encoder.layer.0.intermediate.dense.weight\n","\u001b[32m2024-04-25T22:24:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.intermediate.dense.bias from model.bert.encoder.layer.0.intermediate.dense.bias\n","\u001b[32m2024-04-25T22:24:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.output.dense.weight from model.bert.encoder.layer.0.output.dense.weight\n","\u001b[32m2024-04-25T22:24:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.output.dense.bias from model.bert.encoder.layer.0.output.dense.bias\n","\u001b[32m2024-04-25T22:24:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.output.LayerNorm.weight from model.bert.encoder.layer.0.output.LayerNorm.weight\n","\u001b[32m2024-04-25T22:24:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.output.LayerNorm.bias from model.bert.encoder.layer.0.output.LayerNorm.bias\n","\u001b[32m2024-04-25T22:24:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.self.query.weight from model.bert.encoder.layer.1.attention.self.query.weight\n","\u001b[32m2024-04-25T22:24:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.self.query.bias from model.bert.encoder.layer.1.attention.self.query.bias\n","\u001b[32m2024-04-25T22:24:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.self.key.weight from model.bert.encoder.layer.1.attention.self.key.weight\n","\u001b[32m2024-04-25T22:24:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.self.key.bias from model.bert.encoder.layer.1.attention.self.key.bias\n","\u001b[32m2024-04-25T22:24:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.self.value.weight from model.bert.encoder.layer.1.attention.self.value.weight\n","\u001b[32m2024-04-25T22:24:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.self.value.bias from model.bert.encoder.layer.1.attention.self.value.bias\n","\u001b[32m2024-04-25T22:24:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.output.dense.weight from model.bert.encoder.layer.1.attention.output.dense.weight\n","\u001b[32m2024-04-25T22:24:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.output.dense.bias from model.bert.encoder.layer.1.attention.output.dense.bias\n","\u001b[32m2024-04-25T22:24:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.output.LayerNorm.weight from model.bert.encoder.layer.1.attention.output.LayerNorm.weight\n","\u001b[32m2024-04-25T22:24:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.output.LayerNorm.bias from model.bert.encoder.layer.1.attention.output.LayerNorm.bias\n","\u001b[32m2024-04-25T22:24:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.intermediate.dense.weight from model.bert.encoder.layer.1.intermediate.dense.weight\n","\u001b[32m2024-04-25T22:24:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.intermediate.dense.bias from model.bert.encoder.layer.1.intermediate.dense.bias\n","\u001b[32m2024-04-25T22:24:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.output.dense.weight from model.bert.encoder.layer.1.output.dense.weight\n","\u001b[32m2024-04-25T22:24:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.output.dense.bias from model.bert.encoder.layer.1.output.dense.bias\n","\u001b[32m2024-04-25T22:24:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.output.LayerNorm.weight from model.bert.encoder.layer.1.output.LayerNorm.weight\n","\u001b[32m2024-04-25T22:24:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.output.LayerNorm.bias from model.bert.encoder.layer.1.output.LayerNorm.bias\n","\u001b[32m2024-04-25T22:24:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.self.query.weight from model.bert.encoder.layer.2.attention.self.query.weight\n","\u001b[32m2024-04-25T22:24:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.self.query.bias from model.bert.encoder.layer.2.attention.self.query.bias\n","\u001b[32m2024-04-25T22:24:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.self.key.weight from model.bert.encoder.layer.2.attention.self.key.weight\n","\u001b[32m2024-04-25T22:24:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.self.key.bias from model.bert.encoder.layer.2.attention.self.key.bias\n","\u001b[32m2024-04-25T22:24:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.self.value.weight from model.bert.encoder.layer.2.attention.self.value.weight\n","\u001b[32m2024-04-25T22:24:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.self.value.bias from model.bert.encoder.layer.2.attention.self.value.bias\n","\u001b[32m2024-04-25T22:24:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.output.dense.weight from model.bert.encoder.layer.2.attention.output.dense.weight\n","\u001b[32m2024-04-25T22:24:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.output.dense.bias from model.bert.encoder.layer.2.attention.output.dense.bias\n","\u001b[32m2024-04-25T22:24:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.output.LayerNorm.weight from model.bert.encoder.layer.2.attention.output.LayerNorm.weight\n","\u001b[32m2024-04-25T22:24:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.output.LayerNorm.bias from model.bert.encoder.layer.2.attention.output.LayerNorm.bias\n","\u001b[32m2024-04-25T22:24:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.intermediate.dense.weight from model.bert.encoder.layer.2.intermediate.dense.weight\n","\u001b[32m2024-04-25T22:24:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.intermediate.dense.bias from model.bert.encoder.layer.2.intermediate.dense.bias\n","\u001b[32m2024-04-25T22:24:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.output.dense.weight from model.bert.encoder.layer.2.output.dense.weight\n","\u001b[32m2024-04-25T22:24:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.output.dense.bias from model.bert.encoder.layer.2.output.dense.bias\n","\u001b[32m2024-04-25T22:24:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.output.LayerNorm.weight from model.bert.encoder.layer.2.output.LayerNorm.weight\n","\u001b[32m2024-04-25T22:24:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.output.LayerNorm.bias from model.bert.encoder.layer.2.output.LayerNorm.bias\n","\u001b[32m2024-04-25T22:24:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.self.query.weight from model.bert.encoder.layer.3.attention.self.query.weight\n","\u001b[32m2024-04-25T22:24:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.self.query.bias from model.bert.encoder.layer.3.attention.self.query.bias\n","\u001b[32m2024-04-25T22:24:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.self.key.weight from model.bert.encoder.layer.3.attention.self.key.weight\n","\u001b[32m2024-04-25T22:24:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.self.key.bias from model.bert.encoder.layer.3.attention.self.key.bias\n","\u001b[32m2024-04-25T22:24:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.self.value.weight from model.bert.encoder.layer.3.attention.self.value.weight\n","\u001b[32m2024-04-25T22:24:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.self.value.bias from model.bert.encoder.layer.3.attention.self.value.bias\n","\u001b[32m2024-04-25T22:24:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.output.dense.weight from model.bert.encoder.layer.3.attention.output.dense.weight\n","\u001b[32m2024-04-25T22:24:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.output.dense.bias from model.bert.encoder.layer.3.attention.output.dense.bias\n","\u001b[32m2024-04-25T22:24:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.output.LayerNorm.weight from model.bert.encoder.layer.3.attention.output.LayerNorm.weight\n","\u001b[32m2024-04-25T22:24:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.output.LayerNorm.bias from model.bert.encoder.layer.3.attention.output.LayerNorm.bias\n","\u001b[32m2024-04-25T22:24:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.intermediate.dense.weight from model.bert.encoder.layer.3.intermediate.dense.weight\n","\u001b[32m2024-04-25T22:24:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.intermediate.dense.bias from model.bert.encoder.layer.3.intermediate.dense.bias\n","\u001b[32m2024-04-25T22:24:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.output.dense.weight from model.bert.encoder.layer.3.output.dense.weight\n","\u001b[32m2024-04-25T22:24:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.output.dense.bias from model.bert.encoder.layer.3.output.dense.bias\n","\u001b[32m2024-04-25T22:24:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.output.LayerNorm.weight from model.bert.encoder.layer.3.output.LayerNorm.weight\n","\u001b[32m2024-04-25T22:24:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.output.LayerNorm.bias from model.bert.encoder.layer.3.output.LayerNorm.bias\n","\u001b[32m2024-04-25T22:24:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.self.query.weight from model.bert.encoder.layer.4.attention.self.query.weight\n","\u001b[32m2024-04-25T22:24:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.self.query.bias from model.bert.encoder.layer.4.attention.self.query.bias\n","\u001b[32m2024-04-25T22:24:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.self.key.weight from model.bert.encoder.layer.4.attention.self.key.weight\n","\u001b[32m2024-04-25T22:24:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.self.key.bias from model.bert.encoder.layer.4.attention.self.key.bias\n","\u001b[32m2024-04-25T22:24:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.self.value.weight from model.bert.encoder.layer.4.attention.self.value.weight\n","\u001b[32m2024-04-25T22:24:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.self.value.bias from model.bert.encoder.layer.4.attention.self.value.bias\n","\u001b[32m2024-04-25T22:24:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.output.dense.weight from model.bert.encoder.layer.4.attention.output.dense.weight\n","\u001b[32m2024-04-25T22:24:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.output.dense.bias from model.bert.encoder.layer.4.attention.output.dense.bias\n","\u001b[32m2024-04-25T22:24:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.output.LayerNorm.weight from model.bert.encoder.layer.4.attention.output.LayerNorm.weight\n","\u001b[32m2024-04-25T22:24:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.output.LayerNorm.bias from model.bert.encoder.layer.4.attention.output.LayerNorm.bias\n","\u001b[32m2024-04-25T22:24:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.intermediate.dense.weight from model.bert.encoder.layer.4.intermediate.dense.weight\n","\u001b[32m2024-04-25T22:24:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.intermediate.dense.bias from model.bert.encoder.layer.4.intermediate.dense.bias\n","\u001b[32m2024-04-25T22:24:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.output.dense.weight from model.bert.encoder.layer.4.output.dense.weight\n","\u001b[32m2024-04-25T22:24:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.output.dense.bias from model.bert.encoder.layer.4.output.dense.bias\n","\u001b[32m2024-04-25T22:24:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.output.LayerNorm.weight from model.bert.encoder.layer.4.output.LayerNorm.weight\n","\u001b[32m2024-04-25T22:24:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.output.LayerNorm.bias from model.bert.encoder.layer.4.output.LayerNorm.bias\n","\u001b[32m2024-04-25T22:24:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.self.query.weight from model.bert.encoder.layer.5.attention.self.query.weight\n","\u001b[32m2024-04-25T22:24:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.self.query.bias from model.bert.encoder.layer.5.attention.self.query.bias\n","\u001b[32m2024-04-25T22:24:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.self.key.weight from model.bert.encoder.layer.5.attention.self.key.weight\n","\u001b[32m2024-04-25T22:24:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.self.key.bias from model.bert.encoder.layer.5.attention.self.key.bias\n","\u001b[32m2024-04-25T22:24:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.self.value.weight from model.bert.encoder.layer.5.attention.self.value.weight\n","\u001b[32m2024-04-25T22:24:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.self.value.bias from model.bert.encoder.layer.5.attention.self.value.bias\n","\u001b[32m2024-04-25T22:24:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.output.dense.weight from model.bert.encoder.layer.5.attention.output.dense.weight\n","\u001b[32m2024-04-25T22:24:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.output.dense.bias from model.bert.encoder.layer.5.attention.output.dense.bias\n","\u001b[32m2024-04-25T22:24:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.output.LayerNorm.weight from model.bert.encoder.layer.5.attention.output.LayerNorm.weight\n","\u001b[32m2024-04-25T22:24:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.output.LayerNorm.bias from model.bert.encoder.layer.5.attention.output.LayerNorm.bias\n","\u001b[32m2024-04-25T22:24:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.intermediate.dense.weight from model.bert.encoder.layer.5.intermediate.dense.weight\n","\u001b[32m2024-04-25T22:24:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.intermediate.dense.bias from model.bert.encoder.layer.5.intermediate.dense.bias\n","\u001b[32m2024-04-25T22:24:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.output.dense.weight from model.bert.encoder.layer.5.output.dense.weight\n","\u001b[32m2024-04-25T22:24:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.output.dense.bias from model.bert.encoder.layer.5.output.dense.bias\n","\u001b[32m2024-04-25T22:24:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.output.LayerNorm.weight from model.bert.encoder.layer.5.output.LayerNorm.weight\n","\u001b[32m2024-04-25T22:24:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.output.LayerNorm.bias from model.bert.encoder.layer.5.output.LayerNorm.bias\n","\u001b[32m2024-04-25T22:24:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.self.query.weight from model.bert.encoder.layer.6.attention.self.query.weight\n","\u001b[32m2024-04-25T22:24:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.self.query.bias from model.bert.encoder.layer.6.attention.self.query.bias\n","\u001b[32m2024-04-25T22:24:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.self.key.weight from model.bert.encoder.layer.6.attention.self.key.weight\n","\u001b[32m2024-04-25T22:24:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.self.key.bias from model.bert.encoder.layer.6.attention.self.key.bias\n","\u001b[32m2024-04-25T22:24:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.self.value.weight from model.bert.encoder.layer.6.attention.self.value.weight\n","\u001b[32m2024-04-25T22:24:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.self.value.bias from model.bert.encoder.layer.6.attention.self.value.bias\n","\u001b[32m2024-04-25T22:24:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.output.dense.weight from model.bert.encoder.layer.6.attention.output.dense.weight\n","\u001b[32m2024-04-25T22:24:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.output.dense.bias from model.bert.encoder.layer.6.attention.output.dense.bias\n","\u001b[32m2024-04-25T22:24:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.output.LayerNorm.weight from model.bert.encoder.layer.6.attention.output.LayerNorm.weight\n","\u001b[32m2024-04-25T22:24:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.output.LayerNorm.bias from model.bert.encoder.layer.6.attention.output.LayerNorm.bias\n","\u001b[32m2024-04-25T22:24:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.intermediate.dense.weight from model.bert.encoder.layer.6.intermediate.dense.weight\n","\u001b[32m2024-04-25T22:24:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.intermediate.dense.bias from model.bert.encoder.layer.6.intermediate.dense.bias\n","\u001b[32m2024-04-25T22:24:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.output.dense.weight from model.bert.encoder.layer.6.output.dense.weight\n","\u001b[32m2024-04-25T22:24:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.output.dense.bias from model.bert.encoder.layer.6.output.dense.bias\n","\u001b[32m2024-04-25T22:24:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.output.LayerNorm.weight from model.bert.encoder.layer.6.output.LayerNorm.weight\n","\u001b[32m2024-04-25T22:24:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.output.LayerNorm.bias from model.bert.encoder.layer.6.output.LayerNorm.bias\n","\u001b[32m2024-04-25T22:24:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.self.query.weight from model.bert.encoder.layer.7.attention.self.query.weight\n","\u001b[32m2024-04-25T22:24:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.self.query.bias from model.bert.encoder.layer.7.attention.self.query.bias\n","\u001b[32m2024-04-25T22:24:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.self.key.weight from model.bert.encoder.layer.7.attention.self.key.weight\n","\u001b[32m2024-04-25T22:24:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.self.key.bias from model.bert.encoder.layer.7.attention.self.key.bias\n","\u001b[32m2024-04-25T22:24:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.self.value.weight from model.bert.encoder.layer.7.attention.self.value.weight\n","\u001b[32m2024-04-25T22:24:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.self.value.bias from model.bert.encoder.layer.7.attention.self.value.bias\n","\u001b[32m2024-04-25T22:24:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.output.dense.weight from model.bert.encoder.layer.7.attention.output.dense.weight\n","\u001b[32m2024-04-25T22:24:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.output.dense.bias from model.bert.encoder.layer.7.attention.output.dense.bias\n","\u001b[32m2024-04-25T22:24:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.output.LayerNorm.weight from model.bert.encoder.layer.7.attention.output.LayerNorm.weight\n","\u001b[32m2024-04-25T22:24:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.output.LayerNorm.bias from model.bert.encoder.layer.7.attention.output.LayerNorm.bias\n","\u001b[32m2024-04-25T22:24:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.intermediate.dense.weight from model.bert.encoder.layer.7.intermediate.dense.weight\n","\u001b[32m2024-04-25T22:24:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.intermediate.dense.bias from model.bert.encoder.layer.7.intermediate.dense.bias\n","\u001b[32m2024-04-25T22:24:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.output.dense.weight from model.bert.encoder.layer.7.output.dense.weight\n","\u001b[32m2024-04-25T22:24:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.output.dense.bias from model.bert.encoder.layer.7.output.dense.bias\n","\u001b[32m2024-04-25T22:24:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.output.LayerNorm.weight from model.bert.encoder.layer.7.output.LayerNorm.weight\n","\u001b[32m2024-04-25T22:24:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.output.LayerNorm.bias from model.bert.encoder.layer.7.output.LayerNorm.bias\n","\u001b[32m2024-04-25T22:24:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.self.query.weight from model.bert.encoder.layer.8.attention.self.query.weight\n","\u001b[32m2024-04-25T22:24:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.self.query.bias from model.bert.encoder.layer.8.attention.self.query.bias\n","\u001b[32m2024-04-25T22:24:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.self.key.weight from model.bert.encoder.layer.8.attention.self.key.weight\n","\u001b[32m2024-04-25T22:24:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.self.key.bias from model.bert.encoder.layer.8.attention.self.key.bias\n","\u001b[32m2024-04-25T22:24:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.self.value.weight from model.bert.encoder.layer.8.attention.self.value.weight\n","\u001b[32m2024-04-25T22:24:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.self.value.bias from model.bert.encoder.layer.8.attention.self.value.bias\n","\u001b[32m2024-04-25T22:24:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.output.dense.weight from model.bert.encoder.layer.8.attention.output.dense.weight\n","\u001b[32m2024-04-25T22:24:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.output.dense.bias from model.bert.encoder.layer.8.attention.output.dense.bias\n","\u001b[32m2024-04-25T22:24:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.output.LayerNorm.weight from model.bert.encoder.layer.8.attention.output.LayerNorm.weight\n","\u001b[32m2024-04-25T22:24:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.output.LayerNorm.bias from model.bert.encoder.layer.8.attention.output.LayerNorm.bias\n","\u001b[32m2024-04-25T22:24:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.intermediate.dense.weight from model.bert.encoder.layer.8.intermediate.dense.weight\n","\u001b[32m2024-04-25T22:24:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.intermediate.dense.bias from model.bert.encoder.layer.8.intermediate.dense.bias\n","\u001b[32m2024-04-25T22:24:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.output.dense.weight from model.bert.encoder.layer.8.output.dense.weight\n","\u001b[32m2024-04-25T22:24:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.output.dense.bias from model.bert.encoder.layer.8.output.dense.bias\n","\u001b[32m2024-04-25T22:24:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.output.LayerNorm.weight from model.bert.encoder.layer.8.output.LayerNorm.weight\n","\u001b[32m2024-04-25T22:24:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.output.LayerNorm.bias from model.bert.encoder.layer.8.output.LayerNorm.bias\n","\u001b[32m2024-04-25T22:24:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.self.query.weight from model.bert.encoder.layer.9.attention.self.query.weight\n","\u001b[32m2024-04-25T22:24:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.self.query.bias from model.bert.encoder.layer.9.attention.self.query.bias\n","\u001b[32m2024-04-25T22:24:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.self.key.weight from model.bert.encoder.layer.9.attention.self.key.weight\n","\u001b[32m2024-04-25T22:24:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.self.key.bias from model.bert.encoder.layer.9.attention.self.key.bias\n","\u001b[32m2024-04-25T22:24:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.self.value.weight from model.bert.encoder.layer.9.attention.self.value.weight\n","\u001b[32m2024-04-25T22:24:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.self.value.bias from model.bert.encoder.layer.9.attention.self.value.bias\n","\u001b[32m2024-04-25T22:24:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.output.dense.weight from model.bert.encoder.layer.9.attention.output.dense.weight\n","\u001b[32m2024-04-25T22:24:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.output.dense.bias from model.bert.encoder.layer.9.attention.output.dense.bias\n","\u001b[32m2024-04-25T22:24:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.output.LayerNorm.weight from model.bert.encoder.layer.9.attention.output.LayerNorm.weight\n","\u001b[32m2024-04-25T22:24:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.output.LayerNorm.bias from model.bert.encoder.layer.9.attention.output.LayerNorm.bias\n","\u001b[32m2024-04-25T22:24:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.intermediate.dense.weight from model.bert.encoder.layer.9.intermediate.dense.weight\n","\u001b[32m2024-04-25T22:24:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.intermediate.dense.bias from model.bert.encoder.layer.9.intermediate.dense.bias\n","\u001b[32m2024-04-25T22:24:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.output.dense.weight from model.bert.encoder.layer.9.output.dense.weight\n","\u001b[32m2024-04-25T22:24:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.output.dense.bias from model.bert.encoder.layer.9.output.dense.bias\n","\u001b[32m2024-04-25T22:24:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.output.LayerNorm.weight from model.bert.encoder.layer.9.output.LayerNorm.weight\n","\u001b[32m2024-04-25T22:24:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.output.LayerNorm.bias from model.bert.encoder.layer.9.output.LayerNorm.bias\n","\u001b[32m2024-04-25T22:24:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.self.query.weight from model.bert.encoder.layer.10.attention.self.query.weight\n","\u001b[32m2024-04-25T22:24:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.self.query.bias from model.bert.encoder.layer.10.attention.self.query.bias\n","\u001b[32m2024-04-25T22:24:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.self.key.weight from model.bert.encoder.layer.10.attention.self.key.weight\n","\u001b[32m2024-04-25T22:24:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.self.key.bias from model.bert.encoder.layer.10.attention.self.key.bias\n","\u001b[32m2024-04-25T22:24:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.self.value.weight from model.bert.encoder.layer.10.attention.self.value.weight\n","\u001b[32m2024-04-25T22:24:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.self.value.bias from model.bert.encoder.layer.10.attention.self.value.bias\n","\u001b[32m2024-04-25T22:24:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.output.dense.weight from model.bert.encoder.layer.10.attention.output.dense.weight\n","\u001b[32m2024-04-25T22:24:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.output.dense.bias from model.bert.encoder.layer.10.attention.output.dense.bias\n","\u001b[32m2024-04-25T22:24:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.output.LayerNorm.weight from model.bert.encoder.layer.10.attention.output.LayerNorm.weight\n","\u001b[32m2024-04-25T22:24:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.output.LayerNorm.bias from model.bert.encoder.layer.10.attention.output.LayerNorm.bias\n","\u001b[32m2024-04-25T22:24:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.intermediate.dense.weight from model.bert.encoder.layer.10.intermediate.dense.weight\n","\u001b[32m2024-04-25T22:24:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.intermediate.dense.bias from model.bert.encoder.layer.10.intermediate.dense.bias\n","\u001b[32m2024-04-25T22:24:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.output.dense.weight from model.bert.encoder.layer.10.output.dense.weight\n","\u001b[32m2024-04-25T22:24:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.output.dense.bias from model.bert.encoder.layer.10.output.dense.bias\n","\u001b[32m2024-04-25T22:24:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.output.LayerNorm.weight from model.bert.encoder.layer.10.output.LayerNorm.weight\n","\u001b[32m2024-04-25T22:24:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.output.LayerNorm.bias from model.bert.encoder.layer.10.output.LayerNorm.bias\n","\u001b[32m2024-04-25T22:24:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.self.query.weight from model.bert.encoder.layer.11.attention.self.query.weight\n","\u001b[32m2024-04-25T22:24:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.self.query.bias from model.bert.encoder.layer.11.attention.self.query.bias\n","\u001b[32m2024-04-25T22:24:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.self.key.weight from model.bert.encoder.layer.11.attention.self.key.weight\n","\u001b[32m2024-04-25T22:24:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.self.key.bias from model.bert.encoder.layer.11.attention.self.key.bias\n","\u001b[32m2024-04-25T22:24:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.self.value.weight from model.bert.encoder.layer.11.attention.self.value.weight\n","\u001b[32m2024-04-25T22:24:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.self.value.bias from model.bert.encoder.layer.11.attention.self.value.bias\n","\u001b[32m2024-04-25T22:24:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.output.dense.weight from model.bert.encoder.layer.11.attention.output.dense.weight\n","\u001b[32m2024-04-25T22:24:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.output.dense.bias from model.bert.encoder.layer.11.attention.output.dense.bias\n","\u001b[32m2024-04-25T22:24:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.output.LayerNorm.weight from model.bert.encoder.layer.11.attention.output.LayerNorm.weight\n","\u001b[32m2024-04-25T22:24:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.output.LayerNorm.bias from model.bert.encoder.layer.11.attention.output.LayerNorm.bias\n","\u001b[32m2024-04-25T22:24:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.intermediate.dense.weight from model.bert.encoder.layer.11.intermediate.dense.weight\n","\u001b[32m2024-04-25T22:24:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.intermediate.dense.bias from model.bert.encoder.layer.11.intermediate.dense.bias\n","\u001b[32m2024-04-25T22:24:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.output.dense.weight from model.bert.encoder.layer.11.output.dense.weight\n","\u001b[32m2024-04-25T22:24:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.output.dense.bias from model.bert.encoder.layer.11.output.dense.bias\n","\u001b[32m2024-04-25T22:24:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.output.LayerNorm.weight from model.bert.encoder.layer.11.output.LayerNorm.weight\n","\u001b[32m2024-04-25T22:24:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.output.LayerNorm.bias from model.bert.encoder.layer.11.output.LayerNorm.bias\n","\u001b[32m2024-04-25T22:24:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.pooler.dense.weight from model.bert.pooler.dense.weight\n","\u001b[32m2024-04-25T22:24:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.pooler.dense.bias from model.bert.pooler.dense.bias\n","\u001b[32m2024-04-25T22:24:57 | mmf.utils.checkpoint: \u001b[0mPretrained model loaded\n","\u001b[32m2024-04-25T22:24:57 | mmf.utils.checkpoint: \u001b[0mCheckpoint loaded.\n","\u001b[32m2024-04-25T22:24:57 | mmf.utils.checkpoint: \u001b[0mCurrent num updates: 0\n","\u001b[32m2024-04-25T22:24:57 | mmf.utils.checkpoint: \u001b[0mCurrent iteration: 0\n","\u001b[32m2024-04-25T22:24:57 | mmf.utils.checkpoint: \u001b[0mCurrent epoch: 0\n","\u001b[32m2024-04-25T22:24:57 | mmf.trainers.mmf_trainer: \u001b[0m===== Model =====\n","\u001b[32m2024-04-25T22:24:57 | mmf.trainers.mmf_trainer: \u001b[0mVisualBERT(\n","  (model): VisualBERTForClassification(\n","    (bert): VisualBERTBase(\n","      (embeddings): BertVisioLinguisticEmbeddings(\n","        (word_embeddings): Embedding(30522, 768, padding_idx=0)\n","        (position_embeddings): Embedding(512, 768)\n","        (token_type_embeddings): Embedding(2, 768)\n","        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","        (token_type_embeddings_visual): Embedding(2, 768)\n","        (position_embeddings_visual): Embedding(512, 768)\n","        (projection): Linear(in_features=2048, out_features=768, bias=True)\n","      )\n","      (encoder): BertEncoderJit(\n","        (layer): ModuleList(\n","          (0-11): 12 x BertLayerJit(\n","            (attention): BertAttentionJit(\n","              (self): BertSelfAttentionJit(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","              (intermediate_act_fn): GELUActivation()\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","        )\n","      )\n","      (pooler): BertPooler(\n","        (dense): Linear(in_features=768, out_features=768, bias=True)\n","        (activation): Tanh()\n","      )\n","    )\n","    (dropout): Dropout(p=0.1, inplace=False)\n","    (classifier): Sequential(\n","      (0): BertPredictionHeadTransform(\n","        (dense): Linear(in_features=768, out_features=768, bias=True)\n","        (transform_act_fn): GELUActivation()\n","        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      )\n","      (1): Linear(in_features=768, out_features=2, bias=True)\n","    )\n","  )\n","  (losses): Losses(\n","    (losses): ModuleList(\n","      (0): MMFLoss(\n","        (loss_criterion): CrossEntropyLoss(\n","          (loss_fn): CrossEntropyLoss()\n","        )\n","      )\n","    )\n","  )\n",")\n","\u001b[32m2024-04-25T22:24:57 | mmf.utils.general: \u001b[0mTotal Parameters: 112044290. Trained Parameters: 112044290\n","\u001b[32m2024-04-25T22:24:57 | mmf.trainers.core.training_loop: \u001b[0mStarting training...\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2024-04-25T22:24:57 | py.warnings: \u001b[0m/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n","  self.pid = os.fork()\n","\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2024-04-25T22:24:57 | py.warnings: \u001b[0m/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n","  self.pid = os.fork()\n","\n","\u001b[32m2024-04-25T22:27:01 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n","\u001b[32m2024-04-25T22:27:01 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2024-04-25T22:27:05 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2024-04-25T22:27:38 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2024-04-25T22:27:38 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 200/1500, train/hateful_memes/cross_entropy: 0.5922, train/hateful_memes/cross_entropy/avg: 0.5922, train/total_loss: 0.5922, train/total_loss/avg: 0.5922, max mem: 14364.0, experiment: run, epoch: 2, num_updates: 200, iterations: 200, max_updates: 1500, lr: 0.00001, ups: 1.24, time: 02m 41s 906ms, time_since_start: 03m 08s 509ms, eta: 18m 19s 752ms\n","\u001b[32m2024-04-25T22:27:38 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n","\u001b[32m2024-04-25T22:27:38 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n","\u001b[32m2024-04-25T22:27:41 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished evaluation inference. Loaded 9\n","\u001b[32m2024-04-25T22:27:41 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n","\u001b[32m2024-04-25T22:27:41 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2024-04-25T22:27:45 | mmf.utils.checkpoint: \u001b[0mSaving best checkpoint\n","\u001b[32m2024-04-25T22:28:03 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2024-04-25T22:28:10 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2024-04-25T22:28:10 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 200/1500, val/hateful_memes/cross_entropy: 0.6641, val/total_loss: 0.6641, val/hateful_memes/accuracy: 0.6296, val/hateful_memes/binary_f1: 0.0000, val/hateful_memes/roc_auc: 0.5403, num_updates: 200, epoch: 2, iterations: 200, max_updates: 1500, val_time: 31s 665ms, best_update: 200, best_iteration: 200, best_val/hateful_memes/roc_auc: 0.540250\n","\u001b[32m2024-04-25T22:30:14 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n","\u001b[32m2024-04-25T22:30:14 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2024-04-25T22:30:18 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2024-04-25T22:30:21 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2024-04-25T22:30:21 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 400/1500, train/hateful_memes/cross_entropy: 0.4719, train/hateful_memes/cross_entropy/avg: 0.5321, train/total_loss: 0.4719, train/total_loss/avg: 0.5321, max mem: 14413.0, experiment: run, epoch: 4, num_updates: 400, iterations: 400, max_updates: 1500, lr: 0.00001, ups: 1.53, time: 02m 11s 315ms, time_since_start: 05m 51s 505ms, eta: 12m 34s 738ms\n","\u001b[32m2024-04-25T22:30:21 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n","\u001b[32m2024-04-25T22:30:21 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2024-04-25T22:30:21 | py.warnings: \u001b[0m/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n","  self.pid = os.fork()\n","\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2024-04-25T22:30:21 | py.warnings: \u001b[0m/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n","  self.pid = os.fork()\n","\n","\u001b[32m2024-04-25T22:30:24 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished evaluation inference. Loaded 9\n","\u001b[32m2024-04-25T22:30:24 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n","\u001b[32m2024-04-25T22:30:24 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2024-04-25T22:30:27 | mmf.utils.checkpoint: \u001b[0mSaving best checkpoint\n","\u001b[32m2024-04-25T22:30:34 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2024-04-25T22:30:39 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2024-04-25T22:30:39 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 400/1500, val/hateful_memes/cross_entropy: 0.7518, val/total_loss: 0.7518, val/hateful_memes/accuracy: 0.6259, val/hateful_memes/binary_f1: 0.2171, val/hateful_memes/roc_auc: 0.6409, num_updates: 400, epoch: 4, iterations: 400, max_updates: 1500, val_time: 17s 723ms, best_update: 400, best_iteration: 400, best_val/hateful_memes/roc_auc: 0.640868\n","\u001b[32m2024-04-25T22:32:43 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n","\u001b[32m2024-04-25T22:32:43 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2024-04-25T22:32:47 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2024-04-25T22:32:50 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2024-04-25T22:32:50 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 600/1500, train/hateful_memes/cross_entropy: 0.4719, train/hateful_memes/cross_entropy/avg: 0.4246, train/total_loss: 0.4719, train/total_loss/avg: 0.4246, max mem: 14413.0, experiment: run, epoch: 5, num_updates: 600, iterations: 600, max_updates: 1500, lr: 0.00002, ups: 1.53, time: 02m 11s 142ms, time_since_start: 08m 20s 373ms, eta: 10m 16s 699ms\n","\u001b[32m2024-04-25T22:32:50 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n","\u001b[32m2024-04-25T22:32:50 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2024-04-25T22:32:50 | py.warnings: \u001b[0m/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n","  self.pid = os.fork()\n","\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2024-04-25T22:32:50 | py.warnings: \u001b[0m/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n","  self.pid = os.fork()\n","\n","\u001b[32m2024-04-25T22:32:53 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished evaluation inference. Loaded 9\n","\u001b[32m2024-04-25T22:32:53 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n","\u001b[32m2024-04-25T22:32:53 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2024-04-25T22:32:56 | mmf.utils.checkpoint: \u001b[0mSaving best checkpoint\n","\u001b[32m2024-04-25T22:33:03 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2024-04-25T22:33:08 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2024-04-25T22:33:08 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 600/1500, val/hateful_memes/cross_entropy: 0.7455, val/total_loss: 0.7455, val/hateful_memes/accuracy: 0.6741, val/hateful_memes/binary_f1: 0.3759, val/hateful_memes/roc_auc: 0.6860, num_updates: 600, epoch: 5, iterations: 600, max_updates: 1500, val_time: 17s 992ms, best_update: 600, best_iteration: 600, best_val/hateful_memes/roc_auc: 0.686015\n","\u001b[32m2024-04-25T22:35:13 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n","\u001b[32m2024-04-25T22:35:13 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2024-04-25T22:35:16 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2024-04-25T22:35:20 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2024-04-25T22:35:20 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 800/1500, train/hateful_memes/cross_entropy: 0.2097, train/hateful_memes/cross_entropy/avg: 0.3625, train/total_loss: 0.2097, train/total_loss/avg: 0.3625, max mem: 14413.0, experiment: run, epoch: 7, num_updates: 800, iterations: 800, max_updates: 1500, lr: 0.00002, ups: 1.53, time: 02m 11s 656ms, time_since_start: 10m 50s 024ms, eta: 08m 01s 533ms\n","\u001b[32m2024-04-25T22:35:20 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n","\u001b[32m2024-04-25T22:35:20 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2024-04-25T22:35:20 | py.warnings: \u001b[0m/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n","  self.pid = os.fork()\n","\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2024-04-25T22:35:20 | py.warnings: \u001b[0m/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n","  self.pid = os.fork()\n","\n","\u001b[32m2024-04-25T22:35:22 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished evaluation inference. Loaded 9\n","\u001b[32m2024-04-25T22:35:22 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n","\u001b[32m2024-04-25T22:35:23 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2024-04-25T22:35:27 | mmf.utils.checkpoint: \u001b[0mSaving best checkpoint\n","\u001b[32m2024-04-25T22:35:30 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2024-04-25T22:35:35 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2024-04-25T22:35:35 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 800/1500, val/hateful_memes/cross_entropy: 0.8498, val/total_loss: 0.8498, val/hateful_memes/accuracy: 0.6815, val/hateful_memes/binary_f1: 0.4069, val/hateful_memes/roc_auc: 0.7158, num_updates: 800, epoch: 7, iterations: 800, max_updates: 1500, val_time: 14s 915ms, best_update: 800, best_iteration: 800, best_val/hateful_memes/roc_auc: 0.715794\n","\u001b[32m2024-04-25T22:37:40 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n","\u001b[32m2024-04-25T22:37:40 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2024-04-25T22:38:11 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2024-04-25T22:38:16 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2024-04-25T22:38:16 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1000/1500, train/hateful_memes/cross_entropy: 0.2097, train/hateful_memes/cross_entropy/avg: 0.3126, train/total_loss: 0.2097, train/total_loss/avg: 0.3126, max mem: 14413.0, experiment: run, epoch: 8, num_updates: 1000, iterations: 1000, max_updates: 1500, lr: 0.00003, ups: 1.24, time: 02m 41s 633ms, time_since_start: 13m 46s 576ms, eta: 07m 02s 268ms\n","\u001b[32m2024-04-25T22:38:16 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n","\u001b[32m2024-04-25T22:38:17 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2024-04-25T22:38:17 | py.warnings: \u001b[0m/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n","  self.pid = os.fork()\n","\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2024-04-25T22:38:17 | py.warnings: \u001b[0m/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n","  self.pid = os.fork()\n","\n","\u001b[32m2024-04-25T22:38:19 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished evaluation inference. Loaded 9\n","\u001b[32m2024-04-25T22:38:19 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n","\u001b[32m2024-04-25T22:38:19 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2024-04-25T22:38:22 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2024-04-25T22:38:29 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2024-04-25T22:38:29 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1000/1500, val/hateful_memes/cross_entropy: 1.2163, val/total_loss: 1.2163, val/hateful_memes/accuracy: 0.6778, val/hateful_memes/binary_f1: 0.4200, val/hateful_memes/roc_auc: 0.7054, num_updates: 1000, epoch: 8, iterations: 1000, max_updates: 1500, val_time: 12s 249ms, best_update: 800, best_iteration: 800, best_val/hateful_memes/roc_auc: 0.715794\n","\u001b[32m2024-04-25T22:40:34 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n","\u001b[32m2024-04-25T22:40:34 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2024-04-25T22:40:37 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2024-04-25T22:40:41 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2024-04-25T22:40:41 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1200/1500, train/hateful_memes/cross_entropy: 0.1761, train/hateful_memes/cross_entropy/avg: 0.2622, train/total_loss: 0.1761, train/total_loss/avg: 0.2622, max mem: 14413.0, experiment: run, epoch: 10, num_updates: 1200, iterations: 1200, max_updates: 1500, lr: 0.00003, ups: 1.52, time: 02m 12s 019ms, time_since_start: 16m 10s 847ms, eta: 03m 26s 940ms\n","\u001b[32m2024-04-25T22:40:41 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n","\u001b[32m2024-04-25T22:40:41 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2024-04-25T22:40:41 | py.warnings: \u001b[0m/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n","  self.pid = os.fork()\n","\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2024-04-25T22:40:41 | py.warnings: \u001b[0m/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n","  self.pid = os.fork()\n","\n","\u001b[32m2024-04-25T22:40:43 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished evaluation inference. Loaded 9\n","\u001b[32m2024-04-25T22:40:43 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n","\u001b[32m2024-04-25T22:40:43 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2024-04-25T22:40:47 | mmf.utils.checkpoint: \u001b[0mSaving best checkpoint\n","\u001b[32m2024-04-25T22:40:51 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2024-04-25T22:40:56 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2024-04-25T22:40:56 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1200/1500, val/hateful_memes/cross_entropy: 1.3059, val/total_loss: 1.3059, val/hateful_memes/accuracy: 0.6944, val/hateful_memes/binary_f1: 0.4860, val/hateful_memes/roc_auc: 0.7255, num_updates: 1200, epoch: 10, iterations: 1200, max_updates: 1500, val_time: 15s 699ms, best_update: 1200, best_iteration: 1200, best_val/hateful_memes/roc_auc: 0.725500\n","\u001b[32m2024-04-25T22:43:01 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n","\u001b[32m2024-04-25T22:43:01 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2024-04-25T22:43:04 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2024-04-25T22:43:08 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2024-04-25T22:43:08 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1400/1500, train/hateful_memes/cross_entropy: 0.1761, train/hateful_memes/cross_entropy/avg: 0.2424, train/total_loss: 0.1761, train/total_loss/avg: 0.2424, max mem: 14413.0, experiment: run, epoch: 11, num_updates: 1400, iterations: 1400, max_updates: 1500, lr: 0.00003, ups: 1.53, time: 02m 11s 448ms, time_since_start: 18m 37s 998ms, eta: 01m 08s 681ms\n","\u001b[32m2024-04-25T22:43:08 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n","\u001b[32m2024-04-25T22:43:08 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2024-04-25T22:43:08 | py.warnings: \u001b[0m/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n","  self.pid = os.fork()\n","\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2024-04-25T22:43:08 | py.warnings: \u001b[0m/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n","  self.pid = os.fork()\n","\n","\u001b[32m2024-04-25T22:43:10 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished evaluation inference. Loaded 9\n","\u001b[32m2024-04-25T22:43:10 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n","\u001b[32m2024-04-25T22:43:10 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2024-04-25T22:43:14 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2024-04-25T22:43:17 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2024-04-25T22:43:17 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1400/1500, val/hateful_memes/cross_entropy: 1.2695, val/total_loss: 1.2695, val/hateful_memes/accuracy: 0.6963, val/hateful_memes/binary_f1: 0.4605, val/hateful_memes/roc_auc: 0.7226, num_updates: 1400, epoch: 11, iterations: 1400, max_updates: 1500, val_time: 09s 524ms, best_update: 1200, best_iteration: 1200, best_val/hateful_memes/roc_auc: 0.725500\n","\u001b[32m2024-04-25T22:44:20 | mmf.trainers.core.training_loop: \u001b[0mStepping into final validation check\n","\u001b[32m2024-04-25T22:44:20 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2024-04-25T22:44:20 | py.warnings: \u001b[0m/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n","  self.pid = os.fork()\n","\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2024-04-25T22:44:20 | py.warnings: \u001b[0m/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n","  self.pid = os.fork()\n","\n","\u001b[32m2024-04-25T22:44:22 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished evaluation inference. Loaded 9\n","\u001b[32m2024-04-25T22:44:22 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n","\u001b[32m2024-04-25T22:44:22 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1500/1500, val/hateful_memes/cross_entropy: 1.3925, val/total_loss: 1.3925, val/hateful_memes/accuracy: 0.7148, val/hateful_memes/binary_f1: 0.5497, val/hateful_memes/roc_auc: 0.7167, num_updates: 1500, epoch: 12, iterations: 1500, max_updates: 1500, val_time: 01m 14s 396ms, best_update: 1200, best_iteration: 1200, best_val/hateful_memes/roc_auc: 0.725500\n","\u001b[32m2024-04-25T22:44:22 | mmf.utils.checkpoint: \u001b[0mRestoring checkpoint\n","\u001b[32m2024-04-25T22:44:22 | mmf.utils.checkpoint: \u001b[0mLoading checkpoint\n","\u001b[32m2024-04-25T22:44:24 | mmf.utils.checkpoint: \u001b[0mCheckpoint loaded.\n","\u001b[32m2024-04-25T22:44:24 | mmf.utils.checkpoint: \u001b[0mCurrent num updates: 1200\n","\u001b[32m2024-04-25T22:44:24 | mmf.utils.checkpoint: \u001b[0mCurrent iteration: 1200\n","\u001b[32m2024-04-25T22:44:24 | mmf.utils.checkpoint: \u001b[0mCurrent epoch: 10\n","\u001b[32m2024-04-25T22:44:25 | mmf.trainers.mmf_trainer: \u001b[0mStarting inference on val set\n","\u001b[32m2024-04-25T22:44:25 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n","  0% 0/9 [00:00<?, ?it/s]\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2024-04-25T22:44:25 | py.warnings: \u001b[0m/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n","  self.pid = os.fork()\n","\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2024-04-25T22:44:25 | py.warnings: \u001b[0m/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n","  self.pid = os.fork()\n","\n","100% 9/9 [00:02<00:00,  3.88it/s]\n","\u001b[32m2024-04-25T22:44:27 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished evaluation inference. Loaded 9\n","\u001b[32m2024-04-25T22:44:27 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n","\u001b[32m2024-04-25T22:44:27 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1200/1500, val/hateful_memes/cross_entropy: 1.3059, val/total_loss: 1.3059, val/hateful_memes/accuracy: 0.6944, val/hateful_memes/binary_f1: 0.4860, val/hateful_memes/roc_auc: 0.7255\n","\u001b[32m2024-04-25T22:44:27 | mmf.trainers.callbacks.logistics: \u001b[0mFinished run in 19m 57s 332ms\n"]}]},{"cell_type":"markdown","source":["### Dataset: Appended with images from dev"],"metadata":{"id":"Og1oL2ALX00e"}},{"cell_type":"code","source":["!mmf_run config=\"projects/visual_bert/configs/hateful_memes/defaults.yaml\" \\\n","  model=visual_bert \\\n","  dataset=hateful_memes \\\n","  run_type=train_val \\\n","  training.log_interval=200 \\\n","  training.batch_size=64 \\\n","  training.evaluation_interval=200 \\\n","  training.tensorboard=True \\\n","  training.checkpoint_interval=200 \\\n","  checkpoint.resume_pretrained=True \\\n","  checkpoint.resume_zoo=visual_bert.pretrained.cc \\\n","  dataset_config.hateful_memes.annotations.train[0]=\"hateful_memes/defaults/annotations/train_dev.jsonl\" \\\n","  dataset_config.hateful_memes.annotations.val[0]=\"hateful_memes/defaults/annotations/dev_unseen.jsonl\" \\\n","  dataset_config.hateful_memes.annotations.test[0]=\"hateful_memes/defaults/annotations/test_unseen.jsonl\""],"metadata":{"id":"6tBFsepoVj5n","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1714090444653,"user_tz":420,"elapsed":1125077,"user":{"displayName":"Patrick Prestridge","userId":"01452612983786479124"}},"outputId":"974a641a-47d3-4e63-a4cf-ab847b22479a"},"execution_count":38,"outputs":[{"output_type":"stream","name":"stdout","text":["2024-04-25 23:55:22.242318: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","2024-04-25 23:55:22.242373: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","2024-04-25 23:55:22.244068: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","2024-04-25 23:55:23.324846: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","2024-04-25 23:55:27.801612: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:47] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n","/usr/local/lib/python3.10/dist-packages/transformers/deepspeed.py:23: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/omegaconf/grammar_visitor.py:253: UserWarning: In the sequence `MMF_USER_DIR,` some elements are missing: please replace them with empty quoted strings. See https://github.com/omry/omegaconf/issues/572 for details.\n","  warnings.warn(\n","\u001b[32m2024-04-25T23:55:32 | mmf.utils.configuration: \u001b[0mOverriding option config to projects/visual_bert/configs/hateful_memes/defaults.yaml\n","\u001b[32m2024-04-25T23:55:32 | mmf.utils.configuration: \u001b[0mOverriding option model to visual_bert\n","\u001b[32m2024-04-25T23:55:32 | mmf.utils.configuration: \u001b[0mOverriding option datasets to hateful_memes\n","\u001b[32m2024-04-25T23:55:32 | mmf.utils.configuration: \u001b[0mOverriding option run_type to train_val\n","\u001b[32m2024-04-25T23:55:32 | mmf.utils.configuration: \u001b[0mOverriding option training.log_interval to 200\n","\u001b[32m2024-04-25T23:55:32 | mmf.utils.configuration: \u001b[0mOverriding option training.batch_size to 64\n","\u001b[32m2024-04-25T23:55:32 | mmf.utils.configuration: \u001b[0mOverriding option training.evaluation_interval to 200\n","\u001b[32m2024-04-25T23:55:32 | mmf.utils.configuration: \u001b[0mOverriding option training.tensorboard to True\n","\u001b[32m2024-04-25T23:55:32 | mmf.utils.configuration: \u001b[0mOverriding option training.checkpoint_interval to 200\n","\u001b[32m2024-04-25T23:55:32 | mmf.utils.configuration: \u001b[0mOverriding option checkpoint.resume_pretrained to True\n","\u001b[32m2024-04-25T23:55:32 | mmf.utils.configuration: \u001b[0mOverriding option checkpoint.resume_zoo to visual_bert.pretrained.cc\n","\u001b[32m2024-04-25T23:55:32 | mmf.utils.configuration: \u001b[0mOverriding option dataset_config.hateful_memes.annotations.train[0] to hateful_memes/defaults/annotations/train_dev.jsonl\n","\u001b[32m2024-04-25T23:55:32 | mmf.utils.configuration: \u001b[0mOverriding option dataset_config.hateful_memes.annotations.val[0] to hateful_memes/defaults/annotations/dev_unseen.jsonl\n","\u001b[32m2024-04-25T23:55:32 | mmf.utils.configuration: \u001b[0mOverriding option dataset_config.hateful_memes.annotations.test[0] to hateful_memes/defaults/annotations/test_unseen.jsonl\n","/usr/local/lib/python3.10/dist-packages/omegaconf/grammar_visitor.py:253: UserWarning: In the sequence `MMF_LOG_DIR,` some elements are missing: please replace them with empty quoted strings. See https://github.com/omry/omegaconf/issues/572 for details.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/omegaconf/grammar_visitor.py:253: UserWarning: In the sequence `MMF_REPORT_DIR,` some elements are missing: please replace them with empty quoted strings. See https://github.com/omry/omegaconf/issues/572 for details.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/omegaconf/grammar_visitor.py:253: UserWarning: In the sequence `MMF_TENSORBOARD_LOGDIR,` some elements are missing: please replace them with empty quoted strings. See https://github.com/omry/omegaconf/issues/572 for details.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/omegaconf/grammar_visitor.py:253: UserWarning: In the sequence `MMF_WANDB_LOGDIR,` some elements are missing: please replace them with empty quoted strings. See https://github.com/omry/omegaconf/issues/572 for details.\n","  warnings.warn(\n","\u001b[32m2024-04-25T23:55:32 | mmf: \u001b[0mLogging to: ./save/train.log\n","\u001b[32m2024-04-25T23:55:32 | mmf_cli.run: \u001b[0mNamespace(config_override=None, local_rank=None, opts=['config=projects/visual_bert/configs/hateful_memes/defaults.yaml', 'model=visual_bert', 'dataset=hateful_memes', 'run_type=train_val', 'training.log_interval=200', 'training.batch_size=64', 'training.evaluation_interval=200', 'training.tensorboard=True', 'training.checkpoint_interval=200', 'checkpoint.resume_pretrained=True', 'checkpoint.resume_zoo=visual_bert.pretrained.cc', 'dataset_config.hateful_memes.annotations.train[0]=hateful_memes/defaults/annotations/train_dev.jsonl', 'dataset_config.hateful_memes.annotations.val[0]=hateful_memes/defaults/annotations/dev_unseen.jsonl', 'dataset_config.hateful_memes.annotations.test[0]=hateful_memes/defaults/annotations/test_unseen.jsonl'])\n","\u001b[32m2024-04-25T23:55:32 | mmf_cli.run: \u001b[0mTorch version: 2.2.0+cu121\n","\u001b[32m2024-04-25T23:55:32 | mmf.utils.general: \u001b[0mCUDA Device 0 is: NVIDIA A100-SXM4-40GB\n","\u001b[32m2024-04-25T23:55:32 | mmf_cli.run: \u001b[0mUsing seed 32760000\n","\u001b[32m2024-04-25T23:55:32 | mmf.trainers.mmf_trainer: \u001b[0mLoading datasets\n","loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--bert-base-uncased/snapshots/86b5e0934494bd15c9632b12f734a8a67f723594/config.json\n","Model config BertConfig {\n","  \"_name_or_path\": \"bert-base-uncased\",\n","  \"architectures\": [\n","    \"BertForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.39.2\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n","loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--bert-base-uncased/snapshots/86b5e0934494bd15c9632b12f734a8a67f723594/vocab.txt\n","loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--bert-base-uncased/snapshots/86b5e0934494bd15c9632b12f734a8a67f723594/tokenizer.json\n","loading file added_tokens.json from cache at None\n","loading file special_tokens_map.json from cache at None\n","loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--bert-base-uncased/snapshots/86b5e0934494bd15c9632b12f734a8a67f723594/tokenizer_config.json\n","loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--bert-base-uncased/snapshots/86b5e0934494bd15c9632b12f734a8a67f723594/config.json\n","Model config BertConfig {\n","  \"_name_or_path\": \"bert-base-uncased\",\n","  \"architectures\": [\n","    \"BertForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.39.2\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n","\u001b[32m2024-04-25T23:55:33 | mmf.datasets.multi_datamodule: \u001b[0mMultitasking disabled by default for single dataset training\n","\u001b[32m2024-04-25T23:55:33 | mmf.datasets.multi_datamodule: \u001b[0mMultitasking disabled by default for single dataset training\n","\u001b[32m2024-04-25T23:55:33 | mmf.datasets.multi_datamodule: \u001b[0mMultitasking disabled by default for single dataset training\n","\u001b[32m2024-04-25T23:55:33 | mmf.trainers.mmf_trainer: \u001b[0mLoading model\n","Model config BertConfig {\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bert_model_name\": \"bert-base-uncased\",\n","  \"bypass_transformer\": false,\n","  \"classifier_dropout\": null,\n","  \"embedding_strategy\": \"plain\",\n","  \"finetune_lr_multiplier\": 1,\n","  \"freeze_base\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"losses\": [\n","    \"cross_entropy\"\n","  ],\n","  \"max_position_embeddings\": 512,\n","  \"model\": \"visual_bert\",\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"pooler_strategy\": \"default\",\n","  \"position_embedding_type\": \"absolute\",\n","  \"random_initialize\": false,\n","  \"special_visual_initialize\": true,\n","  \"training_head_type\": \"classification\",\n","  \"transformers_version\": \"4.39.2\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"visual_embedding_dim\": 2048,\n","  \"vocab_size\": 30522,\n","  \"zerobias\": false\n","}\n","\n","loading weights file model.safetensors from cache at /root/.cache/torch/mmf/distributed_-1/models--bert-base-uncased/snapshots/86b5e0934494bd15c9632b12f734a8a67f723594/model.safetensors\n","Some weights of the model checkpoint at bert-base-uncased were not used when initializing VisualBERTBase: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n","- This IS expected if you are initializing VisualBERTBase from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing VisualBERTBase from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of VisualBERTBase were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['bert.embeddings.position_embeddings_visual.weight', 'bert.embeddings.projection.bias', 'bert.embeddings.projection.weight', 'bert.embeddings.token_type_embeddings_visual.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","\u001b[32m2024-04-25T23:55:33 | mmf.trainers.mmf_trainer: \u001b[0mLoading optimizer\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2024-04-25T23:55:33 | py.warnings: \u001b[0m/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:457: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n","\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2024-04-25T23:55:33 | py.warnings: \u001b[0m/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:457: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n","\n","\u001b[32m2024-04-25T23:55:33 | mmf.trainers.mmf_trainer: \u001b[0mLoading metrics\n","\u001b[32m2024-04-25T23:55:33 | mmf.utils.checkpoint: \u001b[0mLoading checkpoint\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2024-04-25T23:55:36 | mmf: \u001b[0mKey data_parallel is not present in registry, returning default value of None\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2024-04-25T23:55:36 | mmf: \u001b[0mKey distributed is not present in registry, returning default value of None\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2024-04-25T23:55:36 | mmf: \u001b[0mKey data_parallel is not present in registry, returning default value of None\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2024-04-25T23:55:36 | mmf: \u001b[0mKey distributed is not present in registry, returning default value of None\n","\u001b[32m2024-04-25T23:55:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.word_embeddings.weight from model.bert.embeddings.word_embeddings.weight\n","\u001b[32m2024-04-25T23:55:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.position_embeddings.weight from model.bert.embeddings.position_embeddings.weight\n","\u001b[32m2024-04-25T23:55:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.token_type_embeddings.weight from model.bert.embeddings.token_type_embeddings.weight\n","\u001b[32m2024-04-25T23:55:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.LayerNorm.weight from model.bert.embeddings.LayerNorm.weight\n","\u001b[32m2024-04-25T23:55:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.LayerNorm.bias from model.bert.embeddings.LayerNorm.bias\n","\u001b[32m2024-04-25T23:55:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.token_type_embeddings_visual.weight from model.bert.embeddings.token_type_embeddings_visual.weight\n","\u001b[32m2024-04-25T23:55:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.position_embeddings_visual.weight from model.bert.embeddings.position_embeddings_visual.weight\n","\u001b[32m2024-04-25T23:55:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.projection.weight from model.bert.embeddings.projection.weight\n","\u001b[32m2024-04-25T23:55:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.projection.bias from model.bert.embeddings.projection.bias\n","\u001b[32m2024-04-25T23:55:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.self.query.weight from model.bert.encoder.layer.0.attention.self.query.weight\n","\u001b[32m2024-04-25T23:55:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.self.query.bias from model.bert.encoder.layer.0.attention.self.query.bias\n","\u001b[32m2024-04-25T23:55:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.self.key.weight from model.bert.encoder.layer.0.attention.self.key.weight\n","\u001b[32m2024-04-25T23:55:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.self.key.bias from model.bert.encoder.layer.0.attention.self.key.bias\n","\u001b[32m2024-04-25T23:55:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.self.value.weight from model.bert.encoder.layer.0.attention.self.value.weight\n","\u001b[32m2024-04-25T23:55:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.self.value.bias from model.bert.encoder.layer.0.attention.self.value.bias\n","\u001b[32m2024-04-25T23:55:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.output.dense.weight from model.bert.encoder.layer.0.attention.output.dense.weight\n","\u001b[32m2024-04-25T23:55:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.output.dense.bias from model.bert.encoder.layer.0.attention.output.dense.bias\n","\u001b[32m2024-04-25T23:55:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.output.LayerNorm.weight from model.bert.encoder.layer.0.attention.output.LayerNorm.weight\n","\u001b[32m2024-04-25T23:55:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.output.LayerNorm.bias from model.bert.encoder.layer.0.attention.output.LayerNorm.bias\n","\u001b[32m2024-04-25T23:55:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.intermediate.dense.weight from model.bert.encoder.layer.0.intermediate.dense.weight\n","\u001b[32m2024-04-25T23:55:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.intermediate.dense.bias from model.bert.encoder.layer.0.intermediate.dense.bias\n","\u001b[32m2024-04-25T23:55:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.output.dense.weight from model.bert.encoder.layer.0.output.dense.weight\n","\u001b[32m2024-04-25T23:55:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.output.dense.bias from model.bert.encoder.layer.0.output.dense.bias\n","\u001b[32m2024-04-25T23:55:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.output.LayerNorm.weight from model.bert.encoder.layer.0.output.LayerNorm.weight\n","\u001b[32m2024-04-25T23:55:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.output.LayerNorm.bias from model.bert.encoder.layer.0.output.LayerNorm.bias\n","\u001b[32m2024-04-25T23:55:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.self.query.weight from model.bert.encoder.layer.1.attention.self.query.weight\n","\u001b[32m2024-04-25T23:55:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.self.query.bias from model.bert.encoder.layer.1.attention.self.query.bias\n","\u001b[32m2024-04-25T23:55:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.self.key.weight from model.bert.encoder.layer.1.attention.self.key.weight\n","\u001b[32m2024-04-25T23:55:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.self.key.bias from model.bert.encoder.layer.1.attention.self.key.bias\n","\u001b[32m2024-04-25T23:55:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.self.value.weight from model.bert.encoder.layer.1.attention.self.value.weight\n","\u001b[32m2024-04-25T23:55:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.self.value.bias from model.bert.encoder.layer.1.attention.self.value.bias\n","\u001b[32m2024-04-25T23:55:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.output.dense.weight from model.bert.encoder.layer.1.attention.output.dense.weight\n","\u001b[32m2024-04-25T23:55:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.output.dense.bias from model.bert.encoder.layer.1.attention.output.dense.bias\n","\u001b[32m2024-04-25T23:55:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.output.LayerNorm.weight from model.bert.encoder.layer.1.attention.output.LayerNorm.weight\n","\u001b[32m2024-04-25T23:55:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.output.LayerNorm.bias from model.bert.encoder.layer.1.attention.output.LayerNorm.bias\n","\u001b[32m2024-04-25T23:55:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.intermediate.dense.weight from model.bert.encoder.layer.1.intermediate.dense.weight\n","\u001b[32m2024-04-25T23:55:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.intermediate.dense.bias from model.bert.encoder.layer.1.intermediate.dense.bias\n","\u001b[32m2024-04-25T23:55:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.output.dense.weight from model.bert.encoder.layer.1.output.dense.weight\n","\u001b[32m2024-04-25T23:55:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.output.dense.bias from model.bert.encoder.layer.1.output.dense.bias\n","\u001b[32m2024-04-25T23:55:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.output.LayerNorm.weight from model.bert.encoder.layer.1.output.LayerNorm.weight\n","\u001b[32m2024-04-25T23:55:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.output.LayerNorm.bias from model.bert.encoder.layer.1.output.LayerNorm.bias\n","\u001b[32m2024-04-25T23:55:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.self.query.weight from model.bert.encoder.layer.2.attention.self.query.weight\n","\u001b[32m2024-04-25T23:55:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.self.query.bias from model.bert.encoder.layer.2.attention.self.query.bias\n","\u001b[32m2024-04-25T23:55:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.self.key.weight from model.bert.encoder.layer.2.attention.self.key.weight\n","\u001b[32m2024-04-25T23:55:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.self.key.bias from model.bert.encoder.layer.2.attention.self.key.bias\n","\u001b[32m2024-04-25T23:55:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.self.value.weight from model.bert.encoder.layer.2.attention.self.value.weight\n","\u001b[32m2024-04-25T23:55:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.self.value.bias from model.bert.encoder.layer.2.attention.self.value.bias\n","\u001b[32m2024-04-25T23:55:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.output.dense.weight from model.bert.encoder.layer.2.attention.output.dense.weight\n","\u001b[32m2024-04-25T23:55:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.output.dense.bias from model.bert.encoder.layer.2.attention.output.dense.bias\n","\u001b[32m2024-04-25T23:55:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.output.LayerNorm.weight from model.bert.encoder.layer.2.attention.output.LayerNorm.weight\n","\u001b[32m2024-04-25T23:55:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.output.LayerNorm.bias from model.bert.encoder.layer.2.attention.output.LayerNorm.bias\n","\u001b[32m2024-04-25T23:55:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.intermediate.dense.weight from model.bert.encoder.layer.2.intermediate.dense.weight\n","\u001b[32m2024-04-25T23:55:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.intermediate.dense.bias from model.bert.encoder.layer.2.intermediate.dense.bias\n","\u001b[32m2024-04-25T23:55:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.output.dense.weight from model.bert.encoder.layer.2.output.dense.weight\n","\u001b[32m2024-04-25T23:55:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.output.dense.bias from model.bert.encoder.layer.2.output.dense.bias\n","\u001b[32m2024-04-25T23:55:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.output.LayerNorm.weight from model.bert.encoder.layer.2.output.LayerNorm.weight\n","\u001b[32m2024-04-25T23:55:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.output.LayerNorm.bias from model.bert.encoder.layer.2.output.LayerNorm.bias\n","\u001b[32m2024-04-25T23:55:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.self.query.weight from model.bert.encoder.layer.3.attention.self.query.weight\n","\u001b[32m2024-04-25T23:55:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.self.query.bias from model.bert.encoder.layer.3.attention.self.query.bias\n","\u001b[32m2024-04-25T23:55:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.self.key.weight from model.bert.encoder.layer.3.attention.self.key.weight\n","\u001b[32m2024-04-25T23:55:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.self.key.bias from model.bert.encoder.layer.3.attention.self.key.bias\n","\u001b[32m2024-04-25T23:55:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.self.value.weight from model.bert.encoder.layer.3.attention.self.value.weight\n","\u001b[32m2024-04-25T23:55:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.self.value.bias from model.bert.encoder.layer.3.attention.self.value.bias\n","\u001b[32m2024-04-25T23:55:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.output.dense.weight from model.bert.encoder.layer.3.attention.output.dense.weight\n","\u001b[32m2024-04-25T23:55:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.output.dense.bias from model.bert.encoder.layer.3.attention.output.dense.bias\n","\u001b[32m2024-04-25T23:55:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.output.LayerNorm.weight from model.bert.encoder.layer.3.attention.output.LayerNorm.weight\n","\u001b[32m2024-04-25T23:55:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.output.LayerNorm.bias from model.bert.encoder.layer.3.attention.output.LayerNorm.bias\n","\u001b[32m2024-04-25T23:55:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.intermediate.dense.weight from model.bert.encoder.layer.3.intermediate.dense.weight\n","\u001b[32m2024-04-25T23:55:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.intermediate.dense.bias from model.bert.encoder.layer.3.intermediate.dense.bias\n","\u001b[32m2024-04-25T23:55:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.output.dense.weight from model.bert.encoder.layer.3.output.dense.weight\n","\u001b[32m2024-04-25T23:55:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.output.dense.bias from model.bert.encoder.layer.3.output.dense.bias\n","\u001b[32m2024-04-25T23:55:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.output.LayerNorm.weight from model.bert.encoder.layer.3.output.LayerNorm.weight\n","\u001b[32m2024-04-25T23:55:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.output.LayerNorm.bias from model.bert.encoder.layer.3.output.LayerNorm.bias\n","\u001b[32m2024-04-25T23:55:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.self.query.weight from model.bert.encoder.layer.4.attention.self.query.weight\n","\u001b[32m2024-04-25T23:55:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.self.query.bias from model.bert.encoder.layer.4.attention.self.query.bias\n","\u001b[32m2024-04-25T23:55:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.self.key.weight from model.bert.encoder.layer.4.attention.self.key.weight\n","\u001b[32m2024-04-25T23:55:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.self.key.bias from model.bert.encoder.layer.4.attention.self.key.bias\n","\u001b[32m2024-04-25T23:55:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.self.value.weight from model.bert.encoder.layer.4.attention.self.value.weight\n","\u001b[32m2024-04-25T23:55:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.self.value.bias from model.bert.encoder.layer.4.attention.self.value.bias\n","\u001b[32m2024-04-25T23:55:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.output.dense.weight from model.bert.encoder.layer.4.attention.output.dense.weight\n","\u001b[32m2024-04-25T23:55:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.output.dense.bias from model.bert.encoder.layer.4.attention.output.dense.bias\n","\u001b[32m2024-04-25T23:55:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.output.LayerNorm.weight from model.bert.encoder.layer.4.attention.output.LayerNorm.weight\n","\u001b[32m2024-04-25T23:55:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.output.LayerNorm.bias from model.bert.encoder.layer.4.attention.output.LayerNorm.bias\n","\u001b[32m2024-04-25T23:55:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.intermediate.dense.weight from model.bert.encoder.layer.4.intermediate.dense.weight\n","\u001b[32m2024-04-25T23:55:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.intermediate.dense.bias from model.bert.encoder.layer.4.intermediate.dense.bias\n","\u001b[32m2024-04-25T23:55:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.output.dense.weight from model.bert.encoder.layer.4.output.dense.weight\n","\u001b[32m2024-04-25T23:55:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.output.dense.bias from model.bert.encoder.layer.4.output.dense.bias\n","\u001b[32m2024-04-25T23:55:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.output.LayerNorm.weight from model.bert.encoder.layer.4.output.LayerNorm.weight\n","\u001b[32m2024-04-25T23:55:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.output.LayerNorm.bias from model.bert.encoder.layer.4.output.LayerNorm.bias\n","\u001b[32m2024-04-25T23:55:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.self.query.weight from model.bert.encoder.layer.5.attention.self.query.weight\n","\u001b[32m2024-04-25T23:55:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.self.query.bias from model.bert.encoder.layer.5.attention.self.query.bias\n","\u001b[32m2024-04-25T23:55:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.self.key.weight from model.bert.encoder.layer.5.attention.self.key.weight\n","\u001b[32m2024-04-25T23:55:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.self.key.bias from model.bert.encoder.layer.5.attention.self.key.bias\n","\u001b[32m2024-04-25T23:55:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.self.value.weight from model.bert.encoder.layer.5.attention.self.value.weight\n","\u001b[32m2024-04-25T23:55:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.self.value.bias from model.bert.encoder.layer.5.attention.self.value.bias\n","\u001b[32m2024-04-25T23:55:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.output.dense.weight from model.bert.encoder.layer.5.attention.output.dense.weight\n","\u001b[32m2024-04-25T23:55:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.output.dense.bias from model.bert.encoder.layer.5.attention.output.dense.bias\n","\u001b[32m2024-04-25T23:55:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.output.LayerNorm.weight from model.bert.encoder.layer.5.attention.output.LayerNorm.weight\n","\u001b[32m2024-04-25T23:55:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.output.LayerNorm.bias from model.bert.encoder.layer.5.attention.output.LayerNorm.bias\n","\u001b[32m2024-04-25T23:55:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.intermediate.dense.weight from model.bert.encoder.layer.5.intermediate.dense.weight\n","\u001b[32m2024-04-25T23:55:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.intermediate.dense.bias from model.bert.encoder.layer.5.intermediate.dense.bias\n","\u001b[32m2024-04-25T23:55:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.output.dense.weight from model.bert.encoder.layer.5.output.dense.weight\n","\u001b[32m2024-04-25T23:55:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.output.dense.bias from model.bert.encoder.layer.5.output.dense.bias\n","\u001b[32m2024-04-25T23:55:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.output.LayerNorm.weight from model.bert.encoder.layer.5.output.LayerNorm.weight\n","\u001b[32m2024-04-25T23:55:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.output.LayerNorm.bias from model.bert.encoder.layer.5.output.LayerNorm.bias\n","\u001b[32m2024-04-25T23:55:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.self.query.weight from model.bert.encoder.layer.6.attention.self.query.weight\n","\u001b[32m2024-04-25T23:55:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.self.query.bias from model.bert.encoder.layer.6.attention.self.query.bias\n","\u001b[32m2024-04-25T23:55:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.self.key.weight from model.bert.encoder.layer.6.attention.self.key.weight\n","\u001b[32m2024-04-25T23:55:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.self.key.bias from model.bert.encoder.layer.6.attention.self.key.bias\n","\u001b[32m2024-04-25T23:55:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.self.value.weight from model.bert.encoder.layer.6.attention.self.value.weight\n","\u001b[32m2024-04-25T23:55:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.self.value.bias from model.bert.encoder.layer.6.attention.self.value.bias\n","\u001b[32m2024-04-25T23:55:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.output.dense.weight from model.bert.encoder.layer.6.attention.output.dense.weight\n","\u001b[32m2024-04-25T23:55:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.output.dense.bias from model.bert.encoder.layer.6.attention.output.dense.bias\n","\u001b[32m2024-04-25T23:55:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.output.LayerNorm.weight from model.bert.encoder.layer.6.attention.output.LayerNorm.weight\n","\u001b[32m2024-04-25T23:55:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.output.LayerNorm.bias from model.bert.encoder.layer.6.attention.output.LayerNorm.bias\n","\u001b[32m2024-04-25T23:55:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.intermediate.dense.weight from model.bert.encoder.layer.6.intermediate.dense.weight\n","\u001b[32m2024-04-25T23:55:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.intermediate.dense.bias from model.bert.encoder.layer.6.intermediate.dense.bias\n","\u001b[32m2024-04-25T23:55:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.output.dense.weight from model.bert.encoder.layer.6.output.dense.weight\n","\u001b[32m2024-04-25T23:55:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.output.dense.bias from model.bert.encoder.layer.6.output.dense.bias\n","\u001b[32m2024-04-25T23:55:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.output.LayerNorm.weight from model.bert.encoder.layer.6.output.LayerNorm.weight\n","\u001b[32m2024-04-25T23:55:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.output.LayerNorm.bias from model.bert.encoder.layer.6.output.LayerNorm.bias\n","\u001b[32m2024-04-25T23:55:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.self.query.weight from model.bert.encoder.layer.7.attention.self.query.weight\n","\u001b[32m2024-04-25T23:55:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.self.query.bias from model.bert.encoder.layer.7.attention.self.query.bias\n","\u001b[32m2024-04-25T23:55:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.self.key.weight from model.bert.encoder.layer.7.attention.self.key.weight\n","\u001b[32m2024-04-25T23:55:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.self.key.bias from model.bert.encoder.layer.7.attention.self.key.bias\n","\u001b[32m2024-04-25T23:55:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.self.value.weight from model.bert.encoder.layer.7.attention.self.value.weight\n","\u001b[32m2024-04-25T23:55:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.self.value.bias from model.bert.encoder.layer.7.attention.self.value.bias\n","\u001b[32m2024-04-25T23:55:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.output.dense.weight from model.bert.encoder.layer.7.attention.output.dense.weight\n","\u001b[32m2024-04-25T23:55:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.output.dense.bias from model.bert.encoder.layer.7.attention.output.dense.bias\n","\u001b[32m2024-04-25T23:55:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.output.LayerNorm.weight from model.bert.encoder.layer.7.attention.output.LayerNorm.weight\n","\u001b[32m2024-04-25T23:55:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.output.LayerNorm.bias from model.bert.encoder.layer.7.attention.output.LayerNorm.bias\n","\u001b[32m2024-04-25T23:55:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.intermediate.dense.weight from model.bert.encoder.layer.7.intermediate.dense.weight\n","\u001b[32m2024-04-25T23:55:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.intermediate.dense.bias from model.bert.encoder.layer.7.intermediate.dense.bias\n","\u001b[32m2024-04-25T23:55:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.output.dense.weight from model.bert.encoder.layer.7.output.dense.weight\n","\u001b[32m2024-04-25T23:55:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.output.dense.bias from model.bert.encoder.layer.7.output.dense.bias\n","\u001b[32m2024-04-25T23:55:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.output.LayerNorm.weight from model.bert.encoder.layer.7.output.LayerNorm.weight\n","\u001b[32m2024-04-25T23:55:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.output.LayerNorm.bias from model.bert.encoder.layer.7.output.LayerNorm.bias\n","\u001b[32m2024-04-25T23:55:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.self.query.weight from model.bert.encoder.layer.8.attention.self.query.weight\n","\u001b[32m2024-04-25T23:55:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.self.query.bias from model.bert.encoder.layer.8.attention.self.query.bias\n","\u001b[32m2024-04-25T23:55:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.self.key.weight from model.bert.encoder.layer.8.attention.self.key.weight\n","\u001b[32m2024-04-25T23:55:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.self.key.bias from model.bert.encoder.layer.8.attention.self.key.bias\n","\u001b[32m2024-04-25T23:55:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.self.value.weight from model.bert.encoder.layer.8.attention.self.value.weight\n","\u001b[32m2024-04-25T23:55:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.self.value.bias from model.bert.encoder.layer.8.attention.self.value.bias\n","\u001b[32m2024-04-25T23:55:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.output.dense.weight from model.bert.encoder.layer.8.attention.output.dense.weight\n","\u001b[32m2024-04-25T23:55:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.output.dense.bias from model.bert.encoder.layer.8.attention.output.dense.bias\n","\u001b[32m2024-04-25T23:55:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.output.LayerNorm.weight from model.bert.encoder.layer.8.attention.output.LayerNorm.weight\n","\u001b[32m2024-04-25T23:55:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.output.LayerNorm.bias from model.bert.encoder.layer.8.attention.output.LayerNorm.bias\n","\u001b[32m2024-04-25T23:55:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.intermediate.dense.weight from model.bert.encoder.layer.8.intermediate.dense.weight\n","\u001b[32m2024-04-25T23:55:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.intermediate.dense.bias from model.bert.encoder.layer.8.intermediate.dense.bias\n","\u001b[32m2024-04-25T23:55:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.output.dense.weight from model.bert.encoder.layer.8.output.dense.weight\n","\u001b[32m2024-04-25T23:55:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.output.dense.bias from model.bert.encoder.layer.8.output.dense.bias\n","\u001b[32m2024-04-25T23:55:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.output.LayerNorm.weight from model.bert.encoder.layer.8.output.LayerNorm.weight\n","\u001b[32m2024-04-25T23:55:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.output.LayerNorm.bias from model.bert.encoder.layer.8.output.LayerNorm.bias\n","\u001b[32m2024-04-25T23:55:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.self.query.weight from model.bert.encoder.layer.9.attention.self.query.weight\n","\u001b[32m2024-04-25T23:55:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.self.query.bias from model.bert.encoder.layer.9.attention.self.query.bias\n","\u001b[32m2024-04-25T23:55:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.self.key.weight from model.bert.encoder.layer.9.attention.self.key.weight\n","\u001b[32m2024-04-25T23:55:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.self.key.bias from model.bert.encoder.layer.9.attention.self.key.bias\n","\u001b[32m2024-04-25T23:55:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.self.value.weight from model.bert.encoder.layer.9.attention.self.value.weight\n","\u001b[32m2024-04-25T23:55:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.self.value.bias from model.bert.encoder.layer.9.attention.self.value.bias\n","\u001b[32m2024-04-25T23:55:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.output.dense.weight from model.bert.encoder.layer.9.attention.output.dense.weight\n","\u001b[32m2024-04-25T23:55:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.output.dense.bias from model.bert.encoder.layer.9.attention.output.dense.bias\n","\u001b[32m2024-04-25T23:55:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.output.LayerNorm.weight from model.bert.encoder.layer.9.attention.output.LayerNorm.weight\n","\u001b[32m2024-04-25T23:55:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.output.LayerNorm.bias from model.bert.encoder.layer.9.attention.output.LayerNorm.bias\n","\u001b[32m2024-04-25T23:55:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.intermediate.dense.weight from model.bert.encoder.layer.9.intermediate.dense.weight\n","\u001b[32m2024-04-25T23:55:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.intermediate.dense.bias from model.bert.encoder.layer.9.intermediate.dense.bias\n","\u001b[32m2024-04-25T23:55:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.output.dense.weight from model.bert.encoder.layer.9.output.dense.weight\n","\u001b[32m2024-04-25T23:55:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.output.dense.bias from model.bert.encoder.layer.9.output.dense.bias\n","\u001b[32m2024-04-25T23:55:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.output.LayerNorm.weight from model.bert.encoder.layer.9.output.LayerNorm.weight\n","\u001b[32m2024-04-25T23:55:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.output.LayerNorm.bias from model.bert.encoder.layer.9.output.LayerNorm.bias\n","\u001b[32m2024-04-25T23:55:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.self.query.weight from model.bert.encoder.layer.10.attention.self.query.weight\n","\u001b[32m2024-04-25T23:55:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.self.query.bias from model.bert.encoder.layer.10.attention.self.query.bias\n","\u001b[32m2024-04-25T23:55:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.self.key.weight from model.bert.encoder.layer.10.attention.self.key.weight\n","\u001b[32m2024-04-25T23:55:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.self.key.bias from model.bert.encoder.layer.10.attention.self.key.bias\n","\u001b[32m2024-04-25T23:55:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.self.value.weight from model.bert.encoder.layer.10.attention.self.value.weight\n","\u001b[32m2024-04-25T23:55:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.self.value.bias from model.bert.encoder.layer.10.attention.self.value.bias\n","\u001b[32m2024-04-25T23:55:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.output.dense.weight from model.bert.encoder.layer.10.attention.output.dense.weight\n","\u001b[32m2024-04-25T23:55:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.output.dense.bias from model.bert.encoder.layer.10.attention.output.dense.bias\n","\u001b[32m2024-04-25T23:55:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.output.LayerNorm.weight from model.bert.encoder.layer.10.attention.output.LayerNorm.weight\n","\u001b[32m2024-04-25T23:55:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.output.LayerNorm.bias from model.bert.encoder.layer.10.attention.output.LayerNorm.bias\n","\u001b[32m2024-04-25T23:55:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.intermediate.dense.weight from model.bert.encoder.layer.10.intermediate.dense.weight\n","\u001b[32m2024-04-25T23:55:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.intermediate.dense.bias from model.bert.encoder.layer.10.intermediate.dense.bias\n","\u001b[32m2024-04-25T23:55:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.output.dense.weight from model.bert.encoder.layer.10.output.dense.weight\n","\u001b[32m2024-04-25T23:55:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.output.dense.bias from model.bert.encoder.layer.10.output.dense.bias\n","\u001b[32m2024-04-25T23:55:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.output.LayerNorm.weight from model.bert.encoder.layer.10.output.LayerNorm.weight\n","\u001b[32m2024-04-25T23:55:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.output.LayerNorm.bias from model.bert.encoder.layer.10.output.LayerNorm.bias\n","\u001b[32m2024-04-25T23:55:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.self.query.weight from model.bert.encoder.layer.11.attention.self.query.weight\n","\u001b[32m2024-04-25T23:55:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.self.query.bias from model.bert.encoder.layer.11.attention.self.query.bias\n","\u001b[32m2024-04-25T23:55:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.self.key.weight from model.bert.encoder.layer.11.attention.self.key.weight\n","\u001b[32m2024-04-25T23:55:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.self.key.bias from model.bert.encoder.layer.11.attention.self.key.bias\n","\u001b[32m2024-04-25T23:55:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.self.value.weight from model.bert.encoder.layer.11.attention.self.value.weight\n","\u001b[32m2024-04-25T23:55:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.self.value.bias from model.bert.encoder.layer.11.attention.self.value.bias\n","\u001b[32m2024-04-25T23:55:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.output.dense.weight from model.bert.encoder.layer.11.attention.output.dense.weight\n","\u001b[32m2024-04-25T23:55:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.output.dense.bias from model.bert.encoder.layer.11.attention.output.dense.bias\n","\u001b[32m2024-04-25T23:55:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.output.LayerNorm.weight from model.bert.encoder.layer.11.attention.output.LayerNorm.weight\n","\u001b[32m2024-04-25T23:55:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.output.LayerNorm.bias from model.bert.encoder.layer.11.attention.output.LayerNorm.bias\n","\u001b[32m2024-04-25T23:55:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.intermediate.dense.weight from model.bert.encoder.layer.11.intermediate.dense.weight\n","\u001b[32m2024-04-25T23:55:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.intermediate.dense.bias from model.bert.encoder.layer.11.intermediate.dense.bias\n","\u001b[32m2024-04-25T23:55:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.output.dense.weight from model.bert.encoder.layer.11.output.dense.weight\n","\u001b[32m2024-04-25T23:55:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.output.dense.bias from model.bert.encoder.layer.11.output.dense.bias\n","\u001b[32m2024-04-25T23:55:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.output.LayerNorm.weight from model.bert.encoder.layer.11.output.LayerNorm.weight\n","\u001b[32m2024-04-25T23:55:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.output.LayerNorm.bias from model.bert.encoder.layer.11.output.LayerNorm.bias\n","\u001b[32m2024-04-25T23:55:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.pooler.dense.weight from model.bert.pooler.dense.weight\n","\u001b[32m2024-04-25T23:55:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.pooler.dense.bias from model.bert.pooler.dense.bias\n","\u001b[32m2024-04-25T23:55:36 | mmf.utils.checkpoint: \u001b[0mPretrained model loaded\n","\u001b[32m2024-04-25T23:55:36 | mmf.utils.checkpoint: \u001b[0mCheckpoint loaded.\n","\u001b[32m2024-04-25T23:55:36 | mmf.utils.checkpoint: \u001b[0mCurrent num updates: 0\n","\u001b[32m2024-04-25T23:55:36 | mmf.utils.checkpoint: \u001b[0mCurrent iteration: 0\n","\u001b[32m2024-04-25T23:55:36 | mmf.utils.checkpoint: \u001b[0mCurrent epoch: 0\n","\u001b[32m2024-04-25T23:55:36 | mmf.trainers.mmf_trainer: \u001b[0m===== Model =====\n","\u001b[32m2024-04-25T23:55:36 | mmf.trainers.mmf_trainer: \u001b[0mVisualBERT(\n","  (model): VisualBERTForClassification(\n","    (bert): VisualBERTBase(\n","      (embeddings): BertVisioLinguisticEmbeddings(\n","        (word_embeddings): Embedding(30522, 768, padding_idx=0)\n","        (position_embeddings): Embedding(512, 768)\n","        (token_type_embeddings): Embedding(2, 768)\n","        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","        (token_type_embeddings_visual): Embedding(2, 768)\n","        (position_embeddings_visual): Embedding(512, 768)\n","        (projection): Linear(in_features=2048, out_features=768, bias=True)\n","      )\n","      (encoder): BertEncoderJit(\n","        (layer): ModuleList(\n","          (0-11): 12 x BertLayerJit(\n","            (attention): BertAttentionJit(\n","              (self): BertSelfAttentionJit(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","              (intermediate_act_fn): GELUActivation()\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","        )\n","      )\n","      (pooler): BertPooler(\n","        (dense): Linear(in_features=768, out_features=768, bias=True)\n","        (activation): Tanh()\n","      )\n","    )\n","    (dropout): Dropout(p=0.1, inplace=False)\n","    (classifier): Sequential(\n","      (0): BertPredictionHeadTransform(\n","        (dense): Linear(in_features=768, out_features=768, bias=True)\n","        (transform_act_fn): GELUActivation()\n","        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      )\n","      (1): Linear(in_features=768, out_features=2, bias=True)\n","    )\n","  )\n","  (losses): Losses(\n","    (losses): ModuleList(\n","      (0): MMFLoss(\n","        (loss_criterion): CrossEntropyLoss(\n","          (loss_fn): CrossEntropyLoss()\n","        )\n","      )\n","    )\n","  )\n",")\n","\u001b[32m2024-04-25T23:55:36 | mmf.utils.general: \u001b[0mTotal Parameters: 112044290. Trained Parameters: 112044290\n","\u001b[32m2024-04-25T23:55:36 | mmf.trainers.core.training_loop: \u001b[0mStarting training...\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2024-04-25T23:55:36 | py.warnings: \u001b[0m/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n","  self.pid = os.fork()\n","\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2024-04-25T23:55:36 | py.warnings: \u001b[0m/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n","  self.pid = os.fork()\n","\n","\u001b[32m2024-04-25T23:57:40 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n","\u001b[32m2024-04-25T23:57:40 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2024-04-25T23:57:44 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2024-04-25T23:57:50 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2024-04-25T23:57:50 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 200/1500, train/hateful_memes/cross_entropy: 0.6430, train/hateful_memes/cross_entropy/avg: 0.6430, train/total_loss: 0.6430, train/total_loss/avg: 0.6430, max mem: 14364.0, experiment: run, epoch: 2, num_updates: 200, iterations: 200, max_updates: 1500, lr: 0.00001, ups: 1.49, time: 02m 14s 109ms, time_since_start: 02m 16s 803ms, eta: 15m 10s 940ms\n","\u001b[32m2024-04-25T23:57:50 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n","\u001b[32m2024-04-25T23:57:50 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n","\u001b[32m2024-04-25T23:57:52 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished evaluation inference. Loaded 9\n","\u001b[32m2024-04-25T23:57:52 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n","\u001b[32m2024-04-25T23:57:53 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2024-04-25T23:57:57 | mmf.utils.checkpoint: \u001b[0mSaving best checkpoint\n","\u001b[32m2024-04-25T23:58:00 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2024-04-25T23:58:04 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2024-04-25T23:58:04 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 200/1500, val/hateful_memes/cross_entropy: 0.6569, val/total_loss: 0.6569, val/hateful_memes/accuracy: 0.6296, val/hateful_memes/binary_f1: 0.4286, val/hateful_memes/roc_auc: 0.6172, num_updates: 200, epoch: 2, iterations: 200, max_updates: 1500, val_time: 13s 307ms, best_update: 200, best_iteration: 200, best_val/hateful_memes/roc_auc: 0.617235\n","\u001b[32m2024-04-26T00:00:08 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n","\u001b[32m2024-04-26T00:00:08 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2024-04-26T00:00:12 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2024-04-26T00:00:15 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2024-04-26T00:00:15 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 400/1500, train/hateful_memes/cross_entropy: 0.5585, train/hateful_memes/cross_entropy/avg: 0.6007, train/total_loss: 0.5585, train/total_loss/avg: 0.6007, max mem: 14413.0, experiment: run, epoch: 3, num_updates: 400, iterations: 400, max_updates: 1500, lr: 0.00001, ups: 1.53, time: 02m 11s 573ms, time_since_start: 04m 41s 694ms, eta: 12m 36s 220ms\n","\u001b[32m2024-04-26T00:00:15 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n","\u001b[32m2024-04-26T00:00:15 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2024-04-26T00:00:15 | py.warnings: \u001b[0m/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n","  self.pid = os.fork()\n","\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2024-04-26T00:00:15 | py.warnings: \u001b[0m/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n","  self.pid = os.fork()\n","\n","\u001b[32m2024-04-26T00:00:18 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished evaluation inference. Loaded 9\n","\u001b[32m2024-04-26T00:00:18 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n","\u001b[32m2024-04-26T00:00:18 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2024-04-26T00:00:22 | mmf.utils.checkpoint: \u001b[0mSaving best checkpoint\n","\u001b[32m2024-04-26T00:00:28 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2024-04-26T00:00:34 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2024-04-26T00:00:34 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 400/1500, val/hateful_memes/cross_entropy: 0.6465, val/total_loss: 0.6465, val/hateful_memes/accuracy: 0.6463, val/hateful_memes/binary_f1: 0.4264, val/hateful_memes/roc_auc: 0.6814, num_updates: 400, epoch: 3, iterations: 400, max_updates: 1500, val_time: 19s 201ms, best_update: 400, best_iteration: 400, best_val/hateful_memes/roc_auc: 0.681382\n","\u001b[32m2024-04-26T00:02:39 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n","\u001b[32m2024-04-26T00:02:39 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2024-04-26T00:02:43 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2024-04-26T00:02:46 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2024-04-26T00:02:46 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 600/1500, train/hateful_memes/cross_entropy: 0.5585, train/hateful_memes/cross_entropy/avg: 0.5026, train/total_loss: 0.5585, train/total_loss/avg: 0.5026, max mem: 14413.0, experiment: run, epoch: 5, num_updates: 600, iterations: 600, max_updates: 1500, lr: 0.00002, ups: 1.52, time: 02m 12s 134ms, time_since_start: 07m 13s 031ms, eta: 10m 21s 363ms\n","\u001b[32m2024-04-26T00:02:46 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n","\u001b[32m2024-04-26T00:02:46 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2024-04-26T00:02:46 | py.warnings: \u001b[0m/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n","  self.pid = os.fork()\n","\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2024-04-26T00:02:46 | py.warnings: \u001b[0m/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n","  self.pid = os.fork()\n","\n","\u001b[32m2024-04-26T00:02:49 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished evaluation inference. Loaded 9\n","\u001b[32m2024-04-26T00:02:49 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n","\u001b[32m2024-04-26T00:02:49 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2024-04-26T00:02:53 | mmf.utils.checkpoint: \u001b[0mSaving best checkpoint\n","\u001b[32m2024-04-26T00:02:57 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2024-04-26T00:03:05 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2024-04-26T00:03:05 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 600/1500, val/hateful_memes/cross_entropy: 0.8507, val/total_loss: 0.8507, val/hateful_memes/accuracy: 0.6815, val/hateful_memes/binary_f1: 0.4452, val/hateful_memes/roc_auc: 0.7316, num_updates: 600, epoch: 5, iterations: 600, max_updates: 1500, val_time: 18s 083ms, best_update: 600, best_iteration: 600, best_val/hateful_memes/roc_auc: 0.731574\n","\u001b[32m2024-04-26T00:05:10 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n","\u001b[32m2024-04-26T00:05:10 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2024-04-26T00:05:14 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2024-04-26T00:05:17 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2024-04-26T00:05:17 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 800/1500, train/hateful_memes/cross_entropy: 0.4075, train/hateful_memes/cross_entropy/avg: 0.4788, train/total_loss: 0.4075, train/total_loss/avg: 0.4788, max mem: 14414.0, experiment: run, epoch: 6, num_updates: 800, iterations: 800, max_updates: 1500, lr: 0.00002, ups: 1.52, time: 02m 12s 690ms, time_since_start: 09m 43s 808ms, eta: 08m 05s 317ms\n","\u001b[32m2024-04-26T00:05:17 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n","\u001b[32m2024-04-26T00:05:17 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2024-04-26T00:05:17 | py.warnings: \u001b[0m/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n","  self.pid = os.fork()\n","\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2024-04-26T00:05:17 | py.warnings: \u001b[0m/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n","  self.pid = os.fork()\n","\n","\u001b[32m2024-04-26T00:05:20 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished evaluation inference. Loaded 9\n","\u001b[32m2024-04-26T00:05:20 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n","\u001b[32m2024-04-26T00:05:23 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2024-04-26T00:05:27 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2024-04-26T00:05:31 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2024-04-26T00:05:31 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 800/1500, val/hateful_memes/cross_entropy: 0.7956, val/total_loss: 0.7956, val/hateful_memes/accuracy: 0.6944, val/hateful_memes/binary_f1: 0.5646, val/hateful_memes/roc_auc: 0.7260, num_updates: 800, epoch: 6, iterations: 800, max_updates: 1500, val_time: 13s 931ms, best_update: 600, best_iteration: 600, best_val/hateful_memes/roc_auc: 0.731574\n","\u001b[32m2024-04-26T00:07:36 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n","\u001b[32m2024-04-26T00:07:36 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2024-04-26T00:07:39 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2024-04-26T00:07:44 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2024-04-26T00:07:44 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1000/1500, train/hateful_memes/cross_entropy: 0.4075, train/hateful_memes/cross_entropy/avg: 0.4089, train/total_loss: 0.4075, train/total_loss/avg: 0.4089, max mem: 14414.0, experiment: run, epoch: 8, num_updates: 1000, iterations: 1000, max_updates: 1500, lr: 0.00003, ups: 1.50, time: 02m 13s 086ms, time_since_start: 12m 10s 828ms, eta: 05m 47s 687ms\n","\u001b[32m2024-04-26T00:07:44 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n","\u001b[32m2024-04-26T00:07:44 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2024-04-26T00:07:44 | py.warnings: \u001b[0m/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n","  self.pid = os.fork()\n","\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2024-04-26T00:07:44 | py.warnings: \u001b[0m/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n","  self.pid = os.fork()\n","\n","\u001b[32m2024-04-26T00:07:47 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished evaluation inference. Loaded 9\n","\u001b[32m2024-04-26T00:07:47 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n","\u001b[32m2024-04-26T00:07:47 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2024-04-26T00:07:50 | mmf.utils.checkpoint: \u001b[0mSaving best checkpoint\n","\u001b[32m2024-04-26T00:07:54 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2024-04-26T00:08:00 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2024-04-26T00:08:00 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1000/1500, val/hateful_memes/cross_entropy: 0.9715, val/total_loss: 0.9715, val/hateful_memes/accuracy: 0.7019, val/hateful_memes/binary_f1: 0.5251, val/hateful_memes/roc_auc: 0.7317, num_updates: 1000, epoch: 8, iterations: 1000, max_updates: 1500, val_time: 15s 931ms, best_update: 1000, best_iteration: 1000, best_val/hateful_memes/roc_auc: 0.731691\n","\u001b[32m2024-04-26T00:10:05 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n","\u001b[32m2024-04-26T00:10:05 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2024-04-26T00:10:09 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2024-04-26T00:10:15 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2024-04-26T00:10:15 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1200/1500, train/hateful_memes/cross_entropy: 0.3062, train/hateful_memes/cross_entropy/avg: 0.3549, train/total_loss: 0.3062, train/total_loss/avg: 0.3549, max mem: 14414.0, experiment: run, epoch: 9, num_updates: 1200, iterations: 1200, max_updates: 1500, lr: 0.00003, ups: 1.48, time: 02m 15s 113ms, time_since_start: 14m 41s 875ms, eta: 03m 31s 790ms\n","\u001b[32m2024-04-26T00:10:15 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n","\u001b[32m2024-04-26T00:10:15 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2024-04-26T00:10:15 | py.warnings: \u001b[0m/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n","  self.pid = os.fork()\n","\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2024-04-26T00:10:15 | py.warnings: \u001b[0m/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n","  self.pid = os.fork()\n","\n","\u001b[32m2024-04-26T00:10:18 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished evaluation inference. Loaded 9\n","\u001b[32m2024-04-26T00:10:18 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n","\u001b[32m2024-04-26T00:10:18 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2024-04-26T00:10:21 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2024-04-26T00:10:25 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2024-04-26T00:10:25 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1200/1500, val/hateful_memes/cross_entropy: 1.1049, val/total_loss: 1.1049, val/hateful_memes/accuracy: 0.6981, val/hateful_memes/binary_f1: 0.4792, val/hateful_memes/roc_auc: 0.7314, num_updates: 1200, epoch: 9, iterations: 1200, max_updates: 1500, val_time: 09s 940ms, best_update: 1000, best_iteration: 1000, best_val/hateful_memes/roc_auc: 0.731691\n","\u001b[32m2024-04-26T00:12:30 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n","\u001b[32m2024-04-26T00:12:30 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2024-04-26T00:12:34 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2024-04-26T00:12:40 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2024-04-26T00:12:40 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1400/1500, train/hateful_memes/cross_entropy: 0.3062, train/hateful_memes/cross_entropy/avg: 0.3221, train/total_loss: 0.3062, train/total_loss/avg: 0.3221, max mem: 14414.0, experiment: run, epoch: 11, num_updates: 1400, iterations: 1400, max_updates: 1500, lr: 0.00003, ups: 1.49, time: 02m 14s 357ms, time_since_start: 17m 06s 175ms, eta: 01m 10s 201ms\n","\u001b[32m2024-04-26T00:12:40 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n","\u001b[32m2024-04-26T00:12:40 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2024-04-26T00:12:40 | py.warnings: \u001b[0m/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n","  self.pid = os.fork()\n","\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2024-04-26T00:12:40 | py.warnings: \u001b[0m/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n","  self.pid = os.fork()\n","\n","\u001b[32m2024-04-26T00:12:42 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished evaluation inference. Loaded 9\n","\u001b[32m2024-04-26T00:12:42 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n","\u001b[32m2024-04-26T00:12:42 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2024-04-26T00:12:46 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2024-04-26T00:12:50 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2024-04-26T00:12:50 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1400/1500, val/hateful_memes/cross_entropy: 1.2402, val/total_loss: 1.2402, val/hateful_memes/accuracy: 0.6981, val/hateful_memes/binary_f1: 0.5248, val/hateful_memes/roc_auc: 0.7242, num_updates: 1400, epoch: 11, iterations: 1400, max_updates: 1500, val_time: 10s 193ms, best_update: 1000, best_iteration: 1000, best_val/hateful_memes/roc_auc: 0.731691\n","\u001b[32m2024-04-26T00:13:52 | mmf.trainers.core.training_loop: \u001b[0mStepping into final validation check\n","\u001b[32m2024-04-26T00:13:52 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2024-04-26T00:13:52 | py.warnings: \u001b[0m/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n","  self.pid = os.fork()\n","\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2024-04-26T00:13:52 | py.warnings: \u001b[0m/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n","  self.pid = os.fork()\n","\n","\u001b[32m2024-04-26T00:13:55 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished evaluation inference. Loaded 9\n","\u001b[32m2024-04-26T00:13:55 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n","\u001b[32m2024-04-26T00:13:55 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1500/1500, val/hateful_memes/cross_entropy: 1.2556, val/total_loss: 1.2556, val/hateful_memes/accuracy: 0.6556, val/hateful_memes/binary_f1: 0.5550, val/hateful_memes/roc_auc: 0.7163, num_updates: 1500, epoch: 12, iterations: 1500, max_updates: 1500, val_time: 01m 15s 251ms, best_update: 1000, best_iteration: 1000, best_val/hateful_memes/roc_auc: 0.731691\n","\u001b[32m2024-04-26T00:13:55 | mmf.utils.checkpoint: \u001b[0mRestoring checkpoint\n","\u001b[32m2024-04-26T00:13:55 | mmf.utils.checkpoint: \u001b[0mLoading checkpoint\n","\u001b[32m2024-04-26T00:13:56 | mmf.utils.checkpoint: \u001b[0mCheckpoint loaded.\n","\u001b[32m2024-04-26T00:13:56 | mmf.utils.checkpoint: \u001b[0mCurrent num updates: 1000\n","\u001b[32m2024-04-26T00:13:56 | mmf.utils.checkpoint: \u001b[0mCurrent iteration: 1000\n","\u001b[32m2024-04-26T00:13:56 | mmf.utils.checkpoint: \u001b[0mCurrent epoch: 8\n","\u001b[32m2024-04-26T00:13:58 | mmf.trainers.mmf_trainer: \u001b[0mStarting inference on val set\n","\u001b[32m2024-04-26T00:13:58 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n","  0% 0/9 [00:00<?, ?it/s]\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2024-04-26T00:13:58 | py.warnings: \u001b[0m/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n","  self.pid = os.fork()\n","\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2024-04-26T00:13:58 | py.warnings: \u001b[0m/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n","  self.pid = os.fork()\n","\n","100% 9/9 [00:02<00:00,  3.49it/s]\n","\u001b[32m2024-04-26T00:14:00 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished evaluation inference. Loaded 9\n","\u001b[32m2024-04-26T00:14:00 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n","\u001b[32m2024-04-26T00:14:01 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1000/1500, val/hateful_memes/cross_entropy: 0.9715, val/total_loss: 0.9715, val/hateful_memes/accuracy: 0.7019, val/hateful_memes/binary_f1: 0.5251, val/hateful_memes/roc_auc: 0.7317\n","\u001b[32m2024-04-26T00:14:01 | mmf.trainers.callbacks.logistics: \u001b[0mFinished run in 18m 27s 407ms\n"]}]},{"cell_type":"markdown","source":["### Dataset: Augmented with partial transformations"],"metadata":{"id":"afXZ1LIaYASl"}},{"cell_type":"code","source":["!mmf_run config=\"projects/visual_bert/configs/hateful_memes/defaults.yaml\" \\\n","  model=visual_bert \\\n","  dataset=hateful_memes \\\n","  run_type=train_val \\\n","  training.log_interval=200 \\\n","  training.batch_size=64 \\\n","  training.evaluation_interval=200 \\\n","  training.tensorboard=True \\\n","  training.checkpoint_interval=200 \\\n","  checkpoint.resume_pretrained=True \\\n","  checkpoint.resume_zoo=visual_bert.pretrained.cc \\\n","  dataset_config.hateful_memes.annotations.train[0]=\"hateful_memes/defaults/annotations/train_dev_transform_half.jsonl\" \\\n","  dataset_config.hateful_memes.annotations.val[0]=\"hateful_memes/defaults/annotations/dev_unseen.jsonl\" \\\n","  dataset_config.hateful_memes.annotations.test[0]=\"hateful_memes/defaults/annotations/test_unseen.jsonl\""],"metadata":{"id":"UrnxjaO1VoXn","colab":{"base_uri":"https://localhost:8080/"},"outputId":"9d100e10-5585-478c-cb95-7b50a6ac82d6","executionInfo":{"status":"ok","timestamp":1714091708573,"user_tz":420,"elapsed":1126250,"user":{"displayName":"Patrick Prestridge","userId":"01452612983786479124"}}},"execution_count":39,"outputs":[{"output_type":"stream","name":"stdout","text":["2024-04-26 00:16:25.366655: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","2024-04-26 00:16:25.366712: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","2024-04-26 00:16:25.368375: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","2024-04-26 00:16:26.519990: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","2024-04-26 00:16:31.620813: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:47] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n","/usr/local/lib/python3.10/dist-packages/transformers/deepspeed.py:23: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/omegaconf/grammar_visitor.py:253: UserWarning: In the sequence `MMF_USER_DIR,` some elements are missing: please replace them with empty quoted strings. See https://github.com/omry/omegaconf/issues/572 for details.\n","  warnings.warn(\n","\u001b[32m2024-04-26T00:16:36 | mmf.utils.configuration: \u001b[0mOverriding option config to projects/visual_bert/configs/hateful_memes/defaults.yaml\n","\u001b[32m2024-04-26T00:16:36 | mmf.utils.configuration: \u001b[0mOverriding option model to visual_bert\n","\u001b[32m2024-04-26T00:16:36 | mmf.utils.configuration: \u001b[0mOverriding option datasets to hateful_memes\n","\u001b[32m2024-04-26T00:16:36 | mmf.utils.configuration: \u001b[0mOverriding option run_type to train_val\n","\u001b[32m2024-04-26T00:16:36 | mmf.utils.configuration: \u001b[0mOverriding option training.log_interval to 200\n","\u001b[32m2024-04-26T00:16:36 | mmf.utils.configuration: \u001b[0mOverriding option training.batch_size to 64\n","\u001b[32m2024-04-26T00:16:36 | mmf.utils.configuration: \u001b[0mOverriding option training.evaluation_interval to 200\n","\u001b[32m2024-04-26T00:16:36 | mmf.utils.configuration: \u001b[0mOverriding option training.tensorboard to True\n","\u001b[32m2024-04-26T00:16:36 | mmf.utils.configuration: \u001b[0mOverriding option training.checkpoint_interval to 200\n","\u001b[32m2024-04-26T00:16:36 | mmf.utils.configuration: \u001b[0mOverriding option checkpoint.resume_pretrained to True\n","\u001b[32m2024-04-26T00:16:36 | mmf.utils.configuration: \u001b[0mOverriding option checkpoint.resume_zoo to visual_bert.pretrained.cc\n","\u001b[32m2024-04-26T00:16:36 | mmf.utils.configuration: \u001b[0mOverriding option dataset_config.hateful_memes.annotations.train[0] to hateful_memes/defaults/annotations/train_dev_transform_half.jsonl\n","\u001b[32m2024-04-26T00:16:36 | mmf.utils.configuration: \u001b[0mOverriding option dataset_config.hateful_memes.annotations.val[0] to hateful_memes/defaults/annotations/dev_unseen.jsonl\n","\u001b[32m2024-04-26T00:16:36 | mmf.utils.configuration: \u001b[0mOverriding option dataset_config.hateful_memes.annotations.test[0] to hateful_memes/defaults/annotations/test_unseen.jsonl\n","/usr/local/lib/python3.10/dist-packages/omegaconf/grammar_visitor.py:253: UserWarning: In the sequence `MMF_LOG_DIR,` some elements are missing: please replace them with empty quoted strings. See https://github.com/omry/omegaconf/issues/572 for details.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/omegaconf/grammar_visitor.py:253: UserWarning: In the sequence `MMF_REPORT_DIR,` some elements are missing: please replace them with empty quoted strings. See https://github.com/omry/omegaconf/issues/572 for details.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/omegaconf/grammar_visitor.py:253: UserWarning: In the sequence `MMF_TENSORBOARD_LOGDIR,` some elements are missing: please replace them with empty quoted strings. See https://github.com/omry/omegaconf/issues/572 for details.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/omegaconf/grammar_visitor.py:253: UserWarning: In the sequence `MMF_WANDB_LOGDIR,` some elements are missing: please replace them with empty quoted strings. See https://github.com/omry/omegaconf/issues/572 for details.\n","  warnings.warn(\n","\u001b[32m2024-04-26T00:16:36 | mmf: \u001b[0mLogging to: ./save/train.log\n","\u001b[32m2024-04-26T00:16:36 | mmf_cli.run: \u001b[0mNamespace(config_override=None, local_rank=None, opts=['config=projects/visual_bert/configs/hateful_memes/defaults.yaml', 'model=visual_bert', 'dataset=hateful_memes', 'run_type=train_val', 'training.log_interval=200', 'training.batch_size=64', 'training.evaluation_interval=200', 'training.tensorboard=True', 'training.checkpoint_interval=200', 'checkpoint.resume_pretrained=True', 'checkpoint.resume_zoo=visual_bert.pretrained.cc', 'dataset_config.hateful_memes.annotations.train[0]=hateful_memes/defaults/annotations/train_dev_transform_half.jsonl', 'dataset_config.hateful_memes.annotations.val[0]=hateful_memes/defaults/annotations/dev_unseen.jsonl', 'dataset_config.hateful_memes.annotations.test[0]=hateful_memes/defaults/annotations/test_unseen.jsonl'])\n","\u001b[32m2024-04-26T00:16:36 | mmf_cli.run: \u001b[0mTorch version: 2.2.0+cu121\n","\u001b[32m2024-04-26T00:16:36 | mmf.utils.general: \u001b[0mCUDA Device 0 is: NVIDIA A100-SXM4-40GB\n","\u001b[32m2024-04-26T00:16:36 | mmf_cli.run: \u001b[0mUsing seed 36677849\n","\u001b[32m2024-04-26T00:16:36 | mmf.trainers.mmf_trainer: \u001b[0mLoading datasets\n","loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--bert-base-uncased/snapshots/86b5e0934494bd15c9632b12f734a8a67f723594/config.json\n","Model config BertConfig {\n","  \"_name_or_path\": \"bert-base-uncased\",\n","  \"architectures\": [\n","    \"BertForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.39.2\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n","loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--bert-base-uncased/snapshots/86b5e0934494bd15c9632b12f734a8a67f723594/vocab.txt\n","loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--bert-base-uncased/snapshots/86b5e0934494bd15c9632b12f734a8a67f723594/tokenizer.json\n","loading file added_tokens.json from cache at None\n","loading file special_tokens_map.json from cache at None\n","loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--bert-base-uncased/snapshots/86b5e0934494bd15c9632b12f734a8a67f723594/tokenizer_config.json\n","loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--bert-base-uncased/snapshots/86b5e0934494bd15c9632b12f734a8a67f723594/config.json\n","Model config BertConfig {\n","  \"_name_or_path\": \"bert-base-uncased\",\n","  \"architectures\": [\n","    \"BertForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.39.2\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n","\u001b[32m2024-04-26T00:16:37 | mmf.datasets.multi_datamodule: \u001b[0mMultitasking disabled by default for single dataset training\n","\u001b[32m2024-04-26T00:16:37 | mmf.datasets.multi_datamodule: \u001b[0mMultitasking disabled by default for single dataset training\n","\u001b[32m2024-04-26T00:16:37 | mmf.datasets.multi_datamodule: \u001b[0mMultitasking disabled by default for single dataset training\n","\u001b[32m2024-04-26T00:16:37 | mmf.trainers.mmf_trainer: \u001b[0mLoading model\n","Model config BertConfig {\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bert_model_name\": \"bert-base-uncased\",\n","  \"bypass_transformer\": false,\n","  \"classifier_dropout\": null,\n","  \"embedding_strategy\": \"plain\",\n","  \"finetune_lr_multiplier\": 1,\n","  \"freeze_base\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"losses\": [\n","    \"cross_entropy\"\n","  ],\n","  \"max_position_embeddings\": 512,\n","  \"model\": \"visual_bert\",\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"pooler_strategy\": \"default\",\n","  \"position_embedding_type\": \"absolute\",\n","  \"random_initialize\": false,\n","  \"special_visual_initialize\": true,\n","  \"training_head_type\": \"classification\",\n","  \"transformers_version\": \"4.39.2\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"visual_embedding_dim\": 2048,\n","  \"vocab_size\": 30522,\n","  \"zerobias\": false\n","}\n","\n","loading weights file model.safetensors from cache at /root/.cache/torch/mmf/distributed_-1/models--bert-base-uncased/snapshots/86b5e0934494bd15c9632b12f734a8a67f723594/model.safetensors\n","Some weights of the model checkpoint at bert-base-uncased were not used when initializing VisualBERTBase: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n","- This IS expected if you are initializing VisualBERTBase from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing VisualBERTBase from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of VisualBERTBase were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['bert.embeddings.position_embeddings_visual.weight', 'bert.embeddings.projection.bias', 'bert.embeddings.projection.weight', 'bert.embeddings.token_type_embeddings_visual.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","\u001b[32m2024-04-26T00:16:37 | mmf.trainers.mmf_trainer: \u001b[0mLoading optimizer\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2024-04-26T00:16:37 | py.warnings: \u001b[0m/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:457: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n","\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2024-04-26T00:16:37 | py.warnings: \u001b[0m/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:457: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n","\n","\u001b[32m2024-04-26T00:16:37 | mmf.trainers.mmf_trainer: \u001b[0mLoading metrics\n","\u001b[32m2024-04-26T00:16:37 | mmf.utils.checkpoint: \u001b[0mLoading checkpoint\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2024-04-26T00:16:38 | mmf: \u001b[0mKey data_parallel is not present in registry, returning default value of None\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2024-04-26T00:16:38 | mmf: \u001b[0mKey distributed is not present in registry, returning default value of None\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2024-04-26T00:16:38 | mmf: \u001b[0mKey data_parallel is not present in registry, returning default value of None\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2024-04-26T00:16:38 | mmf: \u001b[0mKey distributed is not present in registry, returning default value of None\n","\u001b[32m2024-04-26T00:16:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.word_embeddings.weight from model.bert.embeddings.word_embeddings.weight\n","\u001b[32m2024-04-26T00:16:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.position_embeddings.weight from model.bert.embeddings.position_embeddings.weight\n","\u001b[32m2024-04-26T00:16:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.token_type_embeddings.weight from model.bert.embeddings.token_type_embeddings.weight\n","\u001b[32m2024-04-26T00:16:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.LayerNorm.weight from model.bert.embeddings.LayerNorm.weight\n","\u001b[32m2024-04-26T00:16:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.LayerNorm.bias from model.bert.embeddings.LayerNorm.bias\n","\u001b[32m2024-04-26T00:16:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.token_type_embeddings_visual.weight from model.bert.embeddings.token_type_embeddings_visual.weight\n","\u001b[32m2024-04-26T00:16:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.position_embeddings_visual.weight from model.bert.embeddings.position_embeddings_visual.weight\n","\u001b[32m2024-04-26T00:16:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.projection.weight from model.bert.embeddings.projection.weight\n","\u001b[32m2024-04-26T00:16:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.projection.bias from model.bert.embeddings.projection.bias\n","\u001b[32m2024-04-26T00:16:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.self.query.weight from model.bert.encoder.layer.0.attention.self.query.weight\n","\u001b[32m2024-04-26T00:16:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.self.query.bias from model.bert.encoder.layer.0.attention.self.query.bias\n","\u001b[32m2024-04-26T00:16:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.self.key.weight from model.bert.encoder.layer.0.attention.self.key.weight\n","\u001b[32m2024-04-26T00:16:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.self.key.bias from model.bert.encoder.layer.0.attention.self.key.bias\n","\u001b[32m2024-04-26T00:16:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.self.value.weight from model.bert.encoder.layer.0.attention.self.value.weight\n","\u001b[32m2024-04-26T00:16:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.self.value.bias from model.bert.encoder.layer.0.attention.self.value.bias\n","\u001b[32m2024-04-26T00:16:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.output.dense.weight from model.bert.encoder.layer.0.attention.output.dense.weight\n","\u001b[32m2024-04-26T00:16:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.output.dense.bias from model.bert.encoder.layer.0.attention.output.dense.bias\n","\u001b[32m2024-04-26T00:16:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.output.LayerNorm.weight from model.bert.encoder.layer.0.attention.output.LayerNorm.weight\n","\u001b[32m2024-04-26T00:16:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.output.LayerNorm.bias from model.bert.encoder.layer.0.attention.output.LayerNorm.bias\n","\u001b[32m2024-04-26T00:16:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.intermediate.dense.weight from model.bert.encoder.layer.0.intermediate.dense.weight\n","\u001b[32m2024-04-26T00:16:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.intermediate.dense.bias from model.bert.encoder.layer.0.intermediate.dense.bias\n","\u001b[32m2024-04-26T00:16:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.output.dense.weight from model.bert.encoder.layer.0.output.dense.weight\n","\u001b[32m2024-04-26T00:16:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.output.dense.bias from model.bert.encoder.layer.0.output.dense.bias\n","\u001b[32m2024-04-26T00:16:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.output.LayerNorm.weight from model.bert.encoder.layer.0.output.LayerNorm.weight\n","\u001b[32m2024-04-26T00:16:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.output.LayerNorm.bias from model.bert.encoder.layer.0.output.LayerNorm.bias\n","\u001b[32m2024-04-26T00:16:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.self.query.weight from model.bert.encoder.layer.1.attention.self.query.weight\n","\u001b[32m2024-04-26T00:16:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.self.query.bias from model.bert.encoder.layer.1.attention.self.query.bias\n","\u001b[32m2024-04-26T00:16:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.self.key.weight from model.bert.encoder.layer.1.attention.self.key.weight\n","\u001b[32m2024-04-26T00:16:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.self.key.bias from model.bert.encoder.layer.1.attention.self.key.bias\n","\u001b[32m2024-04-26T00:16:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.self.value.weight from model.bert.encoder.layer.1.attention.self.value.weight\n","\u001b[32m2024-04-26T00:16:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.self.value.bias from model.bert.encoder.layer.1.attention.self.value.bias\n","\u001b[32m2024-04-26T00:16:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.output.dense.weight from model.bert.encoder.layer.1.attention.output.dense.weight\n","\u001b[32m2024-04-26T00:16:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.output.dense.bias from model.bert.encoder.layer.1.attention.output.dense.bias\n","\u001b[32m2024-04-26T00:16:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.output.LayerNorm.weight from model.bert.encoder.layer.1.attention.output.LayerNorm.weight\n","\u001b[32m2024-04-26T00:16:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.output.LayerNorm.bias from model.bert.encoder.layer.1.attention.output.LayerNorm.bias\n","\u001b[32m2024-04-26T00:16:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.intermediate.dense.weight from model.bert.encoder.layer.1.intermediate.dense.weight\n","\u001b[32m2024-04-26T00:16:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.intermediate.dense.bias from model.bert.encoder.layer.1.intermediate.dense.bias\n","\u001b[32m2024-04-26T00:16:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.output.dense.weight from model.bert.encoder.layer.1.output.dense.weight\n","\u001b[32m2024-04-26T00:16:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.output.dense.bias from model.bert.encoder.layer.1.output.dense.bias\n","\u001b[32m2024-04-26T00:16:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.output.LayerNorm.weight from model.bert.encoder.layer.1.output.LayerNorm.weight\n","\u001b[32m2024-04-26T00:16:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.output.LayerNorm.bias from model.bert.encoder.layer.1.output.LayerNorm.bias\n","\u001b[32m2024-04-26T00:16:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.self.query.weight from model.bert.encoder.layer.2.attention.self.query.weight\n","\u001b[32m2024-04-26T00:16:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.self.query.bias from model.bert.encoder.layer.2.attention.self.query.bias\n","\u001b[32m2024-04-26T00:16:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.self.key.weight from model.bert.encoder.layer.2.attention.self.key.weight\n","\u001b[32m2024-04-26T00:16:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.self.key.bias from model.bert.encoder.layer.2.attention.self.key.bias\n","\u001b[32m2024-04-26T00:16:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.self.value.weight from model.bert.encoder.layer.2.attention.self.value.weight\n","\u001b[32m2024-04-26T00:16:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.self.value.bias from model.bert.encoder.layer.2.attention.self.value.bias\n","\u001b[32m2024-04-26T00:16:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.output.dense.weight from model.bert.encoder.layer.2.attention.output.dense.weight\n","\u001b[32m2024-04-26T00:16:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.output.dense.bias from model.bert.encoder.layer.2.attention.output.dense.bias\n","\u001b[32m2024-04-26T00:16:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.output.LayerNorm.weight from model.bert.encoder.layer.2.attention.output.LayerNorm.weight\n","\u001b[32m2024-04-26T00:16:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.output.LayerNorm.bias from model.bert.encoder.layer.2.attention.output.LayerNorm.bias\n","\u001b[32m2024-04-26T00:16:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.intermediate.dense.weight from model.bert.encoder.layer.2.intermediate.dense.weight\n","\u001b[32m2024-04-26T00:16:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.intermediate.dense.bias from model.bert.encoder.layer.2.intermediate.dense.bias\n","\u001b[32m2024-04-26T00:16:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.output.dense.weight from model.bert.encoder.layer.2.output.dense.weight\n","\u001b[32m2024-04-26T00:16:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.output.dense.bias from model.bert.encoder.layer.2.output.dense.bias\n","\u001b[32m2024-04-26T00:16:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.output.LayerNorm.weight from model.bert.encoder.layer.2.output.LayerNorm.weight\n","\u001b[32m2024-04-26T00:16:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.output.LayerNorm.bias from model.bert.encoder.layer.2.output.LayerNorm.bias\n","\u001b[32m2024-04-26T00:16:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.self.query.weight from model.bert.encoder.layer.3.attention.self.query.weight\n","\u001b[32m2024-04-26T00:16:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.self.query.bias from model.bert.encoder.layer.3.attention.self.query.bias\n","\u001b[32m2024-04-26T00:16:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.self.key.weight from model.bert.encoder.layer.3.attention.self.key.weight\n","\u001b[32m2024-04-26T00:16:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.self.key.bias from model.bert.encoder.layer.3.attention.self.key.bias\n","\u001b[32m2024-04-26T00:16:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.self.value.weight from model.bert.encoder.layer.3.attention.self.value.weight\n","\u001b[32m2024-04-26T00:16:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.self.value.bias from model.bert.encoder.layer.3.attention.self.value.bias\n","\u001b[32m2024-04-26T00:16:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.output.dense.weight from model.bert.encoder.layer.3.attention.output.dense.weight\n","\u001b[32m2024-04-26T00:16:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.output.dense.bias from model.bert.encoder.layer.3.attention.output.dense.bias\n","\u001b[32m2024-04-26T00:16:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.output.LayerNorm.weight from model.bert.encoder.layer.3.attention.output.LayerNorm.weight\n","\u001b[32m2024-04-26T00:16:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.output.LayerNorm.bias from model.bert.encoder.layer.3.attention.output.LayerNorm.bias\n","\u001b[32m2024-04-26T00:16:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.intermediate.dense.weight from model.bert.encoder.layer.3.intermediate.dense.weight\n","\u001b[32m2024-04-26T00:16:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.intermediate.dense.bias from model.bert.encoder.layer.3.intermediate.dense.bias\n","\u001b[32m2024-04-26T00:16:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.output.dense.weight from model.bert.encoder.layer.3.output.dense.weight\n","\u001b[32m2024-04-26T00:16:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.output.dense.bias from model.bert.encoder.layer.3.output.dense.bias\n","\u001b[32m2024-04-26T00:16:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.output.LayerNorm.weight from model.bert.encoder.layer.3.output.LayerNorm.weight\n","\u001b[32m2024-04-26T00:16:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.output.LayerNorm.bias from model.bert.encoder.layer.3.output.LayerNorm.bias\n","\u001b[32m2024-04-26T00:16:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.self.query.weight from model.bert.encoder.layer.4.attention.self.query.weight\n","\u001b[32m2024-04-26T00:16:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.self.query.bias from model.bert.encoder.layer.4.attention.self.query.bias\n","\u001b[32m2024-04-26T00:16:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.self.key.weight from model.bert.encoder.layer.4.attention.self.key.weight\n","\u001b[32m2024-04-26T00:16:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.self.key.bias from model.bert.encoder.layer.4.attention.self.key.bias\n","\u001b[32m2024-04-26T00:16:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.self.value.weight from model.bert.encoder.layer.4.attention.self.value.weight\n","\u001b[32m2024-04-26T00:16:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.self.value.bias from model.bert.encoder.layer.4.attention.self.value.bias\n","\u001b[32m2024-04-26T00:16:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.output.dense.weight from model.bert.encoder.layer.4.attention.output.dense.weight\n","\u001b[32m2024-04-26T00:16:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.output.dense.bias from model.bert.encoder.layer.4.attention.output.dense.bias\n","\u001b[32m2024-04-26T00:16:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.output.LayerNorm.weight from model.bert.encoder.layer.4.attention.output.LayerNorm.weight\n","\u001b[32m2024-04-26T00:16:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.output.LayerNorm.bias from model.bert.encoder.layer.4.attention.output.LayerNorm.bias\n","\u001b[32m2024-04-26T00:16:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.intermediate.dense.weight from model.bert.encoder.layer.4.intermediate.dense.weight\n","\u001b[32m2024-04-26T00:16:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.intermediate.dense.bias from model.bert.encoder.layer.4.intermediate.dense.bias\n","\u001b[32m2024-04-26T00:16:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.output.dense.weight from model.bert.encoder.layer.4.output.dense.weight\n","\u001b[32m2024-04-26T00:16:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.output.dense.bias from model.bert.encoder.layer.4.output.dense.bias\n","\u001b[32m2024-04-26T00:16:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.output.LayerNorm.weight from model.bert.encoder.layer.4.output.LayerNorm.weight\n","\u001b[32m2024-04-26T00:16:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.output.LayerNorm.bias from model.bert.encoder.layer.4.output.LayerNorm.bias\n","\u001b[32m2024-04-26T00:16:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.self.query.weight from model.bert.encoder.layer.5.attention.self.query.weight\n","\u001b[32m2024-04-26T00:16:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.self.query.bias from model.bert.encoder.layer.5.attention.self.query.bias\n","\u001b[32m2024-04-26T00:16:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.self.key.weight from model.bert.encoder.layer.5.attention.self.key.weight\n","\u001b[32m2024-04-26T00:16:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.self.key.bias from model.bert.encoder.layer.5.attention.self.key.bias\n","\u001b[32m2024-04-26T00:16:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.self.value.weight from model.bert.encoder.layer.5.attention.self.value.weight\n","\u001b[32m2024-04-26T00:16:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.self.value.bias from model.bert.encoder.layer.5.attention.self.value.bias\n","\u001b[32m2024-04-26T00:16:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.output.dense.weight from model.bert.encoder.layer.5.attention.output.dense.weight\n","\u001b[32m2024-04-26T00:16:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.output.dense.bias from model.bert.encoder.layer.5.attention.output.dense.bias\n","\u001b[32m2024-04-26T00:16:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.output.LayerNorm.weight from model.bert.encoder.layer.5.attention.output.LayerNorm.weight\n","\u001b[32m2024-04-26T00:16:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.output.LayerNorm.bias from model.bert.encoder.layer.5.attention.output.LayerNorm.bias\n","\u001b[32m2024-04-26T00:16:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.intermediate.dense.weight from model.bert.encoder.layer.5.intermediate.dense.weight\n","\u001b[32m2024-04-26T00:16:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.intermediate.dense.bias from model.bert.encoder.layer.5.intermediate.dense.bias\n","\u001b[32m2024-04-26T00:16:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.output.dense.weight from model.bert.encoder.layer.5.output.dense.weight\n","\u001b[32m2024-04-26T00:16:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.output.dense.bias from model.bert.encoder.layer.5.output.dense.bias\n","\u001b[32m2024-04-26T00:16:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.output.LayerNorm.weight from model.bert.encoder.layer.5.output.LayerNorm.weight\n","\u001b[32m2024-04-26T00:16:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.output.LayerNorm.bias from model.bert.encoder.layer.5.output.LayerNorm.bias\n","\u001b[32m2024-04-26T00:16:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.self.query.weight from model.bert.encoder.layer.6.attention.self.query.weight\n","\u001b[32m2024-04-26T00:16:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.self.query.bias from model.bert.encoder.layer.6.attention.self.query.bias\n","\u001b[32m2024-04-26T00:16:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.self.key.weight from model.bert.encoder.layer.6.attention.self.key.weight\n","\u001b[32m2024-04-26T00:16:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.self.key.bias from model.bert.encoder.layer.6.attention.self.key.bias\n","\u001b[32m2024-04-26T00:16:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.self.value.weight from model.bert.encoder.layer.6.attention.self.value.weight\n","\u001b[32m2024-04-26T00:16:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.self.value.bias from model.bert.encoder.layer.6.attention.self.value.bias\n","\u001b[32m2024-04-26T00:16:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.output.dense.weight from model.bert.encoder.layer.6.attention.output.dense.weight\n","\u001b[32m2024-04-26T00:16:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.output.dense.bias from model.bert.encoder.layer.6.attention.output.dense.bias\n","\u001b[32m2024-04-26T00:16:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.output.LayerNorm.weight from model.bert.encoder.layer.6.attention.output.LayerNorm.weight\n","\u001b[32m2024-04-26T00:16:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.output.LayerNorm.bias from model.bert.encoder.layer.6.attention.output.LayerNorm.bias\n","\u001b[32m2024-04-26T00:16:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.intermediate.dense.weight from model.bert.encoder.layer.6.intermediate.dense.weight\n","\u001b[32m2024-04-26T00:16:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.intermediate.dense.bias from model.bert.encoder.layer.6.intermediate.dense.bias\n","\u001b[32m2024-04-26T00:16:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.output.dense.weight from model.bert.encoder.layer.6.output.dense.weight\n","\u001b[32m2024-04-26T00:16:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.output.dense.bias from model.bert.encoder.layer.6.output.dense.bias\n","\u001b[32m2024-04-26T00:16:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.output.LayerNorm.weight from model.bert.encoder.layer.6.output.LayerNorm.weight\n","\u001b[32m2024-04-26T00:16:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.output.LayerNorm.bias from model.bert.encoder.layer.6.output.LayerNorm.bias\n","\u001b[32m2024-04-26T00:16:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.self.query.weight from model.bert.encoder.layer.7.attention.self.query.weight\n","\u001b[32m2024-04-26T00:16:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.self.query.bias from model.bert.encoder.layer.7.attention.self.query.bias\n","\u001b[32m2024-04-26T00:16:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.self.key.weight from model.bert.encoder.layer.7.attention.self.key.weight\n","\u001b[32m2024-04-26T00:16:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.self.key.bias from model.bert.encoder.layer.7.attention.self.key.bias\n","\u001b[32m2024-04-26T00:16:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.self.value.weight from model.bert.encoder.layer.7.attention.self.value.weight\n","\u001b[32m2024-04-26T00:16:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.self.value.bias from model.bert.encoder.layer.7.attention.self.value.bias\n","\u001b[32m2024-04-26T00:16:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.output.dense.weight from model.bert.encoder.layer.7.attention.output.dense.weight\n","\u001b[32m2024-04-26T00:16:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.output.dense.bias from model.bert.encoder.layer.7.attention.output.dense.bias\n","\u001b[32m2024-04-26T00:16:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.output.LayerNorm.weight from model.bert.encoder.layer.7.attention.output.LayerNorm.weight\n","\u001b[32m2024-04-26T00:16:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.output.LayerNorm.bias from model.bert.encoder.layer.7.attention.output.LayerNorm.bias\n","\u001b[32m2024-04-26T00:16:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.intermediate.dense.weight from model.bert.encoder.layer.7.intermediate.dense.weight\n","\u001b[32m2024-04-26T00:16:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.intermediate.dense.bias from model.bert.encoder.layer.7.intermediate.dense.bias\n","\u001b[32m2024-04-26T00:16:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.output.dense.weight from model.bert.encoder.layer.7.output.dense.weight\n","\u001b[32m2024-04-26T00:16:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.output.dense.bias from model.bert.encoder.layer.7.output.dense.bias\n","\u001b[32m2024-04-26T00:16:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.output.LayerNorm.weight from model.bert.encoder.layer.7.output.LayerNorm.weight\n","\u001b[32m2024-04-26T00:16:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.output.LayerNorm.bias from model.bert.encoder.layer.7.output.LayerNorm.bias\n","\u001b[32m2024-04-26T00:16:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.self.query.weight from model.bert.encoder.layer.8.attention.self.query.weight\n","\u001b[32m2024-04-26T00:16:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.self.query.bias from model.bert.encoder.layer.8.attention.self.query.bias\n","\u001b[32m2024-04-26T00:16:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.self.key.weight from model.bert.encoder.layer.8.attention.self.key.weight\n","\u001b[32m2024-04-26T00:16:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.self.key.bias from model.bert.encoder.layer.8.attention.self.key.bias\n","\u001b[32m2024-04-26T00:16:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.self.value.weight from model.bert.encoder.layer.8.attention.self.value.weight\n","\u001b[32m2024-04-26T00:16:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.self.value.bias from model.bert.encoder.layer.8.attention.self.value.bias\n","\u001b[32m2024-04-26T00:16:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.output.dense.weight from model.bert.encoder.layer.8.attention.output.dense.weight\n","\u001b[32m2024-04-26T00:16:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.output.dense.bias from model.bert.encoder.layer.8.attention.output.dense.bias\n","\u001b[32m2024-04-26T00:16:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.output.LayerNorm.weight from model.bert.encoder.layer.8.attention.output.LayerNorm.weight\n","\u001b[32m2024-04-26T00:16:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.output.LayerNorm.bias from model.bert.encoder.layer.8.attention.output.LayerNorm.bias\n","\u001b[32m2024-04-26T00:16:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.intermediate.dense.weight from model.bert.encoder.layer.8.intermediate.dense.weight\n","\u001b[32m2024-04-26T00:16:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.intermediate.dense.bias from model.bert.encoder.layer.8.intermediate.dense.bias\n","\u001b[32m2024-04-26T00:16:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.output.dense.weight from model.bert.encoder.layer.8.output.dense.weight\n","\u001b[32m2024-04-26T00:16:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.output.dense.bias from model.bert.encoder.layer.8.output.dense.bias\n","\u001b[32m2024-04-26T00:16:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.output.LayerNorm.weight from model.bert.encoder.layer.8.output.LayerNorm.weight\n","\u001b[32m2024-04-26T00:16:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.output.LayerNorm.bias from model.bert.encoder.layer.8.output.LayerNorm.bias\n","\u001b[32m2024-04-26T00:16:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.self.query.weight from model.bert.encoder.layer.9.attention.self.query.weight\n","\u001b[32m2024-04-26T00:16:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.self.query.bias from model.bert.encoder.layer.9.attention.self.query.bias\n","\u001b[32m2024-04-26T00:16:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.self.key.weight from model.bert.encoder.layer.9.attention.self.key.weight\n","\u001b[32m2024-04-26T00:16:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.self.key.bias from model.bert.encoder.layer.9.attention.self.key.bias\n","\u001b[32m2024-04-26T00:16:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.self.value.weight from model.bert.encoder.layer.9.attention.self.value.weight\n","\u001b[32m2024-04-26T00:16:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.self.value.bias from model.bert.encoder.layer.9.attention.self.value.bias\n","\u001b[32m2024-04-26T00:16:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.output.dense.weight from model.bert.encoder.layer.9.attention.output.dense.weight\n","\u001b[32m2024-04-26T00:16:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.output.dense.bias from model.bert.encoder.layer.9.attention.output.dense.bias\n","\u001b[32m2024-04-26T00:16:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.output.LayerNorm.weight from model.bert.encoder.layer.9.attention.output.LayerNorm.weight\n","\u001b[32m2024-04-26T00:16:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.output.LayerNorm.bias from model.bert.encoder.layer.9.attention.output.LayerNorm.bias\n","\u001b[32m2024-04-26T00:16:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.intermediate.dense.weight from model.bert.encoder.layer.9.intermediate.dense.weight\n","\u001b[32m2024-04-26T00:16:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.intermediate.dense.bias from model.bert.encoder.layer.9.intermediate.dense.bias\n","\u001b[32m2024-04-26T00:16:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.output.dense.weight from model.bert.encoder.layer.9.output.dense.weight\n","\u001b[32m2024-04-26T00:16:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.output.dense.bias from model.bert.encoder.layer.9.output.dense.bias\n","\u001b[32m2024-04-26T00:16:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.output.LayerNorm.weight from model.bert.encoder.layer.9.output.LayerNorm.weight\n","\u001b[32m2024-04-26T00:16:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.output.LayerNorm.bias from model.bert.encoder.layer.9.output.LayerNorm.bias\n","\u001b[32m2024-04-26T00:16:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.self.query.weight from model.bert.encoder.layer.10.attention.self.query.weight\n","\u001b[32m2024-04-26T00:16:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.self.query.bias from model.bert.encoder.layer.10.attention.self.query.bias\n","\u001b[32m2024-04-26T00:16:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.self.key.weight from model.bert.encoder.layer.10.attention.self.key.weight\n","\u001b[32m2024-04-26T00:16:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.self.key.bias from model.bert.encoder.layer.10.attention.self.key.bias\n","\u001b[32m2024-04-26T00:16:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.self.value.weight from model.bert.encoder.layer.10.attention.self.value.weight\n","\u001b[32m2024-04-26T00:16:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.self.value.bias from model.bert.encoder.layer.10.attention.self.value.bias\n","\u001b[32m2024-04-26T00:16:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.output.dense.weight from model.bert.encoder.layer.10.attention.output.dense.weight\n","\u001b[32m2024-04-26T00:16:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.output.dense.bias from model.bert.encoder.layer.10.attention.output.dense.bias\n","\u001b[32m2024-04-26T00:16:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.output.LayerNorm.weight from model.bert.encoder.layer.10.attention.output.LayerNorm.weight\n","\u001b[32m2024-04-26T00:16:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.output.LayerNorm.bias from model.bert.encoder.layer.10.attention.output.LayerNorm.bias\n","\u001b[32m2024-04-26T00:16:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.intermediate.dense.weight from model.bert.encoder.layer.10.intermediate.dense.weight\n","\u001b[32m2024-04-26T00:16:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.intermediate.dense.bias from model.bert.encoder.layer.10.intermediate.dense.bias\n","\u001b[32m2024-04-26T00:16:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.output.dense.weight from model.bert.encoder.layer.10.output.dense.weight\n","\u001b[32m2024-04-26T00:16:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.output.dense.bias from model.bert.encoder.layer.10.output.dense.bias\n","\u001b[32m2024-04-26T00:16:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.output.LayerNorm.weight from model.bert.encoder.layer.10.output.LayerNorm.weight\n","\u001b[32m2024-04-26T00:16:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.output.LayerNorm.bias from model.bert.encoder.layer.10.output.LayerNorm.bias\n","\u001b[32m2024-04-26T00:16:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.self.query.weight from model.bert.encoder.layer.11.attention.self.query.weight\n","\u001b[32m2024-04-26T00:16:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.self.query.bias from model.bert.encoder.layer.11.attention.self.query.bias\n","\u001b[32m2024-04-26T00:16:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.self.key.weight from model.bert.encoder.layer.11.attention.self.key.weight\n","\u001b[32m2024-04-26T00:16:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.self.key.bias from model.bert.encoder.layer.11.attention.self.key.bias\n","\u001b[32m2024-04-26T00:16:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.self.value.weight from model.bert.encoder.layer.11.attention.self.value.weight\n","\u001b[32m2024-04-26T00:16:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.self.value.bias from model.bert.encoder.layer.11.attention.self.value.bias\n","\u001b[32m2024-04-26T00:16:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.output.dense.weight from model.bert.encoder.layer.11.attention.output.dense.weight\n","\u001b[32m2024-04-26T00:16:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.output.dense.bias from model.bert.encoder.layer.11.attention.output.dense.bias\n","\u001b[32m2024-04-26T00:16:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.output.LayerNorm.weight from model.bert.encoder.layer.11.attention.output.LayerNorm.weight\n","\u001b[32m2024-04-26T00:16:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.output.LayerNorm.bias from model.bert.encoder.layer.11.attention.output.LayerNorm.bias\n","\u001b[32m2024-04-26T00:16:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.intermediate.dense.weight from model.bert.encoder.layer.11.intermediate.dense.weight\n","\u001b[32m2024-04-26T00:16:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.intermediate.dense.bias from model.bert.encoder.layer.11.intermediate.dense.bias\n","\u001b[32m2024-04-26T00:16:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.output.dense.weight from model.bert.encoder.layer.11.output.dense.weight\n","\u001b[32m2024-04-26T00:16:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.output.dense.bias from model.bert.encoder.layer.11.output.dense.bias\n","\u001b[32m2024-04-26T00:16:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.output.LayerNorm.weight from model.bert.encoder.layer.11.output.LayerNorm.weight\n","\u001b[32m2024-04-26T00:16:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.output.LayerNorm.bias from model.bert.encoder.layer.11.output.LayerNorm.bias\n","\u001b[32m2024-04-26T00:16:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.pooler.dense.weight from model.bert.pooler.dense.weight\n","\u001b[32m2024-04-26T00:16:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.pooler.dense.bias from model.bert.pooler.dense.bias\n","\u001b[32m2024-04-26T00:16:38 | mmf.utils.checkpoint: \u001b[0mPretrained model loaded\n","\u001b[32m2024-04-26T00:16:38 | mmf.utils.checkpoint: \u001b[0mCheckpoint loaded.\n","\u001b[32m2024-04-26T00:16:38 | mmf.utils.checkpoint: \u001b[0mCurrent num updates: 0\n","\u001b[32m2024-04-26T00:16:38 | mmf.utils.checkpoint: \u001b[0mCurrent iteration: 0\n","\u001b[32m2024-04-26T00:16:38 | mmf.utils.checkpoint: \u001b[0mCurrent epoch: 0\n","\u001b[32m2024-04-26T00:16:38 | mmf.trainers.mmf_trainer: \u001b[0m===== Model =====\n","\u001b[32m2024-04-26T00:16:38 | mmf.trainers.mmf_trainer: \u001b[0mVisualBERT(\n","  (model): VisualBERTForClassification(\n","    (bert): VisualBERTBase(\n","      (embeddings): BertVisioLinguisticEmbeddings(\n","        (word_embeddings): Embedding(30522, 768, padding_idx=0)\n","        (position_embeddings): Embedding(512, 768)\n","        (token_type_embeddings): Embedding(2, 768)\n","        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","        (token_type_embeddings_visual): Embedding(2, 768)\n","        (position_embeddings_visual): Embedding(512, 768)\n","        (projection): Linear(in_features=2048, out_features=768, bias=True)\n","      )\n","      (encoder): BertEncoderJit(\n","        (layer): ModuleList(\n","          (0-11): 12 x BertLayerJit(\n","            (attention): BertAttentionJit(\n","              (self): BertSelfAttentionJit(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","              (intermediate_act_fn): GELUActivation()\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","        )\n","      )\n","      (pooler): BertPooler(\n","        (dense): Linear(in_features=768, out_features=768, bias=True)\n","        (activation): Tanh()\n","      )\n","    )\n","    (dropout): Dropout(p=0.1, inplace=False)\n","    (classifier): Sequential(\n","      (0): BertPredictionHeadTransform(\n","        (dense): Linear(in_features=768, out_features=768, bias=True)\n","        (transform_act_fn): GELUActivation()\n","        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      )\n","      (1): Linear(in_features=768, out_features=2, bias=True)\n","    )\n","  )\n","  (losses): Losses(\n","    (losses): ModuleList(\n","      (0): MMFLoss(\n","        (loss_criterion): CrossEntropyLoss(\n","          (loss_fn): CrossEntropyLoss()\n","        )\n","      )\n","    )\n","  )\n",")\n","\u001b[32m2024-04-26T00:16:38 | mmf.utils.general: \u001b[0mTotal Parameters: 112044290. Trained Parameters: 112044290\n","\u001b[32m2024-04-26T00:16:38 | mmf.trainers.core.training_loop: \u001b[0mStarting training...\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2024-04-26T00:16:38 | py.warnings: \u001b[0m/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n","  self.pid = os.fork()\n","\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2024-04-26T00:16:38 | py.warnings: \u001b[0m/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n","  self.pid = os.fork()\n","\n","\u001b[32m2024-04-26T00:18:43 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n","\u001b[32m2024-04-26T00:18:43 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2024-04-26T00:18:46 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2024-04-26T00:18:53 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2024-04-26T00:18:53 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 200/1500, train/hateful_memes/cross_entropy: 0.5529, train/hateful_memes/cross_entropy/avg: 0.5529, train/total_loss: 0.5529, train/total_loss/avg: 0.5529, max mem: 14364.0, experiment: run, epoch: 1, num_updates: 200, iterations: 200, max_updates: 1500, lr: 0.00001, ups: 1.49, time: 02m 14s 952ms, time_since_start: 02m 15s 645ms, eta: 15m 16s 668ms\n","\u001b[32m2024-04-26T00:18:53 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n","\u001b[32m2024-04-26T00:18:53 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n","\u001b[32m2024-04-26T00:18:55 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished evaluation inference. Loaded 9\n","\u001b[32m2024-04-26T00:18:55 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n","\u001b[32m2024-04-26T00:18:56 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2024-04-26T00:18:59 | mmf.utils.checkpoint: \u001b[0mSaving best checkpoint\n","\u001b[32m2024-04-26T00:19:03 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2024-04-26T00:19:07 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2024-04-26T00:19:07 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 200/1500, val/hateful_memes/cross_entropy: 0.6837, val/total_loss: 0.6837, val/hateful_memes/accuracy: 0.6037, val/hateful_memes/binary_f1: 0.2411, val/hateful_memes/roc_auc: 0.5823, num_updates: 200, epoch: 1, iterations: 200, max_updates: 1500, val_time: 14s 249ms, best_update: 200, best_iteration: 200, best_val/hateful_memes/roc_auc: 0.582279\n","\u001b[32m2024-04-26T00:21:12 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n","\u001b[32m2024-04-26T00:21:12 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2024-04-26T00:21:16 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2024-04-26T00:21:19 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2024-04-26T00:21:19 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 400/1500, train/hateful_memes/cross_entropy: 0.5137, train/hateful_memes/cross_entropy/avg: 0.5333, train/total_loss: 0.5137, train/total_loss/avg: 0.5333, max mem: 14413.0, experiment: run, epoch: 2, num_updates: 400, iterations: 400, max_updates: 1500, lr: 0.00001, ups: 1.52, time: 02m 12s 106ms, time_since_start: 04m 42s 010ms, eta: 12m 39s 284ms\n","\u001b[32m2024-04-26T00:21:19 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n","\u001b[32m2024-04-26T00:21:19 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2024-04-26T00:21:19 | py.warnings: \u001b[0m/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n","  self.pid = os.fork()\n","\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2024-04-26T00:21:19 | py.warnings: \u001b[0m/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n","  self.pid = os.fork()\n","\n","\u001b[32m2024-04-26T00:21:22 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished evaluation inference. Loaded 9\n","\u001b[32m2024-04-26T00:21:22 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n","\u001b[32m2024-04-26T00:21:22 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2024-04-26T00:21:27 | mmf.utils.checkpoint: \u001b[0mSaving best checkpoint\n","\u001b[32m2024-04-26T00:21:31 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2024-04-26T00:21:36 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2024-04-26T00:21:36 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 400/1500, val/hateful_memes/cross_entropy: 0.7586, val/total_loss: 0.7586, val/hateful_memes/accuracy: 0.6574, val/hateful_memes/binary_f1: 0.3771, val/hateful_memes/roc_auc: 0.6668, num_updates: 400, epoch: 2, iterations: 400, max_updates: 1500, val_time: 16s 303ms, best_update: 400, best_iteration: 400, best_val/hateful_memes/roc_auc: 0.666838\n","\u001b[32m2024-04-26T00:23:40 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n","\u001b[32m2024-04-26T00:23:40 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2024-04-26T00:23:44 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2024-04-26T00:23:47 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2024-04-26T00:23:47 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 600/1500, train/hateful_memes/cross_entropy: 0.5137, train/hateful_memes/cross_entropy/avg: 0.4342, train/total_loss: 0.5137, train/total_loss/avg: 0.4342, max mem: 14413.0, experiment: run, epoch: 3, num_updates: 600, iterations: 600, max_updates: 1500, lr: 0.00002, ups: 1.53, time: 02m 11s 588ms, time_since_start: 07m 09s 904ms, eta: 10m 18s 795ms\n","\u001b[32m2024-04-26T00:23:47 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n","\u001b[32m2024-04-26T00:23:47 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2024-04-26T00:23:47 | py.warnings: \u001b[0m/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n","  self.pid = os.fork()\n","\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2024-04-26T00:23:47 | py.warnings: \u001b[0m/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n","  self.pid = os.fork()\n","\n","\u001b[32m2024-04-26T00:23:50 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished evaluation inference. Loaded 9\n","\u001b[32m2024-04-26T00:23:50 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n","\u001b[32m2024-04-26T00:23:50 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2024-04-26T00:23:54 | mmf.utils.checkpoint: \u001b[0mSaving best checkpoint\n","\u001b[32m2024-04-26T00:23:58 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2024-04-26T00:24:05 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2024-04-26T00:24:05 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 600/1500, val/hateful_memes/cross_entropy: 0.8940, val/total_loss: 0.8940, val/hateful_memes/accuracy: 0.6778, val/hateful_memes/binary_f1: 0.4200, val/hateful_memes/roc_auc: 0.7021, num_updates: 600, epoch: 3, iterations: 600, max_updates: 1500, val_time: 17s 471ms, best_update: 600, best_iteration: 600, best_val/hateful_memes/roc_auc: 0.702118\n","\u001b[32m2024-04-26T00:26:09 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n","\u001b[32m2024-04-26T00:26:09 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2024-04-26T00:26:13 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2024-04-26T00:26:16 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2024-04-26T00:26:16 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 800/1500, train/hateful_memes/cross_entropy: 0.2510, train/hateful_memes/cross_entropy/avg: 0.3884, train/total_loss: 0.2510, train/total_loss/avg: 0.3884, max mem: 14413.0, experiment: run, epoch: 3, num_updates: 800, iterations: 800, max_updates: 1500, lr: 0.00002, ups: 1.53, time: 02m 11s 484ms, time_since_start: 09m 38s 862ms, eta: 08m 903ms\n","\u001b[32m2024-04-26T00:26:16 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n","\u001b[32m2024-04-26T00:26:16 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2024-04-26T00:26:16 | py.warnings: \u001b[0m/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n","  self.pid = os.fork()\n","\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2024-04-26T00:26:16 | py.warnings: \u001b[0m/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n","  self.pid = os.fork()\n","\n","\u001b[32m2024-04-26T00:26:19 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished evaluation inference. Loaded 9\n","\u001b[32m2024-04-26T00:26:19 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n","\u001b[32m2024-04-26T00:26:19 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2024-04-26T00:26:23 | mmf.utils.checkpoint: \u001b[0mSaving best checkpoint\n","\u001b[32m2024-04-26T00:26:28 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2024-04-26T00:26:33 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2024-04-26T00:26:33 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 800/1500, val/hateful_memes/cross_entropy: 0.9475, val/total_loss: 0.9475, val/hateful_memes/accuracy: 0.6963, val/hateful_memes/binary_f1: 0.4533, val/hateful_memes/roc_auc: 0.7079, num_updates: 800, epoch: 3, iterations: 800, max_updates: 1500, val_time: 16s 919ms, best_update: 800, best_iteration: 800, best_val/hateful_memes/roc_auc: 0.707853\n","\u001b[32m2024-04-26T00:28:37 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n","\u001b[32m2024-04-26T00:28:37 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2024-04-26T00:28:41 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2024-04-26T00:28:44 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2024-04-26T00:28:44 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1000/1500, train/hateful_memes/cross_entropy: 0.2510, train/hateful_memes/cross_entropy/avg: 0.3401, train/total_loss: 0.2510, train/total_loss/avg: 0.3401, max mem: 14413.0, experiment: run, epoch: 4, num_updates: 1000, iterations: 1000, max_updates: 1500, lr: 0.00003, ups: 1.54, time: 02m 10s 811ms, time_since_start: 12m 06s 595ms, eta: 05m 41s 745ms\n","\u001b[32m2024-04-26T00:28:44 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n","\u001b[32m2024-04-26T00:28:44 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2024-04-26T00:28:44 | py.warnings: \u001b[0m/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n","  self.pid = os.fork()\n","\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2024-04-26T00:28:44 | py.warnings: \u001b[0m/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n","  self.pid = os.fork()\n","\n","\u001b[32m2024-04-26T00:28:47 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished evaluation inference. Loaded 9\n","\u001b[32m2024-04-26T00:28:47 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n","\u001b[32m2024-04-26T00:28:47 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2024-04-26T00:28:52 | mmf.utils.checkpoint: \u001b[0mSaving best checkpoint\n","\u001b[32m2024-04-26T00:28:56 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2024-04-26T00:29:00 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2024-04-26T00:29:00 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1000/1500, val/hateful_memes/cross_entropy: 1.0579, val/total_loss: 1.0579, val/hateful_memes/accuracy: 0.6981, val/hateful_memes/binary_f1: 0.5248, val/hateful_memes/roc_auc: 0.7115, num_updates: 1000, epoch: 4, iterations: 1000, max_updates: 1500, val_time: 15s 633ms, best_update: 1000, best_iteration: 1000, best_val/hateful_memes/roc_auc: 0.711529\n","\u001b[32m2024-04-26T00:31:04 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n","\u001b[32m2024-04-26T00:31:04 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2024-04-26T00:31:07 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2024-04-26T00:31:11 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2024-04-26T00:31:11 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1200/1500, train/hateful_memes/cross_entropy: 0.2359, train/hateful_memes/cross_entropy/avg: 0.3103, train/total_loss: 0.2359, train/total_loss/avg: 0.3103, max mem: 14414.0, experiment: run, epoch: 5, num_updates: 1200, iterations: 1200, max_updates: 1500, lr: 0.00003, ups: 1.53, time: 02m 11s 406ms, time_since_start: 14m 33s 638ms, eta: 03m 25s 980ms\n","\u001b[32m2024-04-26T00:31:11 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n","\u001b[32m2024-04-26T00:31:11 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2024-04-26T00:31:11 | py.warnings: \u001b[0m/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n","  self.pid = os.fork()\n","\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2024-04-26T00:31:11 | py.warnings: \u001b[0m/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n","  self.pid = os.fork()\n","\n","\u001b[32m2024-04-26T00:31:13 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished evaluation inference. Loaded 9\n","\u001b[32m2024-04-26T00:31:13 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n","\u001b[32m2024-04-26T00:31:14 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2024-04-26T00:31:19 | mmf.utils.checkpoint: \u001b[0mSaving best checkpoint\n","\u001b[32m2024-04-26T00:31:23 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2024-04-26T00:31:28 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2024-04-26T00:31:28 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1200/1500, val/hateful_memes/cross_entropy: 1.4068, val/total_loss: 1.4068, val/hateful_memes/accuracy: 0.7019, val/hateful_memes/binary_f1: 0.4756, val/hateful_memes/roc_auc: 0.7393, num_updates: 1200, epoch: 5, iterations: 1200, max_updates: 1500, val_time: 17s 378ms, best_update: 1200, best_iteration: 1200, best_val/hateful_memes/roc_auc: 0.739327\n","\u001b[32m2024-04-26T00:33:32 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n","\u001b[32m2024-04-26T00:33:32 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2024-04-26T00:33:36 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2024-04-26T00:33:43 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2024-04-26T00:33:43 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1400/1500, train/hateful_memes/cross_entropy: 0.2359, train/hateful_memes/cross_entropy/avg: 0.2701, train/total_loss: 0.2359, train/total_loss/avg: 0.2701, max mem: 14414.0, experiment: run, epoch: 6, num_updates: 1400, iterations: 1400, max_updates: 1500, lr: 0.00003, ups: 1.49, time: 02m 14s 979ms, time_since_start: 17m 05s 998ms, eta: 01m 10s 526ms\n","\u001b[32m2024-04-26T00:33:43 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n","\u001b[32m2024-04-26T00:33:43 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2024-04-26T00:33:43 | py.warnings: \u001b[0m/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n","  self.pid = os.fork()\n","\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2024-04-26T00:33:43 | py.warnings: \u001b[0m/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n","  self.pid = os.fork()\n","\n","\u001b[32m2024-04-26T00:33:46 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished evaluation inference. Loaded 9\n","\u001b[32m2024-04-26T00:33:46 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n","\u001b[32m2024-04-26T00:33:46 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2024-04-26T00:33:50 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2024-04-26T00:33:54 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2024-04-26T00:33:54 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1400/1500, val/hateful_memes/cross_entropy: 1.5820, val/total_loss: 1.5820, val/hateful_memes/accuracy: 0.6963, val/hateful_memes/binary_f1: 0.4777, val/hateful_memes/roc_auc: 0.7170, num_updates: 1400, epoch: 6, iterations: 1400, max_updates: 1500, val_time: 11s 019ms, best_update: 1200, best_iteration: 1200, best_val/hateful_memes/roc_auc: 0.739327\n","\u001b[32m2024-04-26T00:34:57 | mmf.trainers.core.training_loop: \u001b[0mStepping into final validation check\n","\u001b[32m2024-04-26T00:34:57 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2024-04-26T00:34:57 | py.warnings: \u001b[0m/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n","  self.pid = os.fork()\n","\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2024-04-26T00:34:57 | py.warnings: \u001b[0m/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n","  self.pid = os.fork()\n","\n","\u001b[32m2024-04-26T00:34:59 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished evaluation inference. Loaded 9\n","\u001b[32m2024-04-26T00:34:59 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n","\u001b[32m2024-04-26T00:34:59 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1500/1500, val/hateful_memes/cross_entropy: 1.5008, val/total_loss: 1.5008, val/hateful_memes/accuracy: 0.7037, val/hateful_memes/binary_f1: 0.4872, val/hateful_memes/roc_auc: 0.7205, num_updates: 1500, epoch: 6, iterations: 1500, max_updates: 1500, val_time: 01m 15s 955ms, best_update: 1200, best_iteration: 1200, best_val/hateful_memes/roc_auc: 0.739327\n","\u001b[32m2024-04-26T00:34:59 | mmf.utils.checkpoint: \u001b[0mRestoring checkpoint\n","\u001b[32m2024-04-26T00:34:59 | mmf.utils.checkpoint: \u001b[0mLoading checkpoint\n","\u001b[32m2024-04-26T00:35:01 | mmf.utils.checkpoint: \u001b[0mCheckpoint loaded.\n","\u001b[32m2024-04-26T00:35:01 | mmf.utils.checkpoint: \u001b[0mCurrent num updates: 1200\n","\u001b[32m2024-04-26T00:35:01 | mmf.utils.checkpoint: \u001b[0mCurrent iteration: 1200\n","\u001b[32m2024-04-26T00:35:01 | mmf.utils.checkpoint: \u001b[0mCurrent epoch: 5\n","\u001b[32m2024-04-26T00:35:02 | mmf.trainers.mmf_trainer: \u001b[0mStarting inference on val set\n","\u001b[32m2024-04-26T00:35:02 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n","  0% 0/9 [00:00<?, ?it/s]\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2024-04-26T00:35:02 | py.warnings: \u001b[0m/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n","  self.pid = os.fork()\n","\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2024-04-26T00:35:02 | py.warnings: \u001b[0m/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n","  self.pid = os.fork()\n","\n","100% 9/9 [00:02<00:00,  3.59it/s]\n","\u001b[32m2024-04-26T00:35:05 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished evaluation inference. Loaded 9\n","\u001b[32m2024-04-26T00:35:05 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n","\u001b[32m2024-04-26T00:35:05 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1200/1500, val/hateful_memes/cross_entropy: 1.4068, val/total_loss: 1.4068, val/hateful_memes/accuracy: 0.7019, val/hateful_memes/binary_f1: 0.4756, val/hateful_memes/roc_auc: 0.7393\n","\u001b[32m2024-04-26T00:35:05 | mmf.trainers.callbacks.logistics: \u001b[0mFinished run in 18m 27s 347ms\n"]}]},{"cell_type":"markdown","source":["### Dataset: Augmented with complete transformations"],"metadata":{"id":"w_ydFyhUYFwz"}},{"cell_type":"code","source":["!mmf_run config=\"projects/visual_bert/configs/hateful_memes/defaults.yaml\" \\\n","  model=visual_bert \\\n","  dataset=hateful_memes \\\n","  run_type=train_val \\\n","  training.log_interval=200 \\\n","  training.batch_size=64 \\\n","  training.evaluation_interval=200 \\\n","  training.tensorboard=True \\\n","  training.checkpoint_interval=200 \\\n","  checkpoint.resume_pretrained=True \\\n","  checkpoint.resume_zoo=visual_bert.pretrained.cc \\\n","  dataset_config.hateful_memes.annotations.train[0]=\"hateful_memes/defaults/annotations/train_dev_transform_all.jsonl\" \\\n","  dataset_config.hateful_memes.annotations.val[0]=\"hateful_memes/defaults/annotations/dev_unseen.jsonl\" \\\n","  dataset_config.hateful_memes.annotations.test[0]=\"hateful_memes/defaults/annotations/test_unseen.jsonl\""],"metadata":{"id":"nsINJJQX8SA8","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1714093115905,"user_tz":420,"elapsed":1119150,"user":{"displayName":"Patrick Prestridge","userId":"01452612983786479124"}},"outputId":"76b1cf4d-e783-4dc9-c7de-7dc06ad18a44"},"execution_count":40,"outputs":[{"output_type":"stream","name":"stdout","text":["2024-04-26 00:39:59.595549: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","2024-04-26 00:39:59.595599: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","2024-04-26 00:39:59.597252: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","2024-04-26 00:40:00.656791: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","2024-04-26 00:40:04.875685: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:47] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n","/usr/local/lib/python3.10/dist-packages/transformers/deepspeed.py:23: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/omegaconf/grammar_visitor.py:253: UserWarning: In the sequence `MMF_USER_DIR,` some elements are missing: please replace them with empty quoted strings. See https://github.com/omry/omegaconf/issues/572 for details.\n","  warnings.warn(\n","\u001b[32m2024-04-26T00:40:09 | mmf.utils.configuration: \u001b[0mOverriding option config to projects/visual_bert/configs/hateful_memes/defaults.yaml\n","\u001b[32m2024-04-26T00:40:09 | mmf.utils.configuration: \u001b[0mOverriding option model to visual_bert\n","\u001b[32m2024-04-26T00:40:09 | mmf.utils.configuration: \u001b[0mOverriding option datasets to hateful_memes\n","\u001b[32m2024-04-26T00:40:09 | mmf.utils.configuration: \u001b[0mOverriding option run_type to train_val\n","\u001b[32m2024-04-26T00:40:09 | mmf.utils.configuration: \u001b[0mOverriding option training.log_interval to 200\n","\u001b[32m2024-04-26T00:40:09 | mmf.utils.configuration: \u001b[0mOverriding option training.batch_size to 64\n","\u001b[32m2024-04-26T00:40:09 | mmf.utils.configuration: \u001b[0mOverriding option training.evaluation_interval to 200\n","\u001b[32m2024-04-26T00:40:09 | mmf.utils.configuration: \u001b[0mOverriding option training.tensorboard to True\n","\u001b[32m2024-04-26T00:40:09 | mmf.utils.configuration: \u001b[0mOverriding option training.checkpoint_interval to 200\n","\u001b[32m2024-04-26T00:40:09 | mmf.utils.configuration: \u001b[0mOverriding option checkpoint.resume_pretrained to True\n","\u001b[32m2024-04-26T00:40:09 | mmf.utils.configuration: \u001b[0mOverriding option checkpoint.resume_zoo to visual_bert.pretrained.cc\n","\u001b[32m2024-04-26T00:40:09 | mmf.utils.configuration: \u001b[0mOverriding option dataset_config.hateful_memes.annotations.train[0] to hateful_memes/defaults/annotations/train_dev_transform_all.jsonl\n","\u001b[32m2024-04-26T00:40:09 | mmf.utils.configuration: \u001b[0mOverriding option dataset_config.hateful_memes.annotations.val[0] to hateful_memes/defaults/annotations/dev_unseen.jsonl\n","\u001b[32m2024-04-26T00:40:09 | mmf.utils.configuration: \u001b[0mOverriding option dataset_config.hateful_memes.annotations.test[0] to hateful_memes/defaults/annotations/test_unseen.jsonl\n","/usr/local/lib/python3.10/dist-packages/omegaconf/grammar_visitor.py:253: UserWarning: In the sequence `MMF_LOG_DIR,` some elements are missing: please replace them with empty quoted strings. See https://github.com/omry/omegaconf/issues/572 for details.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/omegaconf/grammar_visitor.py:253: UserWarning: In the sequence `MMF_REPORT_DIR,` some elements are missing: please replace them with empty quoted strings. See https://github.com/omry/omegaconf/issues/572 for details.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/omegaconf/grammar_visitor.py:253: UserWarning: In the sequence `MMF_TENSORBOARD_LOGDIR,` some elements are missing: please replace them with empty quoted strings. See https://github.com/omry/omegaconf/issues/572 for details.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/omegaconf/grammar_visitor.py:253: UserWarning: In the sequence `MMF_WANDB_LOGDIR,` some elements are missing: please replace them with empty quoted strings. See https://github.com/omry/omegaconf/issues/572 for details.\n","  warnings.warn(\n","\u001b[32m2024-04-26T00:40:09 | mmf: \u001b[0mLogging to: ./save/train.log\n","\u001b[32m2024-04-26T00:40:09 | mmf_cli.run: \u001b[0mNamespace(config_override=None, local_rank=None, opts=['config=projects/visual_bert/configs/hateful_memes/defaults.yaml', 'model=visual_bert', 'dataset=hateful_memes', 'run_type=train_val', 'training.log_interval=200', 'training.batch_size=64', 'training.evaluation_interval=200', 'training.tensorboard=True', 'training.checkpoint_interval=200', 'checkpoint.resume_pretrained=True', 'checkpoint.resume_zoo=visual_bert.pretrained.cc', 'dataset_config.hateful_memes.annotations.train[0]=hateful_memes/defaults/annotations/train_dev_transform_all.jsonl', 'dataset_config.hateful_memes.annotations.val[0]=hateful_memes/defaults/annotations/dev_unseen.jsonl', 'dataset_config.hateful_memes.annotations.test[0]=hateful_memes/defaults/annotations/test_unseen.jsonl'])\n","\u001b[32m2024-04-26T00:40:09 | mmf_cli.run: \u001b[0mTorch version: 2.2.0+cu121\n","\u001b[32m2024-04-26T00:40:09 | mmf.utils.general: \u001b[0mCUDA Device 0 is: NVIDIA A100-SXM4-40GB\n","\u001b[32m2024-04-26T00:40:09 | mmf_cli.run: \u001b[0mUsing seed 9752916\n","\u001b[32m2024-04-26T00:40:09 | mmf.trainers.mmf_trainer: \u001b[0mLoading datasets\n","loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--bert-base-uncased/snapshots/86b5e0934494bd15c9632b12f734a8a67f723594/config.json\n","Model config BertConfig {\n","  \"_name_or_path\": \"bert-base-uncased\",\n","  \"architectures\": [\n","    \"BertForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.39.2\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n","loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--bert-base-uncased/snapshots/86b5e0934494bd15c9632b12f734a8a67f723594/vocab.txt\n","loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--bert-base-uncased/snapshots/86b5e0934494bd15c9632b12f734a8a67f723594/tokenizer.json\n","loading file added_tokens.json from cache at None\n","loading file special_tokens_map.json from cache at None\n","loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--bert-base-uncased/snapshots/86b5e0934494bd15c9632b12f734a8a67f723594/tokenizer_config.json\n","loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--bert-base-uncased/snapshots/86b5e0934494bd15c9632b12f734a8a67f723594/config.json\n","Model config BertConfig {\n","  \"_name_or_path\": \"bert-base-uncased\",\n","  \"architectures\": [\n","    \"BertForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.39.2\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n","\u001b[32m2024-04-26T00:40:10 | mmf.datasets.multi_datamodule: \u001b[0mMultitasking disabled by default for single dataset training\n","\u001b[32m2024-04-26T00:40:10 | mmf.datasets.multi_datamodule: \u001b[0mMultitasking disabled by default for single dataset training\n","\u001b[32m2024-04-26T00:40:10 | mmf.datasets.multi_datamodule: \u001b[0mMultitasking disabled by default for single dataset training\n","\u001b[32m2024-04-26T00:40:10 | mmf.trainers.mmf_trainer: \u001b[0mLoading model\n","Model config BertConfig {\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bert_model_name\": \"bert-base-uncased\",\n","  \"bypass_transformer\": false,\n","  \"classifier_dropout\": null,\n","  \"embedding_strategy\": \"plain\",\n","  \"finetune_lr_multiplier\": 1,\n","  \"freeze_base\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"losses\": [\n","    \"cross_entropy\"\n","  ],\n","  \"max_position_embeddings\": 512,\n","  \"model\": \"visual_bert\",\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"pooler_strategy\": \"default\",\n","  \"position_embedding_type\": \"absolute\",\n","  \"random_initialize\": false,\n","  \"special_visual_initialize\": true,\n","  \"training_head_type\": \"classification\",\n","  \"transformers_version\": \"4.39.2\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"visual_embedding_dim\": 2048,\n","  \"vocab_size\": 30522,\n","  \"zerobias\": false\n","}\n","\n","loading weights file model.safetensors from cache at /root/.cache/torch/mmf/distributed_-1/models--bert-base-uncased/snapshots/86b5e0934494bd15c9632b12f734a8a67f723594/model.safetensors\n","Some weights of the model checkpoint at bert-base-uncased were not used when initializing VisualBERTBase: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n","- This IS expected if you are initializing VisualBERTBase from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing VisualBERTBase from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of VisualBERTBase were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['bert.embeddings.position_embeddings_visual.weight', 'bert.embeddings.projection.bias', 'bert.embeddings.projection.weight', 'bert.embeddings.token_type_embeddings_visual.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","\u001b[32m2024-04-26T00:40:10 | mmf.trainers.mmf_trainer: \u001b[0mLoading optimizer\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2024-04-26T00:40:10 | py.warnings: \u001b[0m/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:457: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n","\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2024-04-26T00:40:10 | py.warnings: \u001b[0m/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:457: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n","\n","\u001b[32m2024-04-26T00:40:10 | mmf.trainers.mmf_trainer: \u001b[0mLoading metrics\n","\u001b[32m2024-04-26T00:40:10 | mmf.utils.checkpoint: \u001b[0mLoading checkpoint\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2024-04-26T00:40:11 | mmf: \u001b[0mKey data_parallel is not present in registry, returning default value of None\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2024-04-26T00:40:11 | mmf: \u001b[0mKey distributed is not present in registry, returning default value of None\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2024-04-26T00:40:11 | mmf: \u001b[0mKey data_parallel is not present in registry, returning default value of None\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2024-04-26T00:40:11 | mmf: \u001b[0mKey distributed is not present in registry, returning default value of None\n","\u001b[32m2024-04-26T00:40:11 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.word_embeddings.weight from model.bert.embeddings.word_embeddings.weight\n","\u001b[32m2024-04-26T00:40:11 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.position_embeddings.weight from model.bert.embeddings.position_embeddings.weight\n","\u001b[32m2024-04-26T00:40:11 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.token_type_embeddings.weight from model.bert.embeddings.token_type_embeddings.weight\n","\u001b[32m2024-04-26T00:40:11 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.LayerNorm.weight from model.bert.embeddings.LayerNorm.weight\n","\u001b[32m2024-04-26T00:40:11 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.LayerNorm.bias from model.bert.embeddings.LayerNorm.bias\n","\u001b[32m2024-04-26T00:40:11 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.token_type_embeddings_visual.weight from model.bert.embeddings.token_type_embeddings_visual.weight\n","\u001b[32m2024-04-26T00:40:11 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.position_embeddings_visual.weight from model.bert.embeddings.position_embeddings_visual.weight\n","\u001b[32m2024-04-26T00:40:11 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.projection.weight from model.bert.embeddings.projection.weight\n","\u001b[32m2024-04-26T00:40:11 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.projection.bias from model.bert.embeddings.projection.bias\n","\u001b[32m2024-04-26T00:40:11 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.self.query.weight from model.bert.encoder.layer.0.attention.self.query.weight\n","\u001b[32m2024-04-26T00:40:11 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.self.query.bias from model.bert.encoder.layer.0.attention.self.query.bias\n","\u001b[32m2024-04-26T00:40:11 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.self.key.weight from model.bert.encoder.layer.0.attention.self.key.weight\n","\u001b[32m2024-04-26T00:40:11 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.self.key.bias from model.bert.encoder.layer.0.attention.self.key.bias\n","\u001b[32m2024-04-26T00:40:11 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.self.value.weight from model.bert.encoder.layer.0.attention.self.value.weight\n","\u001b[32m2024-04-26T00:40:11 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.self.value.bias from model.bert.encoder.layer.0.attention.self.value.bias\n","\u001b[32m2024-04-26T00:40:11 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.output.dense.weight from model.bert.encoder.layer.0.attention.output.dense.weight\n","\u001b[32m2024-04-26T00:40:11 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.output.dense.bias from model.bert.encoder.layer.0.attention.output.dense.bias\n","\u001b[32m2024-04-26T00:40:11 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.output.LayerNorm.weight from model.bert.encoder.layer.0.attention.output.LayerNorm.weight\n","\u001b[32m2024-04-26T00:40:11 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.output.LayerNorm.bias from model.bert.encoder.layer.0.attention.output.LayerNorm.bias\n","\u001b[32m2024-04-26T00:40:11 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.intermediate.dense.weight from model.bert.encoder.layer.0.intermediate.dense.weight\n","\u001b[32m2024-04-26T00:40:11 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.intermediate.dense.bias from model.bert.encoder.layer.0.intermediate.dense.bias\n","\u001b[32m2024-04-26T00:40:11 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.output.dense.weight from model.bert.encoder.layer.0.output.dense.weight\n","\u001b[32m2024-04-26T00:40:11 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.output.dense.bias from model.bert.encoder.layer.0.output.dense.bias\n","\u001b[32m2024-04-26T00:40:11 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.output.LayerNorm.weight from model.bert.encoder.layer.0.output.LayerNorm.weight\n","\u001b[32m2024-04-26T00:40:11 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.output.LayerNorm.bias from model.bert.encoder.layer.0.output.LayerNorm.bias\n","\u001b[32m2024-04-26T00:40:11 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.self.query.weight from model.bert.encoder.layer.1.attention.self.query.weight\n","\u001b[32m2024-04-26T00:40:11 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.self.query.bias from model.bert.encoder.layer.1.attention.self.query.bias\n","\u001b[32m2024-04-26T00:40:11 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.self.key.weight from model.bert.encoder.layer.1.attention.self.key.weight\n","\u001b[32m2024-04-26T00:40:11 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.self.key.bias from model.bert.encoder.layer.1.attention.self.key.bias\n","\u001b[32m2024-04-26T00:40:11 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.self.value.weight from model.bert.encoder.layer.1.attention.self.value.weight\n","\u001b[32m2024-04-26T00:40:11 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.self.value.bias from model.bert.encoder.layer.1.attention.self.value.bias\n","\u001b[32m2024-04-26T00:40:11 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.output.dense.weight from model.bert.encoder.layer.1.attention.output.dense.weight\n","\u001b[32m2024-04-26T00:40:11 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.output.dense.bias from model.bert.encoder.layer.1.attention.output.dense.bias\n","\u001b[32m2024-04-26T00:40:11 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.output.LayerNorm.weight from model.bert.encoder.layer.1.attention.output.LayerNorm.weight\n","\u001b[32m2024-04-26T00:40:11 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.output.LayerNorm.bias from model.bert.encoder.layer.1.attention.output.LayerNorm.bias\n","\u001b[32m2024-04-26T00:40:11 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.intermediate.dense.weight from model.bert.encoder.layer.1.intermediate.dense.weight\n","\u001b[32m2024-04-26T00:40:11 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.intermediate.dense.bias from model.bert.encoder.layer.1.intermediate.dense.bias\n","\u001b[32m2024-04-26T00:40:11 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.output.dense.weight from model.bert.encoder.layer.1.output.dense.weight\n","\u001b[32m2024-04-26T00:40:11 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.output.dense.bias from model.bert.encoder.layer.1.output.dense.bias\n","\u001b[32m2024-04-26T00:40:11 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.output.LayerNorm.weight from model.bert.encoder.layer.1.output.LayerNorm.weight\n","\u001b[32m2024-04-26T00:40:11 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.output.LayerNorm.bias from model.bert.encoder.layer.1.output.LayerNorm.bias\n","\u001b[32m2024-04-26T00:40:11 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.self.query.weight from model.bert.encoder.layer.2.attention.self.query.weight\n","\u001b[32m2024-04-26T00:40:11 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.self.query.bias from model.bert.encoder.layer.2.attention.self.query.bias\n","\u001b[32m2024-04-26T00:40:11 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.self.key.weight from model.bert.encoder.layer.2.attention.self.key.weight\n","\u001b[32m2024-04-26T00:40:11 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.self.key.bias from model.bert.encoder.layer.2.attention.self.key.bias\n","\u001b[32m2024-04-26T00:40:11 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.self.value.weight from model.bert.encoder.layer.2.attention.self.value.weight\n","\u001b[32m2024-04-26T00:40:11 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.self.value.bias from model.bert.encoder.layer.2.attention.self.value.bias\n","\u001b[32m2024-04-26T00:40:11 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.output.dense.weight from model.bert.encoder.layer.2.attention.output.dense.weight\n","\u001b[32m2024-04-26T00:40:11 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.output.dense.bias from model.bert.encoder.layer.2.attention.output.dense.bias\n","\u001b[32m2024-04-26T00:40:11 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.output.LayerNorm.weight from model.bert.encoder.layer.2.attention.output.LayerNorm.weight\n","\u001b[32m2024-04-26T00:40:11 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.output.LayerNorm.bias from model.bert.encoder.layer.2.attention.output.LayerNorm.bias\n","\u001b[32m2024-04-26T00:40:11 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.intermediate.dense.weight from model.bert.encoder.layer.2.intermediate.dense.weight\n","\u001b[32m2024-04-26T00:40:11 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.intermediate.dense.bias from model.bert.encoder.layer.2.intermediate.dense.bias\n","\u001b[32m2024-04-26T00:40:11 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.output.dense.weight from model.bert.encoder.layer.2.output.dense.weight\n","\u001b[32m2024-04-26T00:40:11 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.output.dense.bias from model.bert.encoder.layer.2.output.dense.bias\n","\u001b[32m2024-04-26T00:40:11 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.output.LayerNorm.weight from model.bert.encoder.layer.2.output.LayerNorm.weight\n","\u001b[32m2024-04-26T00:40:11 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.output.LayerNorm.bias from model.bert.encoder.layer.2.output.LayerNorm.bias\n","\u001b[32m2024-04-26T00:40:11 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.self.query.weight from model.bert.encoder.layer.3.attention.self.query.weight\n","\u001b[32m2024-04-26T00:40:11 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.self.query.bias from model.bert.encoder.layer.3.attention.self.query.bias\n","\u001b[32m2024-04-26T00:40:11 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.self.key.weight from model.bert.encoder.layer.3.attention.self.key.weight\n","\u001b[32m2024-04-26T00:40:11 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.self.key.bias from model.bert.encoder.layer.3.attention.self.key.bias\n","\u001b[32m2024-04-26T00:40:11 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.self.value.weight from model.bert.encoder.layer.3.attention.self.value.weight\n","\u001b[32m2024-04-26T00:40:11 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.self.value.bias from model.bert.encoder.layer.3.attention.self.value.bias\n","\u001b[32m2024-04-26T00:40:11 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.output.dense.weight from model.bert.encoder.layer.3.attention.output.dense.weight\n","\u001b[32m2024-04-26T00:40:11 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.output.dense.bias from model.bert.encoder.layer.3.attention.output.dense.bias\n","\u001b[32m2024-04-26T00:40:11 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.output.LayerNorm.weight from model.bert.encoder.layer.3.attention.output.LayerNorm.weight\n","\u001b[32m2024-04-26T00:40:11 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.output.LayerNorm.bias from model.bert.encoder.layer.3.attention.output.LayerNorm.bias\n","\u001b[32m2024-04-26T00:40:11 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.intermediate.dense.weight from model.bert.encoder.layer.3.intermediate.dense.weight\n","\u001b[32m2024-04-26T00:40:11 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.intermediate.dense.bias from model.bert.encoder.layer.3.intermediate.dense.bias\n","\u001b[32m2024-04-26T00:40:11 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.output.dense.weight from model.bert.encoder.layer.3.output.dense.weight\n","\u001b[32m2024-04-26T00:40:11 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.output.dense.bias from model.bert.encoder.layer.3.output.dense.bias\n","\u001b[32m2024-04-26T00:40:11 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.output.LayerNorm.weight from model.bert.encoder.layer.3.output.LayerNorm.weight\n","\u001b[32m2024-04-26T00:40:11 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.output.LayerNorm.bias from model.bert.encoder.layer.3.output.LayerNorm.bias\n","\u001b[32m2024-04-26T00:40:11 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.self.query.weight from model.bert.encoder.layer.4.attention.self.query.weight\n","\u001b[32m2024-04-26T00:40:11 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.self.query.bias from model.bert.encoder.layer.4.attention.self.query.bias\n","\u001b[32m2024-04-26T00:40:11 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.self.key.weight from model.bert.encoder.layer.4.attention.self.key.weight\n","\u001b[32m2024-04-26T00:40:11 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.self.key.bias from model.bert.encoder.layer.4.attention.self.key.bias\n","\u001b[32m2024-04-26T00:40:11 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.self.value.weight from model.bert.encoder.layer.4.attention.self.value.weight\n","\u001b[32m2024-04-26T00:40:11 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.self.value.bias from model.bert.encoder.layer.4.attention.self.value.bias\n","\u001b[32m2024-04-26T00:40:11 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.output.dense.weight from model.bert.encoder.layer.4.attention.output.dense.weight\n","\u001b[32m2024-04-26T00:40:11 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.output.dense.bias from model.bert.encoder.layer.4.attention.output.dense.bias\n","\u001b[32m2024-04-26T00:40:11 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.output.LayerNorm.weight from model.bert.encoder.layer.4.attention.output.LayerNorm.weight\n","\u001b[32m2024-04-26T00:40:11 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.output.LayerNorm.bias from model.bert.encoder.layer.4.attention.output.LayerNorm.bias\n","\u001b[32m2024-04-26T00:40:11 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.intermediate.dense.weight from model.bert.encoder.layer.4.intermediate.dense.weight\n","\u001b[32m2024-04-26T00:40:11 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.intermediate.dense.bias from model.bert.encoder.layer.4.intermediate.dense.bias\n","\u001b[32m2024-04-26T00:40:11 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.output.dense.weight from model.bert.encoder.layer.4.output.dense.weight\n","\u001b[32m2024-04-26T00:40:11 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.output.dense.bias from model.bert.encoder.layer.4.output.dense.bias\n","\u001b[32m2024-04-26T00:40:11 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.output.LayerNorm.weight from model.bert.encoder.layer.4.output.LayerNorm.weight\n","\u001b[32m2024-04-26T00:40:11 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.output.LayerNorm.bias from model.bert.encoder.layer.4.output.LayerNorm.bias\n","\u001b[32m2024-04-26T00:40:11 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.self.query.weight from model.bert.encoder.layer.5.attention.self.query.weight\n","\u001b[32m2024-04-26T00:40:11 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.self.query.bias from model.bert.encoder.layer.5.attention.self.query.bias\n","\u001b[32m2024-04-26T00:40:11 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.self.key.weight from model.bert.encoder.layer.5.attention.self.key.weight\n","\u001b[32m2024-04-26T00:40:11 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.self.key.bias from model.bert.encoder.layer.5.attention.self.key.bias\n","\u001b[32m2024-04-26T00:40:11 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.self.value.weight from model.bert.encoder.layer.5.attention.self.value.weight\n","\u001b[32m2024-04-26T00:40:11 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.self.value.bias from model.bert.encoder.layer.5.attention.self.value.bias\n","\u001b[32m2024-04-26T00:40:11 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.output.dense.weight from model.bert.encoder.layer.5.attention.output.dense.weight\n","\u001b[32m2024-04-26T00:40:11 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.output.dense.bias from model.bert.encoder.layer.5.attention.output.dense.bias\n","\u001b[32m2024-04-26T00:40:11 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.output.LayerNorm.weight from model.bert.encoder.layer.5.attention.output.LayerNorm.weight\n","\u001b[32m2024-04-26T00:40:11 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.output.LayerNorm.bias from model.bert.encoder.layer.5.attention.output.LayerNorm.bias\n","\u001b[32m2024-04-26T00:40:11 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.intermediate.dense.weight from model.bert.encoder.layer.5.intermediate.dense.weight\n","\u001b[32m2024-04-26T00:40:11 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.intermediate.dense.bias from model.bert.encoder.layer.5.intermediate.dense.bias\n","\u001b[32m2024-04-26T00:40:11 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.output.dense.weight from model.bert.encoder.layer.5.output.dense.weight\n","\u001b[32m2024-04-26T00:40:11 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.output.dense.bias from model.bert.encoder.layer.5.output.dense.bias\n","\u001b[32m2024-04-26T00:40:11 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.output.LayerNorm.weight from model.bert.encoder.layer.5.output.LayerNorm.weight\n","\u001b[32m2024-04-26T00:40:11 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.output.LayerNorm.bias from model.bert.encoder.layer.5.output.LayerNorm.bias\n","\u001b[32m2024-04-26T00:40:11 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.self.query.weight from model.bert.encoder.layer.6.attention.self.query.weight\n","\u001b[32m2024-04-26T00:40:11 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.self.query.bias from model.bert.encoder.layer.6.attention.self.query.bias\n","\u001b[32m2024-04-26T00:40:11 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.self.key.weight from model.bert.encoder.layer.6.attention.self.key.weight\n","\u001b[32m2024-04-26T00:40:11 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.self.key.bias from model.bert.encoder.layer.6.attention.self.key.bias\n","\u001b[32m2024-04-26T00:40:11 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.self.value.weight from model.bert.encoder.layer.6.attention.self.value.weight\n","\u001b[32m2024-04-26T00:40:11 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.self.value.bias from model.bert.encoder.layer.6.attention.self.value.bias\n","\u001b[32m2024-04-26T00:40:11 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.output.dense.weight from model.bert.encoder.layer.6.attention.output.dense.weight\n","\u001b[32m2024-04-26T00:40:11 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.output.dense.bias from model.bert.encoder.layer.6.attention.output.dense.bias\n","\u001b[32m2024-04-26T00:40:11 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.output.LayerNorm.weight from model.bert.encoder.layer.6.attention.output.LayerNorm.weight\n","\u001b[32m2024-04-26T00:40:11 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.output.LayerNorm.bias from model.bert.encoder.layer.6.attention.output.LayerNorm.bias\n","\u001b[32m2024-04-26T00:40:11 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.intermediate.dense.weight from model.bert.encoder.layer.6.intermediate.dense.weight\n","\u001b[32m2024-04-26T00:40:11 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.intermediate.dense.bias from model.bert.encoder.layer.6.intermediate.dense.bias\n","\u001b[32m2024-04-26T00:40:11 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.output.dense.weight from model.bert.encoder.layer.6.output.dense.weight\n","\u001b[32m2024-04-26T00:40:11 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.output.dense.bias from model.bert.encoder.layer.6.output.dense.bias\n","\u001b[32m2024-04-26T00:40:11 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.output.LayerNorm.weight from model.bert.encoder.layer.6.output.LayerNorm.weight\n","\u001b[32m2024-04-26T00:40:11 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.output.LayerNorm.bias from model.bert.encoder.layer.6.output.LayerNorm.bias\n","\u001b[32m2024-04-26T00:40:11 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.self.query.weight from model.bert.encoder.layer.7.attention.self.query.weight\n","\u001b[32m2024-04-26T00:40:11 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.self.query.bias from model.bert.encoder.layer.7.attention.self.query.bias\n","\u001b[32m2024-04-26T00:40:11 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.self.key.weight from model.bert.encoder.layer.7.attention.self.key.weight\n","\u001b[32m2024-04-26T00:40:11 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.self.key.bias from model.bert.encoder.layer.7.attention.self.key.bias\n","\u001b[32m2024-04-26T00:40:11 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.self.value.weight from model.bert.encoder.layer.7.attention.self.value.weight\n","\u001b[32m2024-04-26T00:40:11 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.self.value.bias from model.bert.encoder.layer.7.attention.self.value.bias\n","\u001b[32m2024-04-26T00:40:11 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.output.dense.weight from model.bert.encoder.layer.7.attention.output.dense.weight\n","\u001b[32m2024-04-26T00:40:11 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.output.dense.bias from model.bert.encoder.layer.7.attention.output.dense.bias\n","\u001b[32m2024-04-26T00:40:11 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.output.LayerNorm.weight from model.bert.encoder.layer.7.attention.output.LayerNorm.weight\n","\u001b[32m2024-04-26T00:40:11 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.output.LayerNorm.bias from model.bert.encoder.layer.7.attention.output.LayerNorm.bias\n","\u001b[32m2024-04-26T00:40:11 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.intermediate.dense.weight from model.bert.encoder.layer.7.intermediate.dense.weight\n","\u001b[32m2024-04-26T00:40:11 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.intermediate.dense.bias from model.bert.encoder.layer.7.intermediate.dense.bias\n","\u001b[32m2024-04-26T00:40:11 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.output.dense.weight from model.bert.encoder.layer.7.output.dense.weight\n","\u001b[32m2024-04-26T00:40:11 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.output.dense.bias from model.bert.encoder.layer.7.output.dense.bias\n","\u001b[32m2024-04-26T00:40:11 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.output.LayerNorm.weight from model.bert.encoder.layer.7.output.LayerNorm.weight\n","\u001b[32m2024-04-26T00:40:11 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.output.LayerNorm.bias from model.bert.encoder.layer.7.output.LayerNorm.bias\n","\u001b[32m2024-04-26T00:40:11 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.self.query.weight from model.bert.encoder.layer.8.attention.self.query.weight\n","\u001b[32m2024-04-26T00:40:11 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.self.query.bias from model.bert.encoder.layer.8.attention.self.query.bias\n","\u001b[32m2024-04-26T00:40:11 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.self.key.weight from model.bert.encoder.layer.8.attention.self.key.weight\n","\u001b[32m2024-04-26T00:40:11 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.self.key.bias from model.bert.encoder.layer.8.attention.self.key.bias\n","\u001b[32m2024-04-26T00:40:11 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.self.value.weight from model.bert.encoder.layer.8.attention.self.value.weight\n","\u001b[32m2024-04-26T00:40:11 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.self.value.bias from model.bert.encoder.layer.8.attention.self.value.bias\n","\u001b[32m2024-04-26T00:40:11 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.output.dense.weight from model.bert.encoder.layer.8.attention.output.dense.weight\n","\u001b[32m2024-04-26T00:40:11 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.output.dense.bias from model.bert.encoder.layer.8.attention.output.dense.bias\n","\u001b[32m2024-04-26T00:40:11 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.output.LayerNorm.weight from model.bert.encoder.layer.8.attention.output.LayerNorm.weight\n","\u001b[32m2024-04-26T00:40:11 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.output.LayerNorm.bias from model.bert.encoder.layer.8.attention.output.LayerNorm.bias\n","\u001b[32m2024-04-26T00:40:11 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.intermediate.dense.weight from model.bert.encoder.layer.8.intermediate.dense.weight\n","\u001b[32m2024-04-26T00:40:11 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.intermediate.dense.bias from model.bert.encoder.layer.8.intermediate.dense.bias\n","\u001b[32m2024-04-26T00:40:11 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.output.dense.weight from model.bert.encoder.layer.8.output.dense.weight\n","\u001b[32m2024-04-26T00:40:11 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.output.dense.bias from model.bert.encoder.layer.8.output.dense.bias\n","\u001b[32m2024-04-26T00:40:11 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.output.LayerNorm.weight from model.bert.encoder.layer.8.output.LayerNorm.weight\n","\u001b[32m2024-04-26T00:40:11 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.output.LayerNorm.bias from model.bert.encoder.layer.8.output.LayerNorm.bias\n","\u001b[32m2024-04-26T00:40:11 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.self.query.weight from model.bert.encoder.layer.9.attention.self.query.weight\n","\u001b[32m2024-04-26T00:40:11 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.self.query.bias from model.bert.encoder.layer.9.attention.self.query.bias\n","\u001b[32m2024-04-26T00:40:11 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.self.key.weight from model.bert.encoder.layer.9.attention.self.key.weight\n","\u001b[32m2024-04-26T00:40:11 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.self.key.bias from model.bert.encoder.layer.9.attention.self.key.bias\n","\u001b[32m2024-04-26T00:40:11 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.self.value.weight from model.bert.encoder.layer.9.attention.self.value.weight\n","\u001b[32m2024-04-26T00:40:11 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.self.value.bias from model.bert.encoder.layer.9.attention.self.value.bias\n","\u001b[32m2024-04-26T00:40:11 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.output.dense.weight from model.bert.encoder.layer.9.attention.output.dense.weight\n","\u001b[32m2024-04-26T00:40:11 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.output.dense.bias from model.bert.encoder.layer.9.attention.output.dense.bias\n","\u001b[32m2024-04-26T00:40:11 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.output.LayerNorm.weight from model.bert.encoder.layer.9.attention.output.LayerNorm.weight\n","\u001b[32m2024-04-26T00:40:11 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.output.LayerNorm.bias from model.bert.encoder.layer.9.attention.output.LayerNorm.bias\n","\u001b[32m2024-04-26T00:40:11 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.intermediate.dense.weight from model.bert.encoder.layer.9.intermediate.dense.weight\n","\u001b[32m2024-04-26T00:40:11 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.intermediate.dense.bias from model.bert.encoder.layer.9.intermediate.dense.bias\n","\u001b[32m2024-04-26T00:40:11 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.output.dense.weight from model.bert.encoder.layer.9.output.dense.weight\n","\u001b[32m2024-04-26T00:40:11 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.output.dense.bias from model.bert.encoder.layer.9.output.dense.bias\n","\u001b[32m2024-04-26T00:40:11 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.output.LayerNorm.weight from model.bert.encoder.layer.9.output.LayerNorm.weight\n","\u001b[32m2024-04-26T00:40:11 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.output.LayerNorm.bias from model.bert.encoder.layer.9.output.LayerNorm.bias\n","\u001b[32m2024-04-26T00:40:11 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.self.query.weight from model.bert.encoder.layer.10.attention.self.query.weight\n","\u001b[32m2024-04-26T00:40:11 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.self.query.bias from model.bert.encoder.layer.10.attention.self.query.bias\n","\u001b[32m2024-04-26T00:40:11 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.self.key.weight from model.bert.encoder.layer.10.attention.self.key.weight\n","\u001b[32m2024-04-26T00:40:11 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.self.key.bias from model.bert.encoder.layer.10.attention.self.key.bias\n","\u001b[32m2024-04-26T00:40:11 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.self.value.weight from model.bert.encoder.layer.10.attention.self.value.weight\n","\u001b[32m2024-04-26T00:40:11 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.self.value.bias from model.bert.encoder.layer.10.attention.self.value.bias\n","\u001b[32m2024-04-26T00:40:11 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.output.dense.weight from model.bert.encoder.layer.10.attention.output.dense.weight\n","\u001b[32m2024-04-26T00:40:11 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.output.dense.bias from model.bert.encoder.layer.10.attention.output.dense.bias\n","\u001b[32m2024-04-26T00:40:11 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.output.LayerNorm.weight from model.bert.encoder.layer.10.attention.output.LayerNorm.weight\n","\u001b[32m2024-04-26T00:40:11 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.output.LayerNorm.bias from model.bert.encoder.layer.10.attention.output.LayerNorm.bias\n","\u001b[32m2024-04-26T00:40:11 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.intermediate.dense.weight from model.bert.encoder.layer.10.intermediate.dense.weight\n","\u001b[32m2024-04-26T00:40:11 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.intermediate.dense.bias from model.bert.encoder.layer.10.intermediate.dense.bias\n","\u001b[32m2024-04-26T00:40:11 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.output.dense.weight from model.bert.encoder.layer.10.output.dense.weight\n","\u001b[32m2024-04-26T00:40:11 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.output.dense.bias from model.bert.encoder.layer.10.output.dense.bias\n","\u001b[32m2024-04-26T00:40:11 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.output.LayerNorm.weight from model.bert.encoder.layer.10.output.LayerNorm.weight\n","\u001b[32m2024-04-26T00:40:11 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.output.LayerNorm.bias from model.bert.encoder.layer.10.output.LayerNorm.bias\n","\u001b[32m2024-04-26T00:40:11 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.self.query.weight from model.bert.encoder.layer.11.attention.self.query.weight\n","\u001b[32m2024-04-26T00:40:11 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.self.query.bias from model.bert.encoder.layer.11.attention.self.query.bias\n","\u001b[32m2024-04-26T00:40:11 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.self.key.weight from model.bert.encoder.layer.11.attention.self.key.weight\n","\u001b[32m2024-04-26T00:40:11 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.self.key.bias from model.bert.encoder.layer.11.attention.self.key.bias\n","\u001b[32m2024-04-26T00:40:11 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.self.value.weight from model.bert.encoder.layer.11.attention.self.value.weight\n","\u001b[32m2024-04-26T00:40:11 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.self.value.bias from model.bert.encoder.layer.11.attention.self.value.bias\n","\u001b[32m2024-04-26T00:40:11 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.output.dense.weight from model.bert.encoder.layer.11.attention.output.dense.weight\n","\u001b[32m2024-04-26T00:40:11 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.output.dense.bias from model.bert.encoder.layer.11.attention.output.dense.bias\n","\u001b[32m2024-04-26T00:40:11 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.output.LayerNorm.weight from model.bert.encoder.layer.11.attention.output.LayerNorm.weight\n","\u001b[32m2024-04-26T00:40:11 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.output.LayerNorm.bias from model.bert.encoder.layer.11.attention.output.LayerNorm.bias\n","\u001b[32m2024-04-26T00:40:11 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.intermediate.dense.weight from model.bert.encoder.layer.11.intermediate.dense.weight\n","\u001b[32m2024-04-26T00:40:11 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.intermediate.dense.bias from model.bert.encoder.layer.11.intermediate.dense.bias\n","\u001b[32m2024-04-26T00:40:11 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.output.dense.weight from model.bert.encoder.layer.11.output.dense.weight\n","\u001b[32m2024-04-26T00:40:11 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.output.dense.bias from model.bert.encoder.layer.11.output.dense.bias\n","\u001b[32m2024-04-26T00:40:11 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.output.LayerNorm.weight from model.bert.encoder.layer.11.output.LayerNorm.weight\n","\u001b[32m2024-04-26T00:40:11 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.output.LayerNorm.bias from model.bert.encoder.layer.11.output.LayerNorm.bias\n","\u001b[32m2024-04-26T00:40:11 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.pooler.dense.weight from model.bert.pooler.dense.weight\n","\u001b[32m2024-04-26T00:40:11 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.pooler.dense.bias from model.bert.pooler.dense.bias\n","\u001b[32m2024-04-26T00:40:11 | mmf.utils.checkpoint: \u001b[0mPretrained model loaded\n","\u001b[32m2024-04-26T00:40:11 | mmf.utils.checkpoint: \u001b[0mCheckpoint loaded.\n","\u001b[32m2024-04-26T00:40:11 | mmf.utils.checkpoint: \u001b[0mCurrent num updates: 0\n","\u001b[32m2024-04-26T00:40:11 | mmf.utils.checkpoint: \u001b[0mCurrent iteration: 0\n","\u001b[32m2024-04-26T00:40:11 | mmf.utils.checkpoint: \u001b[0mCurrent epoch: 0\n","\u001b[32m2024-04-26T00:40:11 | mmf.trainers.mmf_trainer: \u001b[0m===== Model =====\n","\u001b[32m2024-04-26T00:40:11 | mmf.trainers.mmf_trainer: \u001b[0mVisualBERT(\n","  (model): VisualBERTForClassification(\n","    (bert): VisualBERTBase(\n","      (embeddings): BertVisioLinguisticEmbeddings(\n","        (word_embeddings): Embedding(30522, 768, padding_idx=0)\n","        (position_embeddings): Embedding(512, 768)\n","        (token_type_embeddings): Embedding(2, 768)\n","        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","        (token_type_embeddings_visual): Embedding(2, 768)\n","        (position_embeddings_visual): Embedding(512, 768)\n","        (projection): Linear(in_features=2048, out_features=768, bias=True)\n","      )\n","      (encoder): BertEncoderJit(\n","        (layer): ModuleList(\n","          (0-11): 12 x BertLayerJit(\n","            (attention): BertAttentionJit(\n","              (self): BertSelfAttentionJit(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","              (intermediate_act_fn): GELUActivation()\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","        )\n","      )\n","      (pooler): BertPooler(\n","        (dense): Linear(in_features=768, out_features=768, bias=True)\n","        (activation): Tanh()\n","      )\n","    )\n","    (dropout): Dropout(p=0.1, inplace=False)\n","    (classifier): Sequential(\n","      (0): BertPredictionHeadTransform(\n","        (dense): Linear(in_features=768, out_features=768, bias=True)\n","        (transform_act_fn): GELUActivation()\n","        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      )\n","      (1): Linear(in_features=768, out_features=2, bias=True)\n","    )\n","  )\n","  (losses): Losses(\n","    (losses): ModuleList(\n","      (0): MMFLoss(\n","        (loss_criterion): CrossEntropyLoss(\n","          (loss_fn): CrossEntropyLoss()\n","        )\n","      )\n","    )\n","  )\n",")\n","\u001b[32m2024-04-26T00:40:11 | mmf.utils.general: \u001b[0mTotal Parameters: 112044290. Trained Parameters: 112044290\n","\u001b[32m2024-04-26T00:40:11 | mmf.trainers.core.training_loop: \u001b[0mStarting training...\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2024-04-26T00:40:11 | py.warnings: \u001b[0m/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n","  self.pid = os.fork()\n","\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2024-04-26T00:40:11 | py.warnings: \u001b[0m/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n","  self.pid = os.fork()\n","\n","\u001b[32m2024-04-26T00:42:15 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n","\u001b[32m2024-04-26T00:42:15 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2024-04-26T00:42:18 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2024-04-26T00:42:23 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2024-04-26T00:42:24 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 200/1500, train/hateful_memes/cross_entropy: 0.5909, train/hateful_memes/cross_entropy/avg: 0.5909, train/total_loss: 0.5909, train/total_loss/avg: 0.5909, max mem: 14364.0, experiment: run, epoch: 1, num_updates: 200, iterations: 200, max_updates: 1500, lr: 0.00001, ups: 1.52, time: 02m 12s 151ms, time_since_start: 02m 12s 805ms, eta: 14m 57s 636ms\n","\u001b[32m2024-04-26T00:42:24 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n","\u001b[32m2024-04-26T00:42:27 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n","\u001b[32m2024-04-26T00:42:29 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished evaluation inference. Loaded 9\n","\u001b[32m2024-04-26T00:42:29 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n","\u001b[32m2024-04-26T00:42:30 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2024-04-26T00:42:34 | mmf.utils.checkpoint: \u001b[0mSaving best checkpoint\n","\u001b[32m2024-04-26T00:42:37 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2024-04-26T00:42:42 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2024-04-26T00:42:42 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 200/1500, val/hateful_memes/cross_entropy: 0.6625, val/total_loss: 0.6625, val/hateful_memes/accuracy: 0.6278, val/hateful_memes/binary_f1: 0.1590, val/hateful_memes/roc_auc: 0.5787, num_updates: 200, epoch: 1, iterations: 200, max_updates: 1500, val_time: 17s 440ms, best_update: 200, best_iteration: 200, best_val/hateful_memes/roc_auc: 0.578750\n","\u001b[32m2024-04-26T00:44:45 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n","\u001b[32m2024-04-26T00:44:45 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2024-04-26T00:44:48 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2024-04-26T00:44:53 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2024-04-26T00:44:53 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 400/1500, train/hateful_memes/cross_entropy: 0.3724, train/hateful_memes/cross_entropy/avg: 0.4816, train/total_loss: 0.3724, train/total_loss/avg: 0.4816, max mem: 14413.0, experiment: run, epoch: 1, num_updates: 400, iterations: 400, max_updates: 1500, lr: 0.00001, ups: 1.53, time: 02m 11s 670ms, time_since_start: 04m 42s 878ms, eta: 12m 36s 776ms\n","\u001b[32m2024-04-26T00:44:53 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n","\u001b[32m2024-04-26T00:44:53 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2024-04-26T00:44:53 | py.warnings: \u001b[0m/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n","  self.pid = os.fork()\n","\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2024-04-26T00:44:53 | py.warnings: \u001b[0m/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n","  self.pid = os.fork()\n","\n","\u001b[32m2024-04-26T00:44:56 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished evaluation inference. Loaded 9\n","\u001b[32m2024-04-26T00:44:56 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n","\u001b[32m2024-04-26T00:44:56 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2024-04-26T00:44:59 | mmf.utils.checkpoint: \u001b[0mSaving best checkpoint\n","\u001b[32m2024-04-26T00:45:04 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2024-04-26T00:45:09 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2024-04-26T00:45:09 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 400/1500, val/hateful_memes/cross_entropy: 0.8118, val/total_loss: 0.8118, val/hateful_memes/accuracy: 0.6278, val/hateful_memes/binary_f1: 0.2637, val/hateful_memes/roc_auc: 0.6547, num_updates: 400, epoch: 1, iterations: 400, max_updates: 1500, val_time: 15s 691ms, best_update: 400, best_iteration: 400, best_val/hateful_memes/roc_auc: 0.654662\n","\u001b[32m2024-04-26T00:47:12 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n","\u001b[32m2024-04-26T00:47:12 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2024-04-26T00:47:15 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2024-04-26T00:47:19 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2024-04-26T00:47:19 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 600/1500, train/hateful_memes/cross_entropy: 0.3724, train/hateful_memes/cross_entropy/avg: 0.4152, train/total_loss: 0.3724, train/total_loss/avg: 0.4152, max mem: 14413.0, experiment: run, epoch: 2, num_updates: 600, iterations: 600, max_updates: 1500, lr: 0.00002, ups: 1.54, time: 02m 10s 308ms, time_since_start: 07m 08s 879ms, eta: 10m 12s 774ms\n","\u001b[32m2024-04-26T00:47:19 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n","\u001b[32m2024-04-26T00:47:19 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2024-04-26T00:47:19 | py.warnings: \u001b[0m/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n","  self.pid = os.fork()\n","\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2024-04-26T00:47:19 | py.warnings: \u001b[0m/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n","  self.pid = os.fork()\n","\n","\u001b[32m2024-04-26T00:47:22 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished evaluation inference. Loaded 9\n","\u001b[32m2024-04-26T00:47:22 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n","\u001b[32m2024-04-26T00:47:22 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2024-04-26T00:47:25 | mmf.utils.checkpoint: \u001b[0mSaving best checkpoint\n","\u001b[32m2024-04-26T00:47:30 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2024-04-26T00:47:38 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2024-04-26T00:47:38 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 600/1500, val/hateful_memes/cross_entropy: 0.8072, val/total_loss: 0.8072, val/hateful_memes/accuracy: 0.6685, val/hateful_memes/binary_f1: 0.4592, val/hateful_memes/roc_auc: 0.7033, num_updates: 600, epoch: 2, iterations: 600, max_updates: 1500, val_time: 18s 350ms, best_update: 600, best_iteration: 600, best_val/hateful_memes/roc_auc: 0.703265\n","\u001b[32m2024-04-26T00:49:41 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n","\u001b[32m2024-04-26T00:49:41 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2024-04-26T00:49:44 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2024-04-26T00:49:51 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2024-04-26T00:49:51 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 800/1500, train/hateful_memes/cross_entropy: 0.2824, train/hateful_memes/cross_entropy/avg: 0.3642, train/total_loss: 0.2824, train/total_loss/avg: 0.3642, max mem: 14413.0, experiment: run, epoch: 2, num_updates: 800, iterations: 800, max_updates: 1500, lr: 0.00002, ups: 1.52, time: 02m 12s 985ms, time_since_start: 09m 40s 218ms, eta: 08m 06s 395ms\n","\u001b[32m2024-04-26T00:49:51 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n","\u001b[32m2024-04-26T00:49:51 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2024-04-26T00:49:51 | py.warnings: \u001b[0m/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n","  self.pid = os.fork()\n","\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2024-04-26T00:49:51 | py.warnings: \u001b[0m/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n","  self.pid = os.fork()\n","\n","\u001b[32m2024-04-26T00:49:53 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished evaluation inference. Loaded 9\n","\u001b[32m2024-04-26T00:49:53 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n","\u001b[32m2024-04-26T00:49:53 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2024-04-26T00:49:57 | mmf.utils.checkpoint: \u001b[0mSaving best checkpoint\n","\u001b[32m2024-04-26T00:50:01 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2024-04-26T00:50:08 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2024-04-26T00:50:08 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 800/1500, val/hateful_memes/cross_entropy: 0.8096, val/total_loss: 0.8096, val/hateful_memes/accuracy: 0.6852, val/hateful_memes/binary_f1: 0.5058, val/hateful_memes/roc_auc: 0.7062, num_updates: 800, epoch: 2, iterations: 800, max_updates: 1500, val_time: 17s 185ms, best_update: 800, best_iteration: 800, best_val/hateful_memes/roc_auc: 0.706206\n","\u001b[32m2024-04-26T00:52:11 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n","\u001b[32m2024-04-26T00:52:11 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2024-04-26T00:52:14 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2024-04-26T00:52:21 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2024-04-26T00:52:21 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1000/1500, train/hateful_memes/cross_entropy: 0.2824, train/hateful_memes/cross_entropy/avg: 0.3189, train/total_loss: 0.2824, train/total_loss/avg: 0.3189, max mem: 14414.0, experiment: run, epoch: 3, num_updates: 1000, iterations: 1000, max_updates: 1500, lr: 0.00003, ups: 1.52, time: 02m 12s 907ms, time_since_start: 12m 10s 313ms, eta: 05m 47s 221ms\n","\u001b[32m2024-04-26T00:52:21 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n","\u001b[32m2024-04-26T00:52:21 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2024-04-26T00:52:21 | py.warnings: \u001b[0m/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n","  self.pid = os.fork()\n","\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2024-04-26T00:52:21 | py.warnings: \u001b[0m/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n","  self.pid = os.fork()\n","\n","\u001b[32m2024-04-26T00:52:23 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished evaluation inference. Loaded 9\n","\u001b[32m2024-04-26T00:52:23 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n","\u001b[32m2024-04-26T00:52:23 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2024-04-26T00:52:27 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2024-04-26T00:52:31 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2024-04-26T00:52:31 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1000/1500, val/hateful_memes/cross_entropy: 1.2270, val/total_loss: 1.2270, val/hateful_memes/accuracy: 0.6722, val/hateful_memes/binary_f1: 0.3656, val/hateful_memes/roc_auc: 0.6715, num_updates: 1000, epoch: 3, iterations: 1000, max_updates: 1500, val_time: 10s 081ms, best_update: 800, best_iteration: 800, best_val/hateful_memes/roc_auc: 0.706206\n","\u001b[32m2024-04-26T00:54:34 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n","\u001b[32m2024-04-26T00:54:34 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2024-04-26T00:54:37 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2024-04-26T00:54:41 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2024-04-26T00:54:41 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1200/1500, train/hateful_memes/cross_entropy: 0.2110, train/hateful_memes/cross_entropy/avg: 0.2785, train/total_loss: 0.2110, train/total_loss/avg: 0.2785, max mem: 14414.0, experiment: run, epoch: 3, num_updates: 1200, iterations: 1200, max_updates: 1500, lr: 0.00003, ups: 1.54, time: 02m 10s 007ms, time_since_start: 14m 30s 404ms, eta: 03m 23s 786ms\n","\u001b[32m2024-04-26T00:54:41 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n","\u001b[32m2024-04-26T00:54:41 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2024-04-26T00:54:41 | py.warnings: \u001b[0m/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n","  self.pid = os.fork()\n","\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2024-04-26T00:54:41 | py.warnings: \u001b[0m/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n","  self.pid = os.fork()\n","\n","\u001b[32m2024-04-26T00:54:43 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished evaluation inference. Loaded 9\n","\u001b[32m2024-04-26T00:54:43 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n","\u001b[32m2024-04-26T00:54:43 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2024-04-26T00:54:49 | mmf.utils.checkpoint: \u001b[0mSaving best checkpoint\n","\u001b[32m2024-04-26T00:54:52 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2024-04-26T00:54:59 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2024-04-26T00:54:59 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1200/1500, val/hateful_memes/cross_entropy: 1.2017, val/total_loss: 1.2017, val/hateful_memes/accuracy: 0.6926, val/hateful_memes/binary_f1: 0.5174, val/hateful_memes/roc_auc: 0.7162, num_updates: 1200, epoch: 3, iterations: 1200, max_updates: 1500, val_time: 18s 420ms, best_update: 1200, best_iteration: 1200, best_val/hateful_memes/roc_auc: 0.716221\n","\u001b[32m2024-04-26T00:57:04 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n","\u001b[32m2024-04-26T00:57:04 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2024-04-26T00:57:07 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2024-04-26T00:57:11 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2024-04-26T00:57:11 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1400/1500, train/hateful_memes/cross_entropy: 0.2110, train/hateful_memes/cross_entropy/avg: 0.2523, train/total_loss: 0.2110, train/total_loss/avg: 0.2523, max mem: 14414.0, experiment: run, epoch: 4, num_updates: 1400, iterations: 1400, max_updates: 1500, lr: 0.00003, ups: 1.53, time: 02m 11s 821ms, time_since_start: 17m 648ms, eta: 01m 08s 876ms\n","\u001b[32m2024-04-26T00:57:11 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n","\u001b[32m2024-04-26T00:57:11 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2024-04-26T00:57:11 | py.warnings: \u001b[0m/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n","  self.pid = os.fork()\n","\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2024-04-26T00:57:11 | py.warnings: \u001b[0m/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n","  self.pid = os.fork()\n","\n","\u001b[32m2024-04-26T00:57:13 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished evaluation inference. Loaded 9\n","\u001b[32m2024-04-26T00:57:13 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n","\u001b[32m2024-04-26T00:57:13 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2024-04-26T00:57:18 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2024-04-26T00:57:22 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2024-04-26T00:57:22 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1400/1500, val/hateful_memes/cross_entropy: 1.2939, val/total_loss: 1.2939, val/hateful_memes/accuracy: 0.6759, val/hateful_memes/binary_f1: 0.4681, val/hateful_memes/roc_auc: 0.7073, num_updates: 1400, epoch: 4, iterations: 1400, max_updates: 1500, val_time: 10s 856ms, best_update: 1200, best_iteration: 1200, best_val/hateful_memes/roc_auc: 0.716221\n","\u001b[32m2024-04-26T00:58:25 | mmf.trainers.core.training_loop: \u001b[0mStepping into final validation check\n","\u001b[32m2024-04-26T00:58:25 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2024-04-26T00:58:25 | py.warnings: \u001b[0m/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n","  self.pid = os.fork()\n","\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2024-04-26T00:58:25 | py.warnings: \u001b[0m/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n","  self.pid = os.fork()\n","\n","\u001b[32m2024-04-26T00:58:27 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished evaluation inference. Loaded 9\n","\u001b[32m2024-04-26T00:58:27 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n","\u001b[32m2024-04-26T00:58:27 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1500/1500, val/hateful_memes/cross_entropy: 1.7860, val/total_loss: 1.7860, val/hateful_memes/accuracy: 0.6759, val/hateful_memes/binary_f1: 0.3542, val/hateful_memes/roc_auc: 0.6984, num_updates: 1500, epoch: 4, iterations: 1500, max_updates: 1500, val_time: 01m 15s 986ms, best_update: 1200, best_iteration: 1200, best_val/hateful_memes/roc_auc: 0.716221\n","\u001b[32m2024-04-26T00:58:27 | mmf.utils.checkpoint: \u001b[0mRestoring checkpoint\n","\u001b[32m2024-04-26T00:58:27 | mmf.utils.checkpoint: \u001b[0mLoading checkpoint\n","\u001b[32m2024-04-26T00:58:29 | mmf.utils.checkpoint: \u001b[0mCheckpoint loaded.\n","\u001b[32m2024-04-26T00:58:29 | mmf.utils.checkpoint: \u001b[0mCurrent num updates: 1200\n","\u001b[32m2024-04-26T00:58:29 | mmf.utils.checkpoint: \u001b[0mCurrent iteration: 1200\n","\u001b[32m2024-04-26T00:58:29 | mmf.utils.checkpoint: \u001b[0mCurrent epoch: 3\n","\u001b[32m2024-04-26T00:58:30 | mmf.trainers.mmf_trainer: \u001b[0mStarting inference on val set\n","\u001b[32m2024-04-26T00:58:30 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n","  0% 0/9 [00:00<?, ?it/s]\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2024-04-26T00:58:30 | py.warnings: \u001b[0m/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n","  self.pid = os.fork()\n","\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2024-04-26T00:58:30 | py.warnings: \u001b[0m/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n","  self.pid = os.fork()\n","\n","100% 9/9 [00:02<00:00,  4.00it/s]\n","\u001b[32m2024-04-26T00:58:32 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished evaluation inference. Loaded 9\n","\u001b[32m2024-04-26T00:58:32 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n","\u001b[32m2024-04-26T00:58:32 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1200/1500, val/hateful_memes/cross_entropy: 1.2017, val/total_loss: 1.2017, val/hateful_memes/accuracy: 0.6926, val/hateful_memes/binary_f1: 0.5174, val/hateful_memes/roc_auc: 0.7162\n","\u001b[32m2024-04-26T00:58:32 | mmf.trainers.callbacks.logistics: \u001b[0mFinished run in 18m 21s 698ms\n"]}]},{"cell_type":"markdown","source":["### Dataset: Augmented with combined transformations"],"metadata":{"id":"76HjJsngYLVs"}},{"cell_type":"code","source":["!mmf_run config=\"projects/visual_bert/configs/hateful_memes/defaults.yaml\" \\\n","  model=visual_bert \\\n","  dataset=hateful_memes \\\n","  run_type=train_val \\\n","  training.log_interval=200 \\\n","  training.batch_size=64 \\\n","  training.evaluation_interval=200 \\\n","  training.tensorboard=True \\\n","  training.checkpoint_interval=200 \\\n","  checkpoint.resume_pretrained=True \\\n","  checkpoint.resume_zoo=visual_bert.pretrained.cc \\\n","  dataset_config.hateful_memes.annotations.train[0]=\"hateful_memes/defaults/annotations/train_dev_transform_all_combined.jsonl\" \\\n","  dataset_config.hateful_memes.annotations.val[0]=\"hateful_memes/defaults/annotations/dev_unseen.jsonl\" \\\n","  dataset_config.hateful_memes.annotations.test[0]=\"hateful_memes/defaults/annotations/test_unseen.jsonl\""],"metadata":{"id":"E5I5j8qnVu1D","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1714094588309,"user_tz":420,"elapsed":1114608,"user":{"displayName":"Patrick Prestridge","userId":"01452612983786479124"}},"outputId":"6718c9cb-b532-4f0a-e088-38dad28bd4ed"},"execution_count":41,"outputs":[{"output_type":"stream","name":"stdout","text":["2024-04-26 01:04:36.501174: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","2024-04-26 01:04:36.501227: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","2024-04-26 01:04:36.502894: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","2024-04-26 01:04:37.642815: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","2024-04-26 01:04:42.690921: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:47] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n","/usr/local/lib/python3.10/dist-packages/transformers/deepspeed.py:23: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/omegaconf/grammar_visitor.py:253: UserWarning: In the sequence `MMF_USER_DIR,` some elements are missing: please replace them with empty quoted strings. See https://github.com/omry/omegaconf/issues/572 for details.\n","  warnings.warn(\n","\u001b[32m2024-04-26T01:04:47 | mmf.utils.configuration: \u001b[0mOverriding option config to projects/visual_bert/configs/hateful_memes/defaults.yaml\n","\u001b[32m2024-04-26T01:04:47 | mmf.utils.configuration: \u001b[0mOverriding option model to visual_bert\n","\u001b[32m2024-04-26T01:04:47 | mmf.utils.configuration: \u001b[0mOverriding option datasets to hateful_memes\n","\u001b[32m2024-04-26T01:04:47 | mmf.utils.configuration: \u001b[0mOverriding option run_type to train_val\n","\u001b[32m2024-04-26T01:04:47 | mmf.utils.configuration: \u001b[0mOverriding option training.log_interval to 200\n","\u001b[32m2024-04-26T01:04:47 | mmf.utils.configuration: \u001b[0mOverriding option training.batch_size to 64\n","\u001b[32m2024-04-26T01:04:47 | mmf.utils.configuration: \u001b[0mOverriding option training.evaluation_interval to 200\n","\u001b[32m2024-04-26T01:04:47 | mmf.utils.configuration: \u001b[0mOverriding option training.tensorboard to True\n","\u001b[32m2024-04-26T01:04:47 | mmf.utils.configuration: \u001b[0mOverriding option training.checkpoint_interval to 200\n","\u001b[32m2024-04-26T01:04:47 | mmf.utils.configuration: \u001b[0mOverriding option checkpoint.resume_pretrained to True\n","\u001b[32m2024-04-26T01:04:47 | mmf.utils.configuration: \u001b[0mOverriding option checkpoint.resume_zoo to visual_bert.pretrained.cc\n","\u001b[32m2024-04-26T01:04:47 | mmf.utils.configuration: \u001b[0mOverriding option dataset_config.hateful_memes.annotations.train[0] to hateful_memes/defaults/annotations/train_dev_transform_all_combined.jsonl\n","\u001b[32m2024-04-26T01:04:47 | mmf.utils.configuration: \u001b[0mOverriding option dataset_config.hateful_memes.annotations.val[0] to hateful_memes/defaults/annotations/dev_unseen.jsonl\n","\u001b[32m2024-04-26T01:04:47 | mmf.utils.configuration: \u001b[0mOverriding option dataset_config.hateful_memes.annotations.test[0] to hateful_memes/defaults/annotations/test_unseen.jsonl\n","/usr/local/lib/python3.10/dist-packages/omegaconf/grammar_visitor.py:253: UserWarning: In the sequence `MMF_LOG_DIR,` some elements are missing: please replace them with empty quoted strings. See https://github.com/omry/omegaconf/issues/572 for details.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/omegaconf/grammar_visitor.py:253: UserWarning: In the sequence `MMF_REPORT_DIR,` some elements are missing: please replace them with empty quoted strings. See https://github.com/omry/omegaconf/issues/572 for details.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/omegaconf/grammar_visitor.py:253: UserWarning: In the sequence `MMF_TENSORBOARD_LOGDIR,` some elements are missing: please replace them with empty quoted strings. See https://github.com/omry/omegaconf/issues/572 for details.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/omegaconf/grammar_visitor.py:253: UserWarning: In the sequence `MMF_WANDB_LOGDIR,` some elements are missing: please replace them with empty quoted strings. See https://github.com/omry/omegaconf/issues/572 for details.\n","  warnings.warn(\n","\u001b[32m2024-04-26T01:04:47 | mmf: \u001b[0mLogging to: ./save/train.log\n","\u001b[32m2024-04-26T01:04:47 | mmf_cli.run: \u001b[0mNamespace(config_override=None, local_rank=None, opts=['config=projects/visual_bert/configs/hateful_memes/defaults.yaml', 'model=visual_bert', 'dataset=hateful_memes', 'run_type=train_val', 'training.log_interval=200', 'training.batch_size=64', 'training.evaluation_interval=200', 'training.tensorboard=True', 'training.checkpoint_interval=200', 'checkpoint.resume_pretrained=True', 'checkpoint.resume_zoo=visual_bert.pretrained.cc', 'dataset_config.hateful_memes.annotations.train[0]=hateful_memes/defaults/annotations/train_dev_transform_all_combined.jsonl', 'dataset_config.hateful_memes.annotations.val[0]=hateful_memes/defaults/annotations/dev_unseen.jsonl', 'dataset_config.hateful_memes.annotations.test[0]=hateful_memes/defaults/annotations/test_unseen.jsonl'])\n","\u001b[32m2024-04-26T01:04:47 | mmf_cli.run: \u001b[0mTorch version: 2.2.0+cu121\n","\u001b[32m2024-04-26T01:04:47 | mmf.utils.general: \u001b[0mCUDA Device 0 is: NVIDIA A100-SXM4-40GB\n","\u001b[32m2024-04-26T01:04:47 | mmf_cli.run: \u001b[0mUsing seed 47745992\n","\u001b[32m2024-04-26T01:04:47 | mmf.trainers.mmf_trainer: \u001b[0mLoading datasets\n","loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--bert-base-uncased/snapshots/86b5e0934494bd15c9632b12f734a8a67f723594/config.json\n","Model config BertConfig {\n","  \"_name_or_path\": \"bert-base-uncased\",\n","  \"architectures\": [\n","    \"BertForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.39.2\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n","loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--bert-base-uncased/snapshots/86b5e0934494bd15c9632b12f734a8a67f723594/vocab.txt\n","loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--bert-base-uncased/snapshots/86b5e0934494bd15c9632b12f734a8a67f723594/tokenizer.json\n","loading file added_tokens.json from cache at None\n","loading file special_tokens_map.json from cache at None\n","loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--bert-base-uncased/snapshots/86b5e0934494bd15c9632b12f734a8a67f723594/tokenizer_config.json\n","loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--bert-base-uncased/snapshots/86b5e0934494bd15c9632b12f734a8a67f723594/config.json\n","Model config BertConfig {\n","  \"_name_or_path\": \"bert-base-uncased\",\n","  \"architectures\": [\n","    \"BertForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.39.2\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n","\u001b[32m2024-04-26T01:04:48 | mmf.datasets.multi_datamodule: \u001b[0mMultitasking disabled by default for single dataset training\n","\u001b[32m2024-04-26T01:04:48 | mmf.datasets.multi_datamodule: \u001b[0mMultitasking disabled by default for single dataset training\n","\u001b[32m2024-04-26T01:04:48 | mmf.datasets.multi_datamodule: \u001b[0mMultitasking disabled by default for single dataset training\n","\u001b[32m2024-04-26T01:04:48 | mmf.trainers.mmf_trainer: \u001b[0mLoading model\n","Model config BertConfig {\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bert_model_name\": \"bert-base-uncased\",\n","  \"bypass_transformer\": false,\n","  \"classifier_dropout\": null,\n","  \"embedding_strategy\": \"plain\",\n","  \"finetune_lr_multiplier\": 1,\n","  \"freeze_base\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"losses\": [\n","    \"cross_entropy\"\n","  ],\n","  \"max_position_embeddings\": 512,\n","  \"model\": \"visual_bert\",\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"pooler_strategy\": \"default\",\n","  \"position_embedding_type\": \"absolute\",\n","  \"random_initialize\": false,\n","  \"special_visual_initialize\": true,\n","  \"training_head_type\": \"classification\",\n","  \"transformers_version\": \"4.39.2\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"visual_embedding_dim\": 2048,\n","  \"vocab_size\": 30522,\n","  \"zerobias\": false\n","}\n","\n","loading weights file model.safetensors from cache at /root/.cache/torch/mmf/distributed_-1/models--bert-base-uncased/snapshots/86b5e0934494bd15c9632b12f734a8a67f723594/model.safetensors\n","Some weights of the model checkpoint at bert-base-uncased were not used when initializing VisualBERTBase: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n","- This IS expected if you are initializing VisualBERTBase from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing VisualBERTBase from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of VisualBERTBase were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['bert.embeddings.position_embeddings_visual.weight', 'bert.embeddings.projection.bias', 'bert.embeddings.projection.weight', 'bert.embeddings.token_type_embeddings_visual.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","\u001b[32m2024-04-26T01:04:48 | mmf.trainers.mmf_trainer: \u001b[0mLoading optimizer\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2024-04-26T01:04:48 | py.warnings: \u001b[0m/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:457: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n","\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2024-04-26T01:04:48 | py.warnings: \u001b[0m/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:457: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n","\n","\u001b[32m2024-04-26T01:04:48 | mmf.trainers.mmf_trainer: \u001b[0mLoading metrics\n","\u001b[32m2024-04-26T01:04:48 | mmf.utils.checkpoint: \u001b[0mLoading checkpoint\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2024-04-26T01:04:49 | mmf: \u001b[0mKey data_parallel is not present in registry, returning default value of None\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2024-04-26T01:04:49 | mmf: \u001b[0mKey distributed is not present in registry, returning default value of None\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2024-04-26T01:04:49 | mmf: \u001b[0mKey data_parallel is not present in registry, returning default value of None\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2024-04-26T01:04:49 | mmf: \u001b[0mKey distributed is not present in registry, returning default value of None\n","\u001b[32m2024-04-26T01:04:49 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.word_embeddings.weight from model.bert.embeddings.word_embeddings.weight\n","\u001b[32m2024-04-26T01:04:49 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.position_embeddings.weight from model.bert.embeddings.position_embeddings.weight\n","\u001b[32m2024-04-26T01:04:49 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.token_type_embeddings.weight from model.bert.embeddings.token_type_embeddings.weight\n","\u001b[32m2024-04-26T01:04:49 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.LayerNorm.weight from model.bert.embeddings.LayerNorm.weight\n","\u001b[32m2024-04-26T01:04:49 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.LayerNorm.bias from model.bert.embeddings.LayerNorm.bias\n","\u001b[32m2024-04-26T01:04:49 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.token_type_embeddings_visual.weight from model.bert.embeddings.token_type_embeddings_visual.weight\n","\u001b[32m2024-04-26T01:04:49 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.position_embeddings_visual.weight from model.bert.embeddings.position_embeddings_visual.weight\n","\u001b[32m2024-04-26T01:04:49 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.projection.weight from model.bert.embeddings.projection.weight\n","\u001b[32m2024-04-26T01:04:49 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.projection.bias from model.bert.embeddings.projection.bias\n","\u001b[32m2024-04-26T01:04:49 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.self.query.weight from model.bert.encoder.layer.0.attention.self.query.weight\n","\u001b[32m2024-04-26T01:04:49 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.self.query.bias from model.bert.encoder.layer.0.attention.self.query.bias\n","\u001b[32m2024-04-26T01:04:49 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.self.key.weight from model.bert.encoder.layer.0.attention.self.key.weight\n","\u001b[32m2024-04-26T01:04:49 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.self.key.bias from model.bert.encoder.layer.0.attention.self.key.bias\n","\u001b[32m2024-04-26T01:04:49 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.self.value.weight from model.bert.encoder.layer.0.attention.self.value.weight\n","\u001b[32m2024-04-26T01:04:49 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.self.value.bias from model.bert.encoder.layer.0.attention.self.value.bias\n","\u001b[32m2024-04-26T01:04:49 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.output.dense.weight from model.bert.encoder.layer.0.attention.output.dense.weight\n","\u001b[32m2024-04-26T01:04:49 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.output.dense.bias from model.bert.encoder.layer.0.attention.output.dense.bias\n","\u001b[32m2024-04-26T01:04:49 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.output.LayerNorm.weight from model.bert.encoder.layer.0.attention.output.LayerNorm.weight\n","\u001b[32m2024-04-26T01:04:49 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.output.LayerNorm.bias from model.bert.encoder.layer.0.attention.output.LayerNorm.bias\n","\u001b[32m2024-04-26T01:04:49 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.intermediate.dense.weight from model.bert.encoder.layer.0.intermediate.dense.weight\n","\u001b[32m2024-04-26T01:04:49 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.intermediate.dense.bias from model.bert.encoder.layer.0.intermediate.dense.bias\n","\u001b[32m2024-04-26T01:04:49 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.output.dense.weight from model.bert.encoder.layer.0.output.dense.weight\n","\u001b[32m2024-04-26T01:04:49 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.output.dense.bias from model.bert.encoder.layer.0.output.dense.bias\n","\u001b[32m2024-04-26T01:04:49 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.output.LayerNorm.weight from model.bert.encoder.layer.0.output.LayerNorm.weight\n","\u001b[32m2024-04-26T01:04:49 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.output.LayerNorm.bias from model.bert.encoder.layer.0.output.LayerNorm.bias\n","\u001b[32m2024-04-26T01:04:49 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.self.query.weight from model.bert.encoder.layer.1.attention.self.query.weight\n","\u001b[32m2024-04-26T01:04:49 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.self.query.bias from model.bert.encoder.layer.1.attention.self.query.bias\n","\u001b[32m2024-04-26T01:04:49 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.self.key.weight from model.bert.encoder.layer.1.attention.self.key.weight\n","\u001b[32m2024-04-26T01:04:49 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.self.key.bias from model.bert.encoder.layer.1.attention.self.key.bias\n","\u001b[32m2024-04-26T01:04:49 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.self.value.weight from model.bert.encoder.layer.1.attention.self.value.weight\n","\u001b[32m2024-04-26T01:04:49 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.self.value.bias from model.bert.encoder.layer.1.attention.self.value.bias\n","\u001b[32m2024-04-26T01:04:49 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.output.dense.weight from model.bert.encoder.layer.1.attention.output.dense.weight\n","\u001b[32m2024-04-26T01:04:49 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.output.dense.bias from model.bert.encoder.layer.1.attention.output.dense.bias\n","\u001b[32m2024-04-26T01:04:49 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.output.LayerNorm.weight from model.bert.encoder.layer.1.attention.output.LayerNorm.weight\n","\u001b[32m2024-04-26T01:04:49 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.output.LayerNorm.bias from model.bert.encoder.layer.1.attention.output.LayerNorm.bias\n","\u001b[32m2024-04-26T01:04:49 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.intermediate.dense.weight from model.bert.encoder.layer.1.intermediate.dense.weight\n","\u001b[32m2024-04-26T01:04:49 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.intermediate.dense.bias from model.bert.encoder.layer.1.intermediate.dense.bias\n","\u001b[32m2024-04-26T01:04:49 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.output.dense.weight from model.bert.encoder.layer.1.output.dense.weight\n","\u001b[32m2024-04-26T01:04:49 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.output.dense.bias from model.bert.encoder.layer.1.output.dense.bias\n","\u001b[32m2024-04-26T01:04:49 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.output.LayerNorm.weight from model.bert.encoder.layer.1.output.LayerNorm.weight\n","\u001b[32m2024-04-26T01:04:49 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.output.LayerNorm.bias from model.bert.encoder.layer.1.output.LayerNorm.bias\n","\u001b[32m2024-04-26T01:04:49 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.self.query.weight from model.bert.encoder.layer.2.attention.self.query.weight\n","\u001b[32m2024-04-26T01:04:49 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.self.query.bias from model.bert.encoder.layer.2.attention.self.query.bias\n","\u001b[32m2024-04-26T01:04:49 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.self.key.weight from model.bert.encoder.layer.2.attention.self.key.weight\n","\u001b[32m2024-04-26T01:04:49 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.self.key.bias from model.bert.encoder.layer.2.attention.self.key.bias\n","\u001b[32m2024-04-26T01:04:49 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.self.value.weight from model.bert.encoder.layer.2.attention.self.value.weight\n","\u001b[32m2024-04-26T01:04:49 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.self.value.bias from model.bert.encoder.layer.2.attention.self.value.bias\n","\u001b[32m2024-04-26T01:04:49 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.output.dense.weight from model.bert.encoder.layer.2.attention.output.dense.weight\n","\u001b[32m2024-04-26T01:04:49 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.output.dense.bias from model.bert.encoder.layer.2.attention.output.dense.bias\n","\u001b[32m2024-04-26T01:04:49 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.output.LayerNorm.weight from model.bert.encoder.layer.2.attention.output.LayerNorm.weight\n","\u001b[32m2024-04-26T01:04:49 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.output.LayerNorm.bias from model.bert.encoder.layer.2.attention.output.LayerNorm.bias\n","\u001b[32m2024-04-26T01:04:49 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.intermediate.dense.weight from model.bert.encoder.layer.2.intermediate.dense.weight\n","\u001b[32m2024-04-26T01:04:49 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.intermediate.dense.bias from model.bert.encoder.layer.2.intermediate.dense.bias\n","\u001b[32m2024-04-26T01:04:49 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.output.dense.weight from model.bert.encoder.layer.2.output.dense.weight\n","\u001b[32m2024-04-26T01:04:49 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.output.dense.bias from model.bert.encoder.layer.2.output.dense.bias\n","\u001b[32m2024-04-26T01:04:49 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.output.LayerNorm.weight from model.bert.encoder.layer.2.output.LayerNorm.weight\n","\u001b[32m2024-04-26T01:04:49 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.output.LayerNorm.bias from model.bert.encoder.layer.2.output.LayerNorm.bias\n","\u001b[32m2024-04-26T01:04:49 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.self.query.weight from model.bert.encoder.layer.3.attention.self.query.weight\n","\u001b[32m2024-04-26T01:04:49 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.self.query.bias from model.bert.encoder.layer.3.attention.self.query.bias\n","\u001b[32m2024-04-26T01:04:49 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.self.key.weight from model.bert.encoder.layer.3.attention.self.key.weight\n","\u001b[32m2024-04-26T01:04:49 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.self.key.bias from model.bert.encoder.layer.3.attention.self.key.bias\n","\u001b[32m2024-04-26T01:04:49 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.self.value.weight from model.bert.encoder.layer.3.attention.self.value.weight\n","\u001b[32m2024-04-26T01:04:49 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.self.value.bias from model.bert.encoder.layer.3.attention.self.value.bias\n","\u001b[32m2024-04-26T01:04:49 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.output.dense.weight from model.bert.encoder.layer.3.attention.output.dense.weight\n","\u001b[32m2024-04-26T01:04:49 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.output.dense.bias from model.bert.encoder.layer.3.attention.output.dense.bias\n","\u001b[32m2024-04-26T01:04:49 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.output.LayerNorm.weight from model.bert.encoder.layer.3.attention.output.LayerNorm.weight\n","\u001b[32m2024-04-26T01:04:49 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.output.LayerNorm.bias from model.bert.encoder.layer.3.attention.output.LayerNorm.bias\n","\u001b[32m2024-04-26T01:04:49 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.intermediate.dense.weight from model.bert.encoder.layer.3.intermediate.dense.weight\n","\u001b[32m2024-04-26T01:04:49 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.intermediate.dense.bias from model.bert.encoder.layer.3.intermediate.dense.bias\n","\u001b[32m2024-04-26T01:04:49 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.output.dense.weight from model.bert.encoder.layer.3.output.dense.weight\n","\u001b[32m2024-04-26T01:04:49 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.output.dense.bias from model.bert.encoder.layer.3.output.dense.bias\n","\u001b[32m2024-04-26T01:04:49 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.output.LayerNorm.weight from model.bert.encoder.layer.3.output.LayerNorm.weight\n","\u001b[32m2024-04-26T01:04:49 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.output.LayerNorm.bias from model.bert.encoder.layer.3.output.LayerNorm.bias\n","\u001b[32m2024-04-26T01:04:49 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.self.query.weight from model.bert.encoder.layer.4.attention.self.query.weight\n","\u001b[32m2024-04-26T01:04:49 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.self.query.bias from model.bert.encoder.layer.4.attention.self.query.bias\n","\u001b[32m2024-04-26T01:04:49 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.self.key.weight from model.bert.encoder.layer.4.attention.self.key.weight\n","\u001b[32m2024-04-26T01:04:49 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.self.key.bias from model.bert.encoder.layer.4.attention.self.key.bias\n","\u001b[32m2024-04-26T01:04:49 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.self.value.weight from model.bert.encoder.layer.4.attention.self.value.weight\n","\u001b[32m2024-04-26T01:04:49 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.self.value.bias from model.bert.encoder.layer.4.attention.self.value.bias\n","\u001b[32m2024-04-26T01:04:49 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.output.dense.weight from model.bert.encoder.layer.4.attention.output.dense.weight\n","\u001b[32m2024-04-26T01:04:49 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.output.dense.bias from model.bert.encoder.layer.4.attention.output.dense.bias\n","\u001b[32m2024-04-26T01:04:49 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.output.LayerNorm.weight from model.bert.encoder.layer.4.attention.output.LayerNorm.weight\n","\u001b[32m2024-04-26T01:04:49 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.output.LayerNorm.bias from model.bert.encoder.layer.4.attention.output.LayerNorm.bias\n","\u001b[32m2024-04-26T01:04:49 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.intermediate.dense.weight from model.bert.encoder.layer.4.intermediate.dense.weight\n","\u001b[32m2024-04-26T01:04:49 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.intermediate.dense.bias from model.bert.encoder.layer.4.intermediate.dense.bias\n","\u001b[32m2024-04-26T01:04:49 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.output.dense.weight from model.bert.encoder.layer.4.output.dense.weight\n","\u001b[32m2024-04-26T01:04:49 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.output.dense.bias from model.bert.encoder.layer.4.output.dense.bias\n","\u001b[32m2024-04-26T01:04:49 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.output.LayerNorm.weight from model.bert.encoder.layer.4.output.LayerNorm.weight\n","\u001b[32m2024-04-26T01:04:49 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.output.LayerNorm.bias from model.bert.encoder.layer.4.output.LayerNorm.bias\n","\u001b[32m2024-04-26T01:04:49 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.self.query.weight from model.bert.encoder.layer.5.attention.self.query.weight\n","\u001b[32m2024-04-26T01:04:49 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.self.query.bias from model.bert.encoder.layer.5.attention.self.query.bias\n","\u001b[32m2024-04-26T01:04:49 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.self.key.weight from model.bert.encoder.layer.5.attention.self.key.weight\n","\u001b[32m2024-04-26T01:04:49 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.self.key.bias from model.bert.encoder.layer.5.attention.self.key.bias\n","\u001b[32m2024-04-26T01:04:49 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.self.value.weight from model.bert.encoder.layer.5.attention.self.value.weight\n","\u001b[32m2024-04-26T01:04:49 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.self.value.bias from model.bert.encoder.layer.5.attention.self.value.bias\n","\u001b[32m2024-04-26T01:04:49 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.output.dense.weight from model.bert.encoder.layer.5.attention.output.dense.weight\n","\u001b[32m2024-04-26T01:04:49 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.output.dense.bias from model.bert.encoder.layer.5.attention.output.dense.bias\n","\u001b[32m2024-04-26T01:04:49 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.output.LayerNorm.weight from model.bert.encoder.layer.5.attention.output.LayerNorm.weight\n","\u001b[32m2024-04-26T01:04:49 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.output.LayerNorm.bias from model.bert.encoder.layer.5.attention.output.LayerNorm.bias\n","\u001b[32m2024-04-26T01:04:49 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.intermediate.dense.weight from model.bert.encoder.layer.5.intermediate.dense.weight\n","\u001b[32m2024-04-26T01:04:49 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.intermediate.dense.bias from model.bert.encoder.layer.5.intermediate.dense.bias\n","\u001b[32m2024-04-26T01:04:49 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.output.dense.weight from model.bert.encoder.layer.5.output.dense.weight\n","\u001b[32m2024-04-26T01:04:49 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.output.dense.bias from model.bert.encoder.layer.5.output.dense.bias\n","\u001b[32m2024-04-26T01:04:49 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.output.LayerNorm.weight from model.bert.encoder.layer.5.output.LayerNorm.weight\n","\u001b[32m2024-04-26T01:04:49 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.output.LayerNorm.bias from model.bert.encoder.layer.5.output.LayerNorm.bias\n","\u001b[32m2024-04-26T01:04:49 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.self.query.weight from model.bert.encoder.layer.6.attention.self.query.weight\n","\u001b[32m2024-04-26T01:04:49 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.self.query.bias from model.bert.encoder.layer.6.attention.self.query.bias\n","\u001b[32m2024-04-26T01:04:49 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.self.key.weight from model.bert.encoder.layer.6.attention.self.key.weight\n","\u001b[32m2024-04-26T01:04:49 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.self.key.bias from model.bert.encoder.layer.6.attention.self.key.bias\n","\u001b[32m2024-04-26T01:04:49 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.self.value.weight from model.bert.encoder.layer.6.attention.self.value.weight\n","\u001b[32m2024-04-26T01:04:49 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.self.value.bias from model.bert.encoder.layer.6.attention.self.value.bias\n","\u001b[32m2024-04-26T01:04:49 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.output.dense.weight from model.bert.encoder.layer.6.attention.output.dense.weight\n","\u001b[32m2024-04-26T01:04:49 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.output.dense.bias from model.bert.encoder.layer.6.attention.output.dense.bias\n","\u001b[32m2024-04-26T01:04:49 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.output.LayerNorm.weight from model.bert.encoder.layer.6.attention.output.LayerNorm.weight\n","\u001b[32m2024-04-26T01:04:49 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.output.LayerNorm.bias from model.bert.encoder.layer.6.attention.output.LayerNorm.bias\n","\u001b[32m2024-04-26T01:04:49 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.intermediate.dense.weight from model.bert.encoder.layer.6.intermediate.dense.weight\n","\u001b[32m2024-04-26T01:04:49 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.intermediate.dense.bias from model.bert.encoder.layer.6.intermediate.dense.bias\n","\u001b[32m2024-04-26T01:04:49 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.output.dense.weight from model.bert.encoder.layer.6.output.dense.weight\n","\u001b[32m2024-04-26T01:04:49 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.output.dense.bias from model.bert.encoder.layer.6.output.dense.bias\n","\u001b[32m2024-04-26T01:04:49 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.output.LayerNorm.weight from model.bert.encoder.layer.6.output.LayerNorm.weight\n","\u001b[32m2024-04-26T01:04:49 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.output.LayerNorm.bias from model.bert.encoder.layer.6.output.LayerNorm.bias\n","\u001b[32m2024-04-26T01:04:49 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.self.query.weight from model.bert.encoder.layer.7.attention.self.query.weight\n","\u001b[32m2024-04-26T01:04:49 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.self.query.bias from model.bert.encoder.layer.7.attention.self.query.bias\n","\u001b[32m2024-04-26T01:04:49 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.self.key.weight from model.bert.encoder.layer.7.attention.self.key.weight\n","\u001b[32m2024-04-26T01:04:49 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.self.key.bias from model.bert.encoder.layer.7.attention.self.key.bias\n","\u001b[32m2024-04-26T01:04:49 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.self.value.weight from model.bert.encoder.layer.7.attention.self.value.weight\n","\u001b[32m2024-04-26T01:04:49 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.self.value.bias from model.bert.encoder.layer.7.attention.self.value.bias\n","\u001b[32m2024-04-26T01:04:49 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.output.dense.weight from model.bert.encoder.layer.7.attention.output.dense.weight\n","\u001b[32m2024-04-26T01:04:49 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.output.dense.bias from model.bert.encoder.layer.7.attention.output.dense.bias\n","\u001b[32m2024-04-26T01:04:49 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.output.LayerNorm.weight from model.bert.encoder.layer.7.attention.output.LayerNorm.weight\n","\u001b[32m2024-04-26T01:04:49 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.output.LayerNorm.bias from model.bert.encoder.layer.7.attention.output.LayerNorm.bias\n","\u001b[32m2024-04-26T01:04:49 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.intermediate.dense.weight from model.bert.encoder.layer.7.intermediate.dense.weight\n","\u001b[32m2024-04-26T01:04:49 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.intermediate.dense.bias from model.bert.encoder.layer.7.intermediate.dense.bias\n","\u001b[32m2024-04-26T01:04:49 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.output.dense.weight from model.bert.encoder.layer.7.output.dense.weight\n","\u001b[32m2024-04-26T01:04:49 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.output.dense.bias from model.bert.encoder.layer.7.output.dense.bias\n","\u001b[32m2024-04-26T01:04:49 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.output.LayerNorm.weight from model.bert.encoder.layer.7.output.LayerNorm.weight\n","\u001b[32m2024-04-26T01:04:49 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.output.LayerNorm.bias from model.bert.encoder.layer.7.output.LayerNorm.bias\n","\u001b[32m2024-04-26T01:04:49 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.self.query.weight from model.bert.encoder.layer.8.attention.self.query.weight\n","\u001b[32m2024-04-26T01:04:49 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.self.query.bias from model.bert.encoder.layer.8.attention.self.query.bias\n","\u001b[32m2024-04-26T01:04:49 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.self.key.weight from model.bert.encoder.layer.8.attention.self.key.weight\n","\u001b[32m2024-04-26T01:04:49 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.self.key.bias from model.bert.encoder.layer.8.attention.self.key.bias\n","\u001b[32m2024-04-26T01:04:49 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.self.value.weight from model.bert.encoder.layer.8.attention.self.value.weight\n","\u001b[32m2024-04-26T01:04:49 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.self.value.bias from model.bert.encoder.layer.8.attention.self.value.bias\n","\u001b[32m2024-04-26T01:04:49 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.output.dense.weight from model.bert.encoder.layer.8.attention.output.dense.weight\n","\u001b[32m2024-04-26T01:04:49 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.output.dense.bias from model.bert.encoder.layer.8.attention.output.dense.bias\n","\u001b[32m2024-04-26T01:04:49 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.output.LayerNorm.weight from model.bert.encoder.layer.8.attention.output.LayerNorm.weight\n","\u001b[32m2024-04-26T01:04:49 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.output.LayerNorm.bias from model.bert.encoder.layer.8.attention.output.LayerNorm.bias\n","\u001b[32m2024-04-26T01:04:49 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.intermediate.dense.weight from model.bert.encoder.layer.8.intermediate.dense.weight\n","\u001b[32m2024-04-26T01:04:49 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.intermediate.dense.bias from model.bert.encoder.layer.8.intermediate.dense.bias\n","\u001b[32m2024-04-26T01:04:49 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.output.dense.weight from model.bert.encoder.layer.8.output.dense.weight\n","\u001b[32m2024-04-26T01:04:49 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.output.dense.bias from model.bert.encoder.layer.8.output.dense.bias\n","\u001b[32m2024-04-26T01:04:49 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.output.LayerNorm.weight from model.bert.encoder.layer.8.output.LayerNorm.weight\n","\u001b[32m2024-04-26T01:04:49 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.output.LayerNorm.bias from model.bert.encoder.layer.8.output.LayerNorm.bias\n","\u001b[32m2024-04-26T01:04:49 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.self.query.weight from model.bert.encoder.layer.9.attention.self.query.weight\n","\u001b[32m2024-04-26T01:04:49 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.self.query.bias from model.bert.encoder.layer.9.attention.self.query.bias\n","\u001b[32m2024-04-26T01:04:49 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.self.key.weight from model.bert.encoder.layer.9.attention.self.key.weight\n","\u001b[32m2024-04-26T01:04:49 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.self.key.bias from model.bert.encoder.layer.9.attention.self.key.bias\n","\u001b[32m2024-04-26T01:04:49 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.self.value.weight from model.bert.encoder.layer.9.attention.self.value.weight\n","\u001b[32m2024-04-26T01:04:49 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.self.value.bias from model.bert.encoder.layer.9.attention.self.value.bias\n","\u001b[32m2024-04-26T01:04:49 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.output.dense.weight from model.bert.encoder.layer.9.attention.output.dense.weight\n","\u001b[32m2024-04-26T01:04:49 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.output.dense.bias from model.bert.encoder.layer.9.attention.output.dense.bias\n","\u001b[32m2024-04-26T01:04:49 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.output.LayerNorm.weight from model.bert.encoder.layer.9.attention.output.LayerNorm.weight\n","\u001b[32m2024-04-26T01:04:49 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.output.LayerNorm.bias from model.bert.encoder.layer.9.attention.output.LayerNorm.bias\n","\u001b[32m2024-04-26T01:04:49 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.intermediate.dense.weight from model.bert.encoder.layer.9.intermediate.dense.weight\n","\u001b[32m2024-04-26T01:04:49 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.intermediate.dense.bias from model.bert.encoder.layer.9.intermediate.dense.bias\n","\u001b[32m2024-04-26T01:04:49 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.output.dense.weight from model.bert.encoder.layer.9.output.dense.weight\n","\u001b[32m2024-04-26T01:04:49 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.output.dense.bias from model.bert.encoder.layer.9.output.dense.bias\n","\u001b[32m2024-04-26T01:04:49 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.output.LayerNorm.weight from model.bert.encoder.layer.9.output.LayerNorm.weight\n","\u001b[32m2024-04-26T01:04:49 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.output.LayerNorm.bias from model.bert.encoder.layer.9.output.LayerNorm.bias\n","\u001b[32m2024-04-26T01:04:49 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.self.query.weight from model.bert.encoder.layer.10.attention.self.query.weight\n","\u001b[32m2024-04-26T01:04:49 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.self.query.bias from model.bert.encoder.layer.10.attention.self.query.bias\n","\u001b[32m2024-04-26T01:04:49 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.self.key.weight from model.bert.encoder.layer.10.attention.self.key.weight\n","\u001b[32m2024-04-26T01:04:49 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.self.key.bias from model.bert.encoder.layer.10.attention.self.key.bias\n","\u001b[32m2024-04-26T01:04:49 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.self.value.weight from model.bert.encoder.layer.10.attention.self.value.weight\n","\u001b[32m2024-04-26T01:04:49 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.self.value.bias from model.bert.encoder.layer.10.attention.self.value.bias\n","\u001b[32m2024-04-26T01:04:49 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.output.dense.weight from model.bert.encoder.layer.10.attention.output.dense.weight\n","\u001b[32m2024-04-26T01:04:49 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.output.dense.bias from model.bert.encoder.layer.10.attention.output.dense.bias\n","\u001b[32m2024-04-26T01:04:49 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.output.LayerNorm.weight from model.bert.encoder.layer.10.attention.output.LayerNorm.weight\n","\u001b[32m2024-04-26T01:04:49 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.output.LayerNorm.bias from model.bert.encoder.layer.10.attention.output.LayerNorm.bias\n","\u001b[32m2024-04-26T01:04:49 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.intermediate.dense.weight from model.bert.encoder.layer.10.intermediate.dense.weight\n","\u001b[32m2024-04-26T01:04:49 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.intermediate.dense.bias from model.bert.encoder.layer.10.intermediate.dense.bias\n","\u001b[32m2024-04-26T01:04:49 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.output.dense.weight from model.bert.encoder.layer.10.output.dense.weight\n","\u001b[32m2024-04-26T01:04:49 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.output.dense.bias from model.bert.encoder.layer.10.output.dense.bias\n","\u001b[32m2024-04-26T01:04:49 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.output.LayerNorm.weight from model.bert.encoder.layer.10.output.LayerNorm.weight\n","\u001b[32m2024-04-26T01:04:49 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.output.LayerNorm.bias from model.bert.encoder.layer.10.output.LayerNorm.bias\n","\u001b[32m2024-04-26T01:04:49 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.self.query.weight from model.bert.encoder.layer.11.attention.self.query.weight\n","\u001b[32m2024-04-26T01:04:49 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.self.query.bias from model.bert.encoder.layer.11.attention.self.query.bias\n","\u001b[32m2024-04-26T01:04:49 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.self.key.weight from model.bert.encoder.layer.11.attention.self.key.weight\n","\u001b[32m2024-04-26T01:04:49 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.self.key.bias from model.bert.encoder.layer.11.attention.self.key.bias\n","\u001b[32m2024-04-26T01:04:49 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.self.value.weight from model.bert.encoder.layer.11.attention.self.value.weight\n","\u001b[32m2024-04-26T01:04:49 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.self.value.bias from model.bert.encoder.layer.11.attention.self.value.bias\n","\u001b[32m2024-04-26T01:04:49 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.output.dense.weight from model.bert.encoder.layer.11.attention.output.dense.weight\n","\u001b[32m2024-04-26T01:04:49 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.output.dense.bias from model.bert.encoder.layer.11.attention.output.dense.bias\n","\u001b[32m2024-04-26T01:04:49 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.output.LayerNorm.weight from model.bert.encoder.layer.11.attention.output.LayerNorm.weight\n","\u001b[32m2024-04-26T01:04:49 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.output.LayerNorm.bias from model.bert.encoder.layer.11.attention.output.LayerNorm.bias\n","\u001b[32m2024-04-26T01:04:49 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.intermediate.dense.weight from model.bert.encoder.layer.11.intermediate.dense.weight\n","\u001b[32m2024-04-26T01:04:49 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.intermediate.dense.bias from model.bert.encoder.layer.11.intermediate.dense.bias\n","\u001b[32m2024-04-26T01:04:49 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.output.dense.weight from model.bert.encoder.layer.11.output.dense.weight\n","\u001b[32m2024-04-26T01:04:49 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.output.dense.bias from model.bert.encoder.layer.11.output.dense.bias\n","\u001b[32m2024-04-26T01:04:49 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.output.LayerNorm.weight from model.bert.encoder.layer.11.output.LayerNorm.weight\n","\u001b[32m2024-04-26T01:04:49 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.output.LayerNorm.bias from model.bert.encoder.layer.11.output.LayerNorm.bias\n","\u001b[32m2024-04-26T01:04:49 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.pooler.dense.weight from model.bert.pooler.dense.weight\n","\u001b[32m2024-04-26T01:04:49 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.pooler.dense.bias from model.bert.pooler.dense.bias\n","\u001b[32m2024-04-26T01:04:49 | mmf.utils.checkpoint: \u001b[0mPretrained model loaded\n","\u001b[32m2024-04-26T01:04:49 | mmf.utils.checkpoint: \u001b[0mCheckpoint loaded.\n","\u001b[32m2024-04-26T01:04:49 | mmf.utils.checkpoint: \u001b[0mCurrent num updates: 0\n","\u001b[32m2024-04-26T01:04:49 | mmf.utils.checkpoint: \u001b[0mCurrent iteration: 0\n","\u001b[32m2024-04-26T01:04:49 | mmf.utils.checkpoint: \u001b[0mCurrent epoch: 0\n","\u001b[32m2024-04-26T01:04:49 | mmf.trainers.mmf_trainer: \u001b[0m===== Model =====\n","\u001b[32m2024-04-26T01:04:49 | mmf.trainers.mmf_trainer: \u001b[0mVisualBERT(\n","  (model): VisualBERTForClassification(\n","    (bert): VisualBERTBase(\n","      (embeddings): BertVisioLinguisticEmbeddings(\n","        (word_embeddings): Embedding(30522, 768, padding_idx=0)\n","        (position_embeddings): Embedding(512, 768)\n","        (token_type_embeddings): Embedding(2, 768)\n","        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","        (token_type_embeddings_visual): Embedding(2, 768)\n","        (position_embeddings_visual): Embedding(512, 768)\n","        (projection): Linear(in_features=2048, out_features=768, bias=True)\n","      )\n","      (encoder): BertEncoderJit(\n","        (layer): ModuleList(\n","          (0-11): 12 x BertLayerJit(\n","            (attention): BertAttentionJit(\n","              (self): BertSelfAttentionJit(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","              (intermediate_act_fn): GELUActivation()\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","        )\n","      )\n","      (pooler): BertPooler(\n","        (dense): Linear(in_features=768, out_features=768, bias=True)\n","        (activation): Tanh()\n","      )\n","    )\n","    (dropout): Dropout(p=0.1, inplace=False)\n","    (classifier): Sequential(\n","      (0): BertPredictionHeadTransform(\n","        (dense): Linear(in_features=768, out_features=768, bias=True)\n","        (transform_act_fn): GELUActivation()\n","        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      )\n","      (1): Linear(in_features=768, out_features=2, bias=True)\n","    )\n","  )\n","  (losses): Losses(\n","    (losses): ModuleList(\n","      (0): MMFLoss(\n","        (loss_criterion): CrossEntropyLoss(\n","          (loss_fn): CrossEntropyLoss()\n","        )\n","      )\n","    )\n","  )\n",")\n","\u001b[32m2024-04-26T01:04:49 | mmf.utils.general: \u001b[0mTotal Parameters: 112044290. Trained Parameters: 112044290\n","\u001b[32m2024-04-26T01:04:49 | mmf.trainers.core.training_loop: \u001b[0mStarting training...\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2024-04-26T01:04:49 | py.warnings: \u001b[0m/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n","  self.pid = os.fork()\n","\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2024-04-26T01:04:49 | py.warnings: \u001b[0m/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n","  self.pid = os.fork()\n","\n","\u001b[32m2024-04-26T01:06:56 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n","\u001b[32m2024-04-26T01:06:56 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2024-04-26T01:07:00 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2024-04-26T01:07:06 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2024-04-26T01:07:07 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 200/1500, train/hateful_memes/cross_entropy: 0.6291, train/hateful_memes/cross_entropy/avg: 0.6291, train/total_loss: 0.6291, train/total_loss/avg: 0.6291, max mem: 14364.0, experiment: run, epoch: 1, num_updates: 200, iterations: 200, max_updates: 1500, lr: 0.00001, ups: 1.46, time: 02m 17s 096ms, time_since_start: 02m 17s 751ms, eta: 15m 31s 225ms\n","\u001b[32m2024-04-26T01:07:07 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n","\u001b[32m2024-04-26T01:07:07 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n","\u001b[32m2024-04-26T01:07:09 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished evaluation inference. Loaded 9\n","\u001b[32m2024-04-26T01:07:09 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n","\u001b[32m2024-04-26T01:07:09 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2024-04-26T01:07:13 | mmf.utils.checkpoint: \u001b[0mSaving best checkpoint\n","\u001b[32m2024-04-26T01:07:17 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2024-04-26T01:07:22 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2024-04-26T01:07:22 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 200/1500, val/hateful_memes/cross_entropy: 0.6594, val/total_loss: 0.6594, val/hateful_memes/accuracy: 0.6296, val/hateful_memes/binary_f1: 0.2857, val/hateful_memes/roc_auc: 0.5853, num_updates: 200, epoch: 1, iterations: 200, max_updates: 1500, val_time: 15s 144ms, best_update: 200, best_iteration: 200, best_val/hateful_memes/roc_auc: 0.585324\n","\u001b[32m2024-04-26T01:09:26 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n","\u001b[32m2024-04-26T01:09:26 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2024-04-26T01:09:29 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2024-04-26T01:09:33 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2024-04-26T01:09:33 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 400/1500, train/hateful_memes/cross_entropy: 0.5040, train/hateful_memes/cross_entropy/avg: 0.5665, train/total_loss: 0.5040, train/total_loss/avg: 0.5665, max mem: 14413.0, experiment: run, epoch: 2, num_updates: 400, iterations: 400, max_updates: 1500, lr: 0.00001, ups: 1.54, time: 02m 10s 720ms, time_since_start: 04m 44s 361ms, eta: 12m 31s 318ms\n","\u001b[32m2024-04-26T01:09:33 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n","\u001b[32m2024-04-26T01:09:33 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2024-04-26T01:09:33 | py.warnings: \u001b[0m/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n","  self.pid = os.fork()\n","\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2024-04-26T01:09:33 | py.warnings: \u001b[0m/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n","  self.pid = os.fork()\n","\n","\u001b[32m2024-04-26T01:09:35 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished evaluation inference. Loaded 9\n","\u001b[32m2024-04-26T01:09:35 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n","\u001b[32m2024-04-26T01:09:36 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2024-04-26T01:09:40 | mmf.utils.checkpoint: \u001b[0mSaving best checkpoint\n","\u001b[32m2024-04-26T01:09:43 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2024-04-26T01:09:49 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2024-04-26T01:09:49 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 400/1500, val/hateful_memes/cross_entropy: 0.7160, val/total_loss: 0.7160, val/hateful_memes/accuracy: 0.6537, val/hateful_memes/binary_f1: 0.4138, val/hateful_memes/roc_auc: 0.6617, num_updates: 400, epoch: 2, iterations: 400, max_updates: 1500, val_time: 16s 262ms, best_update: 400, best_iteration: 400, best_val/hateful_memes/roc_auc: 0.661691\n","\u001b[32m2024-04-26T01:11:53 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n","\u001b[32m2024-04-26T01:11:53 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2024-04-26T01:11:57 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2024-04-26T01:12:02 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2024-04-26T01:12:02 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 600/1500, train/hateful_memes/cross_entropy: 0.5040, train/hateful_memes/cross_entropy/avg: 0.5332, train/total_loss: 0.5040, train/total_loss/avg: 0.5332, max mem: 14413.0, experiment: run, epoch: 3, num_updates: 600, iterations: 600, max_updates: 1500, lr: 0.00002, ups: 1.50, time: 02m 13s 140ms, time_since_start: 07m 13s 766ms, eta: 10m 26s 093ms\n","\u001b[32m2024-04-26T01:12:02 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n","\u001b[32m2024-04-26T01:12:02 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2024-04-26T01:12:02 | py.warnings: \u001b[0m/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n","  self.pid = os.fork()\n","\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2024-04-26T01:12:02 | py.warnings: \u001b[0m/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n","  self.pid = os.fork()\n","\n","\u001b[32m2024-04-26T01:12:05 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished evaluation inference. Loaded 9\n","\u001b[32m2024-04-26T01:12:05 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n","\u001b[32m2024-04-26T01:12:05 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2024-04-26T01:12:08 | mmf.utils.checkpoint: \u001b[0mSaving best checkpoint\n","\u001b[32m2024-04-26T01:12:12 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2024-04-26T01:12:17 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2024-04-26T01:12:17 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 600/1500, val/hateful_memes/cross_entropy: 0.7396, val/total_loss: 0.7396, val/hateful_memes/accuracy: 0.6926, val/hateful_memes/binary_f1: 0.4812, val/hateful_memes/roc_auc: 0.7148, num_updates: 600, epoch: 3, iterations: 600, max_updates: 1500, val_time: 14s 503ms, best_update: 600, best_iteration: 600, best_val/hateful_memes/roc_auc: 0.714838\n","\u001b[32m2024-04-26T01:14:20 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n","\u001b[32m2024-04-26T01:14:20 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2024-04-26T01:14:24 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2024-04-26T01:14:30 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2024-04-26T01:14:30 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 800/1500, train/hateful_memes/cross_entropy: 0.4667, train/hateful_memes/cross_entropy/avg: 0.4490, train/total_loss: 0.4667, train/total_loss/avg: 0.4490, max mem: 14413.0, experiment: run, epoch: 3, num_updates: 800, iterations: 800, max_updates: 1500, lr: 0.00002, ups: 1.50, time: 02m 13s 272ms, time_since_start: 09m 41s 544ms, eta: 08m 07s 444ms\n","\u001b[32m2024-04-26T01:14:30 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n","\u001b[32m2024-04-26T01:14:30 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2024-04-26T01:14:30 | py.warnings: \u001b[0m/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n","  self.pid = os.fork()\n","\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2024-04-26T01:14:30 | py.warnings: \u001b[0m/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n","  self.pid = os.fork()\n","\n","\u001b[32m2024-04-26T01:14:32 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished evaluation inference. Loaded 9\n","\u001b[32m2024-04-26T01:14:32 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n","\u001b[32m2024-04-26T01:14:33 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2024-04-26T01:14:36 | mmf.utils.checkpoint: \u001b[0mSaving best checkpoint\n","\u001b[32m2024-04-26T01:14:40 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2024-04-26T01:14:44 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2024-04-26T01:14:44 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 800/1500, val/hateful_memes/cross_entropy: 0.8994, val/total_loss: 0.8994, val/hateful_memes/accuracy: 0.7000, val/hateful_memes/binary_f1: 0.4671, val/hateful_memes/roc_auc: 0.7219, num_updates: 800, epoch: 3, iterations: 800, max_updates: 1500, val_time: 14s 357ms, best_update: 800, best_iteration: 800, best_val/hateful_memes/roc_auc: 0.721882\n","\u001b[32m2024-04-26T01:16:48 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n","\u001b[32m2024-04-26T01:16:48 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2024-04-26T01:16:51 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2024-04-26T01:16:57 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2024-04-26T01:16:57 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1000/1500, train/hateful_memes/cross_entropy: 0.4667, train/hateful_memes/cross_entropy/avg: 0.3726, train/total_loss: 0.4667, train/total_loss/avg: 0.3726, max mem: 14413.0, experiment: run, epoch: 4, num_updates: 1000, iterations: 1000, max_updates: 1500, lr: 0.00003, ups: 1.52, time: 02m 12s 468ms, time_since_start: 12m 08s 372ms, eta: 05m 46s 074ms\n","\u001b[32m2024-04-26T01:16:57 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n","\u001b[32m2024-04-26T01:16:57 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2024-04-26T01:16:57 | py.warnings: \u001b[0m/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n","  self.pid = os.fork()\n","\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2024-04-26T01:16:57 | py.warnings: \u001b[0m/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n","  self.pid = os.fork()\n","\n","\u001b[32m2024-04-26T01:16:59 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished evaluation inference. Loaded 9\n","\u001b[32m2024-04-26T01:16:59 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n","\u001b[32m2024-04-26T01:16:59 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2024-04-26T01:17:03 | mmf.utils.checkpoint: \u001b[0mSaving best checkpoint\n","\u001b[32m2024-04-26T01:17:07 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2024-04-26T01:17:12 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2024-04-26T01:17:12 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1000/1500, val/hateful_memes/cross_entropy: 1.1003, val/total_loss: 1.1003, val/hateful_memes/accuracy: 0.6926, val/hateful_memes/binary_f1: 0.4679, val/hateful_memes/roc_auc: 0.7304, num_updates: 1000, epoch: 4, iterations: 1000, max_updates: 1500, val_time: 15s 200ms, best_update: 1000, best_iteration: 1000, best_val/hateful_memes/roc_auc: 0.730368\n","\u001b[32m2024-04-26T01:19:15 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n","\u001b[32m2024-04-26T01:19:15 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2024-04-26T01:19:19 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2024-04-26T01:19:23 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2024-04-26T01:19:23 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1200/1500, train/hateful_memes/cross_entropy: 0.1964, train/hateful_memes/cross_entropy/avg: 0.3185, train/total_loss: 0.1964, train/total_loss/avg: 0.3185, max mem: 14414.0, experiment: run, epoch: 5, num_updates: 1200, iterations: 1200, max_updates: 1500, lr: 0.00003, ups: 1.53, time: 02m 11s 026ms, time_since_start: 14m 34s 602ms, eta: 03m 25s 384ms\n","\u001b[32m2024-04-26T01:19:23 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n","\u001b[32m2024-04-26T01:19:23 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2024-04-26T01:19:23 | py.warnings: \u001b[0m/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n","  self.pid = os.fork()\n","\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2024-04-26T01:19:23 | py.warnings: \u001b[0m/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n","  self.pid = os.fork()\n","\n","\u001b[32m2024-04-26T01:19:25 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished evaluation inference. Loaded 9\n","\u001b[32m2024-04-26T01:19:25 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n","\u001b[32m2024-04-26T01:19:25 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2024-04-26T01:19:29 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2024-04-26T01:19:33 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2024-04-26T01:19:33 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1200/1500, val/hateful_memes/cross_entropy: 1.3964, val/total_loss: 1.3964, val/hateful_memes/accuracy: 0.6778, val/hateful_memes/binary_f1: 0.4630, val/hateful_memes/roc_auc: 0.7143, num_updates: 1200, epoch: 5, iterations: 1200, max_updates: 1500, val_time: 10s 367ms, best_update: 1000, best_iteration: 1000, best_val/hateful_memes/roc_auc: 0.730368\n","\u001b[32m2024-04-26T01:21:37 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n","\u001b[32m2024-04-26T01:21:37 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2024-04-26T01:21:41 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2024-04-26T01:21:45 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2024-04-26T01:21:45 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1400/1500, train/hateful_memes/cross_entropy: 0.1964, train/hateful_memes/cross_entropy/avg: 0.2791, train/total_loss: 0.1964, train/total_loss/avg: 0.2791, max mem: 14414.0, experiment: run, epoch: 6, num_updates: 1400, iterations: 1400, max_updates: 1500, lr: 0.00003, ups: 1.53, time: 02m 11s 230ms, time_since_start: 16m 56s 202ms, eta: 01m 08s 568ms\n","\u001b[32m2024-04-26T01:21:45 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n","\u001b[32m2024-04-26T01:21:45 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2024-04-26T01:21:45 | py.warnings: \u001b[0m/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n","  self.pid = os.fork()\n","\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2024-04-26T01:21:45 | py.warnings: \u001b[0m/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n","  self.pid = os.fork()\n","\n","\u001b[32m2024-04-26T01:21:47 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished evaluation inference. Loaded 9\n","\u001b[32m2024-04-26T01:21:47 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n","\u001b[32m2024-04-26T01:21:47 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2024-04-26T01:21:51 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2024-04-26T01:21:55 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2024-04-26T01:21:55 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1400/1500, val/hateful_memes/cross_entropy: 1.2703, val/total_loss: 1.2703, val/hateful_memes/accuracy: 0.6815, val/hateful_memes/binary_f1: 0.5401, val/hateful_memes/roc_auc: 0.7199, num_updates: 1400, epoch: 6, iterations: 1400, max_updates: 1500, val_time: 10s 017ms, best_update: 1000, best_iteration: 1000, best_val/hateful_memes/roc_auc: 0.730368\n","\u001b[32m2024-04-26T01:22:57 | mmf.trainers.core.training_loop: \u001b[0mStepping into final validation check\n","\u001b[32m2024-04-26T01:22:57 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2024-04-26T01:22:57 | py.warnings: \u001b[0m/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n","  self.pid = os.fork()\n","\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2024-04-26T01:22:57 | py.warnings: \u001b[0m/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n","  self.pid = os.fork()\n","\n","\u001b[32m2024-04-26T01:22:59 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished evaluation inference. Loaded 9\n","\u001b[32m2024-04-26T01:22:59 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n","\u001b[32m2024-04-26T01:22:59 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1500/1500, val/hateful_memes/cross_entropy: 1.6254, val/total_loss: 1.6254, val/hateful_memes/accuracy: 0.6963, val/hateful_memes/binary_f1: 0.4345, val/hateful_memes/roc_auc: 0.7310, num_updates: 1500, epoch: 6, iterations: 1500, max_updates: 1500, val_time: 01m 14s 498ms, best_update: 1000, best_iteration: 1000, best_val/hateful_memes/roc_auc: 0.730368\n","\u001b[32m2024-04-26T01:22:59 | mmf.utils.checkpoint: \u001b[0mRestoring checkpoint\n","\u001b[32m2024-04-26T01:22:59 | mmf.utils.checkpoint: \u001b[0mLoading checkpoint\n","\u001b[32m2024-04-26T01:23:01 | mmf.utils.checkpoint: \u001b[0mCheckpoint loaded.\n","\u001b[32m2024-04-26T01:23:01 | mmf.utils.checkpoint: \u001b[0mCurrent num updates: 1000\n","\u001b[32m2024-04-26T01:23:01 | mmf.utils.checkpoint: \u001b[0mCurrent iteration: 1000\n","\u001b[32m2024-04-26T01:23:01 | mmf.utils.checkpoint: \u001b[0mCurrent epoch: 4\n","\u001b[32m2024-04-26T01:23:02 | mmf.trainers.mmf_trainer: \u001b[0mStarting inference on val set\n","\u001b[32m2024-04-26T01:23:02 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n","  0% 0/9 [00:00<?, ?it/s]\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2024-04-26T01:23:02 | py.warnings: \u001b[0m/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n","  self.pid = os.fork()\n","\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2024-04-26T01:23:02 | py.warnings: \u001b[0m/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n","  self.pid = os.fork()\n","\n","100% 9/9 [00:02<00:00,  3.65it/s]\n","\u001b[32m2024-04-26T01:23:04 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished evaluation inference. Loaded 9\n","\u001b[32m2024-04-26T01:23:04 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n","\u001b[32m2024-04-26T01:23:04 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1000/1500, val/hateful_memes/cross_entropy: 1.1003, val/total_loss: 1.1003, val/hateful_memes/accuracy: 0.6926, val/hateful_memes/binary_f1: 0.4679, val/hateful_memes/roc_auc: 0.7304\n","\u001b[32m2024-04-26T01:23:04 | mmf.trainers.callbacks.logistics: \u001b[0mFinished run in 18m 16s 049ms\n"]}]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"A100","machine_shape":"hm","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}