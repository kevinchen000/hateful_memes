{"cells":[{"cell_type":"markdown","source":["# Prerequisites\n","\n","The following steps must be satisfied before running the rest of the notebook:\n","\n","- Run the `Augment_Data` notebook in order to generate the training files and images that are referred to throughout the notebook.\n","- Make changes to `hm_convert` to read the added files/folders when running `build_dataset`.\n","\n","Throughout the notebook, directory paths are used for references that depend on your local setup. There are comments that describe what each path should be pointing to so that should help set up the notebook to work for your hierarchy."],"metadata":{"id":"D_efaOP0V_8K"}},{"cell_type":"markdown","source":["# Setup"],"metadata":{"id":"Ccw-s21WV8kW"}},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":23209,"status":"ok","timestamp":1713974711275,"user":{"displayName":"Aneri Shah","userId":"14009776940163552827"},"user_tz":420},"id":"oEAyXvI-d4UN","outputId":"fe53aa6d-9a1a-4879-e8f3-4e152c123730"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["### Mount\n","from google.colab import drive\n","drive.mount('/content/drive', force_remount=True)"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"29fJJ1UEhi9D","outputId":"cf7875c4-f380-495f-8ab0-60d81ad732fe","executionInfo":{"status":"ok","timestamp":1713974967182,"user_tz":420,"elapsed":255910,"user":{"displayName":"Aneri Shah","userId":"14009776940163552827"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/hateful-memes/mmf-main\n","Obtaining file:///content/drive/MyDrive/hateful-memes/mmf-main\n","  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Checking if build backend supports build_editable ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build editable ... \u001b[?25l\u001b[?25hdone\n","  Preparing editable metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","Collecting datasets<=2.18.0 (from mmf==1.0.0rc12)\n","  Downloading datasets-2.18.0-py3-none-any.whl (510 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m510.5/510.5 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting iopath==0.1.8 (from mmf==1.0.0rc12)\n","  Downloading iopath-0.1.8-py3-none-any.whl (19 kB)\n","Collecting editdistance==0.5.3 (from mmf==1.0.0rc12)\n","  Downloading editdistance-0.5.3.tar.gz (27 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting fasttext==0.9.1 (from mmf==1.0.0rc12)\n","  Downloading fasttext-0.9.1.tar.gz (57 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.7/57.7 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting ftfy==5.8 (from mmf==1.0.0rc12)\n","  Downloading ftfy-5.8.tar.gz (64 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.1/64.1 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting GitPython==3.1.30 (from mmf==1.0.0rc12)\n","  Downloading GitPython-3.1.30-py3-none-any.whl (184 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m184.0/184.0 kB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting lmdb==0.98 (from mmf==1.0.0rc12)\n","  Downloading lmdb-0.98.tar.gz (869 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m869.9/869.9 kB\u001b[0m \u001b[31m16.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting omegaconf<=2.3,>=2.0.6 (from mmf==1.0.0rc12)\n","  Downloading omegaconf-2.3.0-py3-none-any.whl (79 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.5/79.5 kB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting matplotlib==3.8.0 (from mmf==1.0.0rc12)\n","  Downloading matplotlib-3.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.6 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.6/11.6 MB\u001b[0m \u001b[31m43.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting nltk==3.6.6 (from mmf==1.0.0rc12)\n","  Downloading nltk-3.6.6-py3-none-any.whl (1.5 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m62.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting numpy==1.26.4 (from mmf==1.0.0rc12)\n","  Downloading numpy-1.26.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.2 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.2/18.2 MB\u001b[0m \u001b[31m74.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting pillow==9.3.0 (from mmf==1.0.0rc12)\n","  Downloading Pillow-9.3.0-cp310-cp310-manylinux_2_28_x86_64.whl (3.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m100.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from mmf==1.0.0rc12) (5.9.5)\n","Requirement already satisfied: pycocotools<=2.0.7 in /usr/local/lib/python3.10/dist-packages (from mmf==1.0.0rc12) (2.0.7)\n","Collecting pytorch-lightning<=1.6.0 (from mmf==1.0.0rc12)\n","  Downloading pytorch_lightning-1.6.0-py3-none-any.whl (582 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m582.1/582.1 kB\u001b[0m \u001b[31m57.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting requests==2.23.0 (from mmf==1.0.0rc12)\n","  Downloading requests-2.23.0-py2.py3-none-any.whl (58 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.4/58.4 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (from mmf==1.0.0rc12) (0.1.99)\n","Collecting sklearn==0.0 (from mmf==1.0.0rc12)\n","  Downloading sklearn-0.0.tar.gz (1.1 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting termcolor==1.1.0 (from mmf==1.0.0rc12)\n","  Downloading termcolor-1.1.0.tar.gz (3.9 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting torchaudio<=2.2.0 (from mmf==1.0.0rc12)\n","  Downloading torchaudio-2.2.0-cp310-cp310-manylinux1_x86_64.whl (3.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m103.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting torchtext<=0.17.0 (from mmf==1.0.0rc12)\n","  Downloading torchtext-0.17.0-cp310-cp310-manylinux1_x86_64.whl (2.0 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m89.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting torchvision<=0.17.0 (from mmf==1.0.0rc12)\n","  Downloading torchvision-0.17.0-cp310-cp310-manylinux1_x86_64.whl (6.9 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m101.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: tqdm>=4.43.0 in /usr/local/lib/python3.10/dist-packages (from mmf==1.0.0rc12) (4.66.2)\n","Collecting transformers<=4.39.2 (from mmf==1.0.0rc12)\n","  Downloading transformers-4.39.2-py3-none-any.whl (8.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.8/8.8 MB\u001b[0m \u001b[31m99.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting pybind11>=2.2 (from fasttext==0.9.1->mmf==1.0.0rc12)\n","  Using cached pybind11-2.12.0-py3-none-any.whl (234 kB)\n","Requirement already satisfied: setuptools>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from fasttext==0.9.1->mmf==1.0.0rc12) (67.7.2)\n","Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from ftfy==5.8->mmf==1.0.0rc12) (0.2.13)\n","Collecting gitdb<5,>=4.0.1 (from GitPython==3.1.30->mmf==1.0.0rc12)\n","  Downloading gitdb-4.0.11-py3-none-any.whl (62 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting portalocker (from iopath==0.1.8->mmf==1.0.0rc12)\n","  Downloading portalocker-2.8.2-py3-none-any.whl (17 kB)\n","Requirement already satisfied: cffi>=0.8 in /usr/local/lib/python3.10/dist-packages (from lmdb==0.98->mmf==1.0.0rc12) (1.16.0)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib==3.8.0->mmf==1.0.0rc12) (1.2.1)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib==3.8.0->mmf==1.0.0rc12) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib==3.8.0->mmf==1.0.0rc12) (4.51.0)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib==3.8.0->mmf==1.0.0rc12) (1.4.5)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib==3.8.0->mmf==1.0.0rc12) (24.0)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib==3.8.0->mmf==1.0.0rc12) (3.1.2)\n","Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib==3.8.0->mmf==1.0.0rc12) (2.8.2)\n","Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk==3.6.6->mmf==1.0.0rc12) (8.1.7)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk==3.6.6->mmf==1.0.0rc12) (1.4.0)\n","Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk==3.6.6->mmf==1.0.0rc12) (2023.12.25)\n","Collecting chardet<4,>=3.0.2 (from requests==2.23.0->mmf==1.0.0rc12)\n","  Downloading chardet-3.0.4-py2.py3-none-any.whl (133 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.4/133.4 kB\u001b[0m \u001b[31m22.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting idna<3,>=2.5 (from requests==2.23.0->mmf==1.0.0rc12)\n","  Downloading idna-2.10-py2.py3-none-any.whl (58 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.8/58.8 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 (from requests==2.23.0->mmf==1.0.0rc12)\n","  Downloading urllib3-1.25.11-py2.py3-none-any.whl (127 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m128.0/128.0 kB\u001b[0m \u001b[31m20.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests==2.23.0->mmf==1.0.0rc12) (2024.2.2)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sklearn==0.0->mmf==1.0.0rc12) (1.2.2)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets<=2.18.0->mmf==1.0.0rc12) (3.13.4)\n","Requirement already satisfied: pyarrow>=12.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets<=2.18.0->mmf==1.0.0rc12) (14.0.2)\n","Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets<=2.18.0->mmf==1.0.0rc12) (0.6)\n","Collecting dill<0.3.9,>=0.3.0 (from datasets<=2.18.0->mmf==1.0.0rc12)\n","  Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m18.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets<=2.18.0->mmf==1.0.0rc12) (2.0.3)\n","Collecting xxhash (from datasets<=2.18.0->mmf==1.0.0rc12)\n","  Downloading xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m28.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting multiprocess (from datasets<=2.18.0->mmf==1.0.0rc12)\n","  Downloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m21.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: fsspec[http]<=2024.2.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets<=2.18.0->mmf==1.0.0rc12) (2023.6.0)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets<=2.18.0->mmf==1.0.0rc12) (3.9.5)\n","Requirement already satisfied: huggingface-hub>=0.19.4 in /usr/local/lib/python3.10/dist-packages (from datasets<=2.18.0->mmf==1.0.0rc12) (0.20.3)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets<=2.18.0->mmf==1.0.0rc12) (6.0.1)\n","Collecting antlr4-python3-runtime==4.9.* (from omegaconf<=2.3,>=2.0.6->mmf==1.0.0rc12)\n","  Downloading antlr4-python3-runtime-4.9.3.tar.gz (117 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.0/117.0 kB\u001b[0m \u001b[31m19.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: torch>=1.8.* in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning<=1.6.0->mmf==1.0.0rc12) (2.2.1+cu121)\n","Requirement already satisfied: tensorboard>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning<=1.6.0->mmf==1.0.0rc12) (2.15.2)\n","Collecting torchmetrics>=0.4.1 (from pytorch-lightning<=1.6.0->mmf==1.0.0rc12)\n","  Downloading torchmetrics-1.3.2-py3-none-any.whl (841 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m841.5/841.5 kB\u001b[0m \u001b[31m72.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting pyDeprecate<0.4.0,>=0.3.1 (from pytorch-lightning<=1.6.0->mmf==1.0.0rc12)\n","  Downloading pyDeprecate-0.3.2-py3-none-any.whl (10 kB)\n","Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning<=1.6.0->mmf==1.0.0rc12) (4.11.0)\n","Collecting torch>=1.8.* (from pytorch-lightning<=1.6.0->mmf==1.0.0rc12)\n","  Downloading torch-2.2.0-cp310-cp310-manylinux1_x86_64.whl (755.5 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m755.5/755.5 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.*->pytorch-lightning<=1.6.0->mmf==1.0.0rc12) (1.12)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.*->pytorch-lightning<=1.6.0->mmf==1.0.0rc12) (3.3)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.*->pytorch-lightning<=1.6.0->mmf==1.0.0rc12) (3.1.3)\n","Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch>=1.8.*->pytorch-lightning<=1.6.0->mmf==1.0.0rc12)\n","  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n","Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch>=1.8.*->pytorch-lightning<=1.6.0->mmf==1.0.0rc12)\n","  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n","Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch>=1.8.*->pytorch-lightning<=1.6.0->mmf==1.0.0rc12)\n","  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n","Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch>=1.8.*->pytorch-lightning<=1.6.0->mmf==1.0.0rc12)\n","  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n","Collecting nvidia-cublas-cu12==12.1.3.1 (from torch>=1.8.*->pytorch-lightning<=1.6.0->mmf==1.0.0rc12)\n","  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n","Collecting nvidia-cufft-cu12==11.0.2.54 (from torch>=1.8.*->pytorch-lightning<=1.6.0->mmf==1.0.0rc12)\n","  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n","Collecting nvidia-curand-cu12==10.3.2.106 (from torch>=1.8.*->pytorch-lightning<=1.6.0->mmf==1.0.0rc12)\n","  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n","Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch>=1.8.*->pytorch-lightning<=1.6.0->mmf==1.0.0rc12)\n","  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n","Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch>=1.8.*->pytorch-lightning<=1.6.0->mmf==1.0.0rc12)\n","  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n","Collecting nvidia-nccl-cu12==2.19.3 (from torch>=1.8.*->pytorch-lightning<=1.6.0->mmf==1.0.0rc12)\n","  Using cached nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n","Collecting nvidia-nvtx-cu12==12.1.105 (from torch>=1.8.*->pytorch-lightning<=1.6.0->mmf==1.0.0rc12)\n","  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n","Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.*->pytorch-lightning<=1.6.0->mmf==1.0.0rc12) (2.2.0)\n","Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.8.*->pytorch-lightning<=1.6.0->mmf==1.0.0rc12)\n","  Using cached nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n","Requirement already satisfied: torchdata==0.7.1 in /usr/local/lib/python3.10/dist-packages (from torchtext<=0.17.0->mmf==1.0.0rc12) (0.7.1)\n","Collecting tokenizers<0.19,>=0.14 (from transformers<=4.39.2->mmf==1.0.0rc12)\n","  Downloading tokenizers-0.15.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m108.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers<=4.39.2->mmf==1.0.0rc12) (0.4.3)\n","Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=0.8->lmdb==0.98->mmf==1.0.0rc12) (2.22)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets<=2.18.0->mmf==1.0.0rc12) (1.3.1)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets<=2.18.0->mmf==1.0.0rc12) (23.2.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets<=2.18.0->mmf==1.0.0rc12) (1.4.1)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets<=2.18.0->mmf==1.0.0rc12) (6.0.5)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets<=2.18.0->mmf==1.0.0rc12) (1.9.4)\n","Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets<=2.18.0->mmf==1.0.0rc12) (4.0.3)\n","Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->GitPython==3.1.30->mmf==1.0.0rc12)\n","  Downloading smmap-5.0.1-py3-none-any.whl (24 kB)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib==3.8.0->mmf==1.0.0rc12) (1.16.0)\n","Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.2.0->pytorch-lightning<=1.6.0->mmf==1.0.0rc12) (1.4.0)\n","Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.2.0->pytorch-lightning<=1.6.0->mmf==1.0.0rc12) (1.62.2)\n","Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.2.0->pytorch-lightning<=1.6.0->mmf==1.0.0rc12) (2.27.0)\n","Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.2.0->pytorch-lightning<=1.6.0->mmf==1.0.0rc12) (1.2.0)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.2.0->pytorch-lightning<=1.6.0->mmf==1.0.0rc12) (3.6)\n","Requirement already satisfied: protobuf!=4.24.0,>=3.19.6 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.2.0->pytorch-lightning<=1.6.0->mmf==1.0.0rc12) (3.20.3)\n","Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.2.0->pytorch-lightning<=1.6.0->mmf==1.0.0rc12) (0.7.2)\n","Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.2.0->pytorch-lightning<=1.6.0->mmf==1.0.0rc12) (3.0.2)\n","Collecting lightning-utilities>=0.8.0 (from torchmetrics>=0.4.1->pytorch-lightning<=1.6.0->mmf==1.0.0rc12)\n","  Downloading lightning_utilities-0.11.2-py3-none-any.whl (26 kB)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets<=2.18.0->mmf==1.0.0rc12) (2023.4)\n","Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets<=2.18.0->mmf==1.0.0rc12) (2024.1)\n","Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sklearn==0.0->mmf==1.0.0rc12) (1.11.4)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sklearn==0.0->mmf==1.0.0rc12) (3.4.0)\n","Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning<=1.6.0->mmf==1.0.0rc12) (5.3.3)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning<=1.6.0->mmf==1.0.0rc12) (0.4.0)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning<=1.6.0->mmf==1.0.0rc12) (4.9)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard>=2.2.0->pytorch-lightning<=1.6.0->mmf==1.0.0rc12) (1.3.1)\n","Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard>=2.2.0->pytorch-lightning<=1.6.0->mmf==1.0.0rc12) (2.1.5)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.8.*->pytorch-lightning<=1.6.0->mmf==1.0.0rc12) (1.3.0)\n","Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning<=1.6.0->mmf==1.0.0rc12) (0.6.0)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard>=2.2.0->pytorch-lightning<=1.6.0->mmf==1.0.0rc12) (3.2.2)\n","Building wheels for collected packages: mmf, editdistance, fasttext, ftfy, lmdb, sklearn, termcolor, antlr4-python3-runtime\n","  Building editable for mmf (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for mmf: filename=mmf-1.0.0rc12-0.editable-cp310-cp310-linux_x86_64.whl size=10755 sha256=15f1acd402371700891ced46eb31d11f23a46f22d07bf06f543fc1e5c9fff6f7\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-o0_enr0g/wheels/f3/41/b2/9552a7d4dcecee3a8003f8e3361bd279e71e8dd1cc0f5b3684\n","  Building wheel for editdistance (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for editdistance: filename=editdistance-0.5.3-cp310-cp310-linux_x86_64.whl size=255254 sha256=b44b6f7fad97ed75d562fa5046988ebe748bb048841db5d64f207dd903659fa0\n","  Stored in directory: /root/.cache/pip/wheels/06/b3/f7/557b89622c8b55251efd308bd467d9ef0f81080a97e027674c\n","  Building wheel for fasttext (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for fasttext: filename=fasttext-0.9.1-cp310-cp310-linux_x86_64.whl size=3409616 sha256=1b7db7a0e321715deede39b3355657db547849029eb085ff510aa0e64a8a3158\n","  Stored in directory: /root/.cache/pip/wheels/24/4a/e3/0bcad6def7f2e2b231ebc85d09536e81cf13d12e73621e88fc\n","  Building wheel for ftfy (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for ftfy: filename=ftfy-5.8-py3-none-any.whl size=45630 sha256=20293b40e55595c26f1634560a8064c3643dfc45f1dd89c06620ef9a9b6f00ea\n","  Stored in directory: /root/.cache/pip/wheels/3c/4c/e5/2e9608f1a6e8ec8410d77afadbba870d1c3bf11eaddfa4088d\n","  Building wheel for lmdb (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for lmdb: filename=lmdb-0.98-cp310-cp310-linux_x86_64.whl size=255330 sha256=d42d2d825d27fc568497f44138ac63128cab5ae00f9297aea3d106f53eb77941\n","  Stored in directory: /root/.cache/pip/wheels/07/40/67/62af391284e5720eee8f56c590ba72770b32e91461ed08c4c8\n","  Building wheel for sklearn (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for sklearn: filename=sklearn-0.0-py2.py3-none-any.whl size=1303 sha256=24f6d9a3ee1b4793b16581df0329d8bf89fa3ad38ed96be781c2dead8dcbf510\n","  Stored in directory: /root/.cache/pip/wheels/9b/13/01/6f3a7fd641f90e1f6c8c7cded057f3394f451f340371c68f3d\n","  Building wheel for termcolor (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for termcolor: filename=termcolor-1.1.0-py3-none-any.whl size=4832 sha256=c77d2cbb3bbb4cc98bc55941983a82a90aba4d948a4fb0c7205ee9857e8e0653\n","  Stored in directory: /root/.cache/pip/wheels/a1/49/46/1b13a65d8da11238af9616b00fdde6d45b0f95d9291bac8452\n","  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.9.3-py3-none-any.whl size=144554 sha256=676f3caa3476a0a46c3cc6737fbe6253373e6ef6585b3fa32c8f5e510532bdf4\n","  Stored in directory: /root/.cache/pip/wheels/12/93/dd/1f6a127edc45659556564c5730f6d4e300888f4bca2d4c5a88\n","Successfully built mmf editdistance fasttext ftfy lmdb sklearn termcolor antlr4-python3-runtime\n","Installing collected packages: termcolor, editdistance, chardet, antlr4-python3-runtime, xxhash, urllib3, smmap, pyDeprecate, pybind11, portalocker, pillow, omegaconf, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, numpy, nltk, lightning-utilities, idna, ftfy, dill, requests, nvidia-cusparse-cu12, nvidia-cudnn-cu12, multiprocess, lmdb, iopath, gitdb, fasttext, nvidia-cusolver-cu12, matplotlib, GitPython, torch, tokenizers, sklearn, transformers, torchvision, torchmetrics, torchaudio, datasets, torchtext, pytorch-lightning, mmf\n","  Attempting uninstall: termcolor\n","    Found existing installation: termcolor 2.4.0\n","    Uninstalling termcolor-2.4.0:\n","      Successfully uninstalled termcolor-2.4.0\n","  Attempting uninstall: editdistance\n","    Found existing installation: editdistance 0.6.2\n","    Uninstalling editdistance-0.6.2:\n","      Successfully uninstalled editdistance-0.6.2\n","  Attempting uninstall: chardet\n","    Found existing installation: chardet 5.2.0\n","    Uninstalling chardet-5.2.0:\n","      Successfully uninstalled chardet-5.2.0\n","  Attempting uninstall: urllib3\n","    Found existing installation: urllib3 2.0.7\n","    Uninstalling urllib3-2.0.7:\n","      Successfully uninstalled urllib3-2.0.7\n","  Attempting uninstall: pillow\n","    Found existing installation: Pillow 9.4.0\n","    Uninstalling Pillow-9.4.0:\n","      Successfully uninstalled Pillow-9.4.0\n","  Attempting uninstall: numpy\n","    Found existing installation: numpy 1.25.2\n","    Uninstalling numpy-1.25.2:\n","      Successfully uninstalled numpy-1.25.2\n","  Attempting uninstall: nltk\n","    Found existing installation: nltk 3.8.1\n","    Uninstalling nltk-3.8.1:\n","      Successfully uninstalled nltk-3.8.1\n","  Attempting uninstall: idna\n","    Found existing installation: idna 3.7\n","    Uninstalling idna-3.7:\n","      Successfully uninstalled idna-3.7\n","  Attempting uninstall: requests\n","    Found existing installation: requests 2.31.0\n","    Uninstalling requests-2.31.0:\n","      Successfully uninstalled requests-2.31.0\n","  Attempting uninstall: matplotlib\n","    Found existing installation: matplotlib 3.7.1\n","    Uninstalling matplotlib-3.7.1:\n","      Successfully uninstalled matplotlib-3.7.1\n","  Attempting uninstall: torch\n","    Found existing installation: torch 2.2.1+cu121\n","    Uninstalling torch-2.2.1+cu121:\n","      Successfully uninstalled torch-2.2.1+cu121\n","  Attempting uninstall: tokenizers\n","    Found existing installation: tokenizers 0.19.1\n","    Uninstalling tokenizers-0.19.1:\n","      Successfully uninstalled tokenizers-0.19.1\n","  Attempting uninstall: transformers\n","    Found existing installation: transformers 4.40.0\n","    Uninstalling transformers-4.40.0:\n","      Successfully uninstalled transformers-4.40.0\n","  Attempting uninstall: torchvision\n","    Found existing installation: torchvision 0.17.1+cu121\n","    Uninstalling torchvision-0.17.1+cu121:\n","      Successfully uninstalled torchvision-0.17.1+cu121\n","  Attempting uninstall: torchaudio\n","    Found existing installation: torchaudio 2.2.1+cu121\n","    Uninstalling torchaudio-2.2.1+cu121:\n","      Successfully uninstalled torchaudio-2.2.1+cu121\n","  Attempting uninstall: torchtext\n","    Found existing installation: torchtext 0.17.1\n","    Uninstalling torchtext-0.17.1:\n","      Successfully uninstalled torchtext-0.17.1\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","bigframes 1.2.0 requires requests>=2.27.1, but you have requests 2.23.0 which is incompatible.\n","google-colab 1.0.0 requires requests==2.31.0, but you have requests 2.23.0 which is incompatible.\n","tweepy 4.14.0 requires requests<3,>=2.27.0, but you have requests 2.23.0 which is incompatible.\n","yfinance 0.2.38 requires requests>=2.31, but you have requests 2.23.0 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed GitPython-3.1.30 antlr4-python3-runtime-4.9.3 chardet-3.0.4 datasets-2.18.0 dill-0.3.8 editdistance-0.5.3 fasttext-0.9.1 ftfy-5.8 gitdb-4.0.11 idna-2.10 iopath-0.1.8 lightning-utilities-0.11.2 lmdb-0.98 matplotlib-3.8.0 mmf-1.0.0rc12 multiprocess-0.70.16 nltk-3.6.6 numpy-1.26.4 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.1.105 omegaconf-2.3.0 pillow-9.3.0 portalocker-2.8.2 pyDeprecate-0.3.2 pybind11-2.12.0 pytorch-lightning-1.6.0 requests-2.23.0 sklearn-0.0 smmap-5.0.1 termcolor-1.1.0 tokenizers-0.15.2 torch-2.2.0 torchaudio-2.2.0 torchmetrics-1.3.2 torchtext-0.17.0 torchvision-0.17.0 transformers-4.39.2 urllib3-1.25.11 xxhash-3.4.1\n"]},{"output_type":"display_data","data":{"application/vnd.colab-display-data+json":{"pip_warning":{"packages":["PIL","matplotlib","mpl_toolkits","pydevd_plugins"]},"id":"3a3b83244fde4c81b9d9dfa72caa4074"}},"metadata":{}}],"source":["# local mmf installation\n","%cd '/content/drive/MyDrive/hateful-memes/mmf-main'\n","!pip install -e ."]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":119085,"status":"ok","timestamp":1713979064590,"user":{"displayName":"Aneri Shah","userId":"14009776940163552827"},"user_tz":420},"id":"AQxnn1Tvhv9y","outputId":"72c95f72-8a8b-4969-e2d8-9aa6c86cda98"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/transformers/deepspeed.py:23: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations\n","  warnings.warn(\n"]}],"source":["### Import General Modules, 3min\n","from mmf.common.registry import registry\n","from mmf.models.mmbt import MMBT\n","from mmf.utils.build import build_dataset\n","from mmf.utils.env import setup_imports\n","import matplotlib.pyplot as plt\n","from PIL import Image\n","import torch\n","\n","### Import Modules for Fusion Model\n","# All model using MMF need to inherit BaseModel\n","from mmf.models.base_model import BaseModel\n","# Builder methods for image encoder and classifier\n","from mmf.utils.build import build_classifier_layer, build_image_encoder,build_text_encoder\n","\n","setup_imports()\n","\n","# !mmf_convert_hm --zip_file=\"/content/drive/MyDrive/hateful-memes/hateful_memes.zip\" --bypass_checksum=1\n","# dataset = build_dataset(\"hateful_memes\")"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"ffpJ8n9aiMDA","executionInfo":{"status":"error","timestamp":1713979529685,"user_tz":420,"elapsed":465111,"user":{"displayName":"Aneri Shah","userId":"14009776940163552827"}},"outputId":"f276b31d-c87f-41bb-afad-00a1b9a2adc9"},"outputs":[{"output_type":"stream","name":"stdout","text":["2024-04-24 17:17:47.906276: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","2024-04-24 17:17:47.906331: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","2024-04-24 17:17:47.908242: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","2024-04-24 17:17:48.966906: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","2024-04-24 17:17:53.173737: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:47] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n","/usr/local/lib/python3.10/dist-packages/transformers/deepspeed.py:23: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/omegaconf/grammar_visitor.py:253: UserWarning: In the sequence `MMF_LOG_DIR,` some elements are missing: please replace them with empty quoted strings. See https://github.com/omry/omegaconf/issues/572 for details.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/omegaconf/grammar_visitor.py:253: UserWarning: In the sequence `MMF_REPORT_DIR,` some elements are missing: please replace them with empty quoted strings. See https://github.com/omry/omegaconf/issues/572 for details.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/omegaconf/grammar_visitor.py:253: UserWarning: In the sequence `MMF_TENSORBOARD_LOGDIR,` some elements are missing: please replace them with empty quoted strings. See https://github.com/omry/omegaconf/issues/572 for details.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/omegaconf/grammar_visitor.py:253: UserWarning: In the sequence `MMF_WANDB_LOGDIR,` some elements are missing: please replace them with empty quoted strings. See https://github.com/omry/omegaconf/issues/572 for details.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/omegaconf/grammar_visitor.py:253: UserWarning: In the sequence `MMF_USER_DIR,` some elements are missing: please replace them with empty quoted strings. See https://github.com/omry/omegaconf/issues/572 for details.\n","  warnings.warn(\n","Data folder is /root/.cache/torch/mmf/data\n","Zip path is /content/drive/MyDrive/Deep Learning Final Project Part 2/hateful_memes.zip\n","Copying /content/drive/MyDrive/Deep Learning Final Project Part 2/hateful_memes.zip\n","Unzipping /content/drive/MyDrive/Deep Learning Final Project Part 2/hateful_memes.zip\n","Extracting the zip can take time. Sit back and relax.\n","Moving train.jsonl\n","Moving dev_seen.jsonl\n","Moving test_seen.jsonl\n","Moving dev_unseen.jsonl\n","Moving test_unseen.jsonl\n","Moving train_dev.jsonl\n","Moving train_dev_transform_half.jsonl\n","Moving train_dev_transform_all.jsonl\n","Moving train_dev_transform_all_combined.jsonl\n","Moving img\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/omegaconf/grammar_visitor.py:253: UserWarning: In the sequence `MMF_LOG_DIR,` some elements are missing: please replace them with empty quoted strings. See https://github.com/omry/omegaconf/issues/572 for details.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/omegaconf/grammar_visitor.py:253: UserWarning: In the sequence `MMF_REPORT_DIR,` some elements are missing: please replace them with empty quoted strings. See https://github.com/omry/omegaconf/issues/572 for details.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/omegaconf/grammar_visitor.py:253: UserWarning: In the sequence `MMF_TENSORBOARD_LOGDIR,` some elements are missing: please replace them with empty quoted strings. See https://github.com/omry/omegaconf/issues/572 for details.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/omegaconf/grammar_visitor.py:253: UserWarning: In the sequence `MMF_WANDB_LOGDIR,` some elements are missing: please replace them with empty quoted strings. See https://github.com/omry/omegaconf/issues/572 for details.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/omegaconf/grammar_visitor.py:253: UserWarning: In the sequence `MMF_USER_DIR,` some elements are missing: please replace them with empty quoted strings. See https://github.com/omry/omegaconf/issues/572 for details.\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["[ Downloading: https://dl.fbaipublicfiles.com/mmf/data/datasets/hateful_memes/defaults/extras.tar.gz to /root/.cache/torch/mmf/data/datasets/hateful_memes/defaults/extras.tar.gz ]\n"]},{"output_type":"stream","name":"stderr","text":["Downloading extras.tar.gz: 100%|██████████| 211k/211k [00:00<00:00, 1.43MB/s]"]},{"output_type":"stream","name":"stdout","text":["[ Starting checksum for extras.tar.gz]\n","[ Checksum successful for extras.tar.gz]\n","Unpacking extras.tar.gz\n"]},{"output_type":"stream","name":"stderr","text":["\n","/root/.cache/torch/mmf/glove.6B.zip: 0.00B [02:09, ?B/s]\n"]},{"output_type":"error","ename":"URLError","evalue":"<urlopen error [Errno 110] Connection timed out>","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTimeoutError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m/usr/lib/python3.10/urllib/request.py\u001b[0m in \u001b[0;36mdo_open\u001b[0;34m(self, http_class, req, **http_conn_args)\u001b[0m\n\u001b[1;32m   1347\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1348\u001b[0;31m                 h.request(req.get_method(), req.selector, req.data, headers,\n\u001b[0m\u001b[1;32m   1349\u001b[0m                           encode_chunked=req.has_header('Transfer-encoding'))\n","\u001b[0;32m/usr/lib/python3.10/http/client.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, body, headers, encode_chunked)\u001b[0m\n\u001b[1;32m   1282\u001b[0m         \u001b[0;34m\"\"\"Send a complete request to the server.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1283\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_send_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencode_chunked\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1284\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.10/http/client.py\u001b[0m in \u001b[0;36m_send_request\u001b[0;34m(self, method, url, body, headers, encode_chunked)\u001b[0m\n\u001b[1;32m   1328\u001b[0m             \u001b[0mbody\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_encode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'body'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1329\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendheaders\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencode_chunked\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencode_chunked\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1330\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.10/http/client.py\u001b[0m in \u001b[0;36mendheaders\u001b[0;34m(self, message_body, encode_chunked)\u001b[0m\n\u001b[1;32m   1277\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mCannotSendHeader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1278\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_send_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage_body\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencode_chunked\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencode_chunked\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1279\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.10/http/client.py\u001b[0m in \u001b[0;36m_send_output\u001b[0;34m(self, message_body, encode_chunked)\u001b[0m\n\u001b[1;32m   1037\u001b[0m         \u001b[0;32mdel\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_buffer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1038\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1039\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.10/http/client.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    975\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_open\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 976\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    977\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.10/http/client.py\u001b[0m in \u001b[0;36mconnect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    941\u001b[0m         \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maudit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"http.client.connect\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhost\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mport\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 942\u001b[0;31m         self.sock = self._create_connection(\n\u001b[0m\u001b[1;32m    943\u001b[0m             (self.host,self.port), self.timeout, self.source_address)\n","\u001b[0;32m/usr/lib/python3.10/socket.py\u001b[0m in \u001b[0;36mcreate_connection\u001b[0;34m(address, timeout, source_address)\u001b[0m\n\u001b[1;32m    844\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 845\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    846\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.10/socket.py\u001b[0m in \u001b[0;36mcreate_connection\u001b[0;34m(address, timeout, source_address)\u001b[0m\n\u001b[1;32m    832\u001b[0m                 \u001b[0msock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource_address\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 833\u001b[0;31m             \u001b[0msock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msa\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    834\u001b[0m             \u001b[0;31m# Break explicitly a reference cycle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mTimeoutError\u001b[0m: [Errno 110] Connection timed out","\nDuring handling of the above exception, another exception occurred:\n","\u001b[0;31mURLError\u001b[0m                                  Traceback (most recent call last)","\u001b[0;32m<ipython-input-2-47f36b8dd7ab>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# provide the path to the compressed dataset folder (should have hateful_memes as root folder when uncompressed)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'mmf_convert_hm --zip_file=\"/content/drive/MyDrive/Deep Learning Final Project Part 2/hateful_memes.zip\" --bypass_checksum=1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"hateful_memes\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/content/drive/MyDrive/hateful-memes/mmf-main/mmf/utils/build.py\u001b[0m in \u001b[0;36mbuild_dataset\u001b[0;34m(dataset_key, config, dataset_type)\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0mdatamodule_instance\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 200\u001b[0;31m     \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatamodule_instance\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    201\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdatamodule_instance\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"update_registry_for_model\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m         \u001b[0mdatamodule_instance\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_registry_for_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/content/drive/MyDrive/hateful-memes/mmf-main/mmf/datasets/base_dataset_builder.py\u001b[0m in \u001b[0;36mload_dataset\u001b[0;34m(self, config, dataset_type, *args, **kwargs)\u001b[0m\n\u001b[1;32m    165\u001b[0m             \u001b[0;31m# Checking for init_processors allows us to load some datasets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m             \u001b[0;31m# which don't have processors and don't inherit from BaseDataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 167\u001b[0;31m             \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit_processors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    168\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/content/drive/MyDrive/hateful-memes/mmf-main/mmf/datasets/concat_dataset.py\u001b[0m in \u001b[0;36m_call_all_datasets_func\u001b[0;34m(self, name, *args, **kwargs)\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_all_datasets_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mdataset\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatasets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m                 \u001b[0;31m# TODO: Log a warning here\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/content/drive/MyDrive/hateful-memes/mmf-main/mmf/datasets/builders/hateful_memes/dataset.py\u001b[0m in \u001b[0;36minit_processors\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minit_processors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit_processors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    110\u001b[0m         \u001b[0;31m# Assign transforms to the image_db\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage_db\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage_processor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/content/drive/MyDrive/hateful-memes/mmf-main/mmf/datasets/base_dataset.py\u001b[0m in \u001b[0;36minit_processors\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0mextra_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"data_dir\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_dir\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0mreg_key\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"{self._dataset_name}_{{}}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m         processor_dict = build_processors(\n\u001b[0m\u001b[1;32m     59\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocessors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreg_key\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mextra_params\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m         )\n","\u001b[0;32m/content/drive/MyDrive/hateful-memes/mmf-main/mmf/utils/build.py\u001b[0m in \u001b[0;36mbuild_processors\u001b[0;34m(processors_config, registry_key, *args, **kwargs)\u001b[0m\n\u001b[1;32m    580\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    581\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mprocessor_instance\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 582\u001b[0;31m             \u001b[0mprocessor_instance\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mProcessor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocessor_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    583\u001b[0m             \u001b[0;31m# We don't register back here as in case of hub interface, we\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    584\u001b[0m             \u001b[0;31m# want the processors to be instantiate every time. BaseDataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/content/drive/MyDrive/hateful-memes/mmf-main/mmf/datasets/processors/processors.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, config, *args, **kwargs)\u001b[0m\n\u001b[1;32m    162\u001b[0m             \u001b[0mparams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 164\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocessor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocessor_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    165\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dir_representation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/content/drive/MyDrive/hateful-memes/mmf-main/mmf/datasets/processors/processors.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, config, *args, **kwargs)\u001b[0m\n\u001b[1;32m    251\u001b[0m             )\n\u001b[1;32m    252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 253\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVocab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    254\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_init_extras\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/content/drive/MyDrive/hateful-memes/mmf-main/mmf/utils/vocab.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, *args, **params)\u001b[0m\n\u001b[1;32m     41\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"No vocab path or embedding_name passed for vocab\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mIntersectedVocab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mvocab_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"extracted\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/content/drive/MyDrive/hateful-memes/mmf-main/mmf/utils/vocab.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, vocab_file, embedding_name, *args, **kwargs)\u001b[0m\n\u001b[1;32m    296\u001b[0m         \u001b[0;31m# download it in case it doesn't exist\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_main\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 298\u001b[0;31m             \u001b[0mvocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpretrained_aliases\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0membedding_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcache\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvector_cache\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    299\u001b[0m         \u001b[0msynchronize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    300\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchtext/vocab/vectors.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, dim, **kwargs)\u001b[0m\n\u001b[1;32m    221\u001b[0m         \u001b[0murl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m         \u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"glove.{}.{}d.txt\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 223\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mGloVe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    224\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchtext/vocab/vectors.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, cache, url, unk_init, max_vectors)\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munk_init\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0munk_init\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0munk_init\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcache\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_vectors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_vectors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchtext/vocab/vectors.py\u001b[0m in \u001b[0;36mcache\u001b[0;34m(self, name, cache, url, max_vectors)\u001b[0m\n\u001b[1;32m     96\u001b[0m                     \u001b[0;32mwith\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"B\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munit_scale\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mminiters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdesc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdest\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m                         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m                             \u001b[0murlretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreporthook\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreporthook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m                         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# remove the partial zip file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m                             \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mremove\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.10/urllib/request.py\u001b[0m in \u001b[0;36murlretrieve\u001b[0;34m(url, filename, reporthook, data)\u001b[0m\n\u001b[1;32m    239\u001b[0m     \u001b[0murl_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_splittype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 241\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mcontextlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclosing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murlopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    242\u001b[0m         \u001b[0mheaders\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.10/urllib/request.py\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(url, data, timeout, cafile, capath, cadefault, context)\u001b[0m\n\u001b[1;32m    214\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m         \u001b[0mopener\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_opener\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mopener\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0minstall_opener\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopener\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.10/urllib/request.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(self, fullurl, data, timeout)\u001b[0m\n\u001b[1;32m    517\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    518\u001b[0m         \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maudit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'urllib.Request'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfull_url\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 519\u001b[0;31m         \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    520\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    521\u001b[0m         \u001b[0;31m# post-process response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.10/urllib/request.py\u001b[0m in \u001b[0;36m_open\u001b[0;34m(self, req, data)\u001b[0m\n\u001b[1;32m    534\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    535\u001b[0m         \u001b[0mprotocol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 536\u001b[0;31m         result = self._call_chain(self.handle_open, protocol, protocol +\n\u001b[0m\u001b[1;32m    537\u001b[0m                                   '_open', req)\n\u001b[1;32m    538\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.10/urllib/request.py\u001b[0m in \u001b[0;36m_call_chain\u001b[0;34m(self, chain, kind, meth_name, *args)\u001b[0m\n\u001b[1;32m    494\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhandler\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mhandlers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m             \u001b[0mfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeth_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 496\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    497\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    498\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.10/urllib/request.py\u001b[0m in \u001b[0;36mhttp_open\u001b[0;34m(self, req)\u001b[0m\n\u001b[1;32m   1375\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1376\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mhttp_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1377\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdo_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhttp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mHTTPConnection\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1378\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1379\u001b[0m     \u001b[0mhttp_request\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAbstractHTTPHandler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdo_request_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.10/urllib/request.py\u001b[0m in \u001b[0;36mdo_open\u001b[0;34m(self, http_class, req, **http_conn_args)\u001b[0m\n\u001b[1;32m   1349\u001b[0m                           encode_chunked=req.has_header('Transfer-encoding'))\n\u001b[1;32m   1350\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# timeout error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1351\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mURLError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1352\u001b[0m             \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetresponse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1353\u001b[0m         \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mURLError\u001b[0m: <urlopen error [Errno 110] Connection timed out>"]}],"source":["### Build Dataset\n","# provide the path to the compressed dataset folder (should have hateful_memes as root folder when uncompressed)\n","!mmf_convert_hm --zip_file=\"/content/drive/MyDrive/Deep Learning Final Project Part 2/hateful_memes.zip\" --bypass_checksum=1\n","dataset = build_dataset(\"hateful_memes\")"]},{"cell_type":"markdown","source":["# Training\n","\n","The sections below rely on training `json` files that were created in a separate notebook in the Prerequisites section. If you see a warning about these files not existing please make sure to follow the steps in that section."],"metadata":{"id":"TQDCamGAVzuk"}},{"cell_type":"markdown","source":["## Pre-trained with COCO"],"metadata":{"id":"2-qj2OFtNpOO"}},{"cell_type":"markdown","source":["### Dataset: Default"],"metadata":{"id":"eAzNDbW5XwfB"}},{"cell_type":"code","source":["!mmf_run config=\"./configs/defaults.yaml\" \\\n","  model=visual_bert \\\n","  dataset=hateful_memes \\\n","  run_type=train_val \\\n","  training.log_interval=200 \\\n","  training.max_updates=1500 \\\n","  training.batch_size=64 \\\n","  training.evaluation_interval=200 \\\n","  training.tensorboard=True \\\n","  training.checkpoint_interval=200 \\\n","  checkpoint.resume_pretrained=True \\\n","  checkpoint.resume_zoo=visual_bert.pretrained.coco \\\n","  dataset_config.hateful_memes.annotations.train[0]=\"hateful_memes/defaults/annotations/train.jsonl\" \\\n","  dataset_config.hateful_memes.annotations.val[0]=\"hateful_memes/defaults/annotations/dev_unseen.jsonl\" \\\n","  dataset_config.hateful_memes.annotations.test[0]=\"hateful_memes/defaults/annotations/test_unseen.jsonl\""],"metadata":{"id":"aT8WYzHNUop9","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1713717420335,"user_tz":420,"elapsed":2217208,"user":{"displayName":"Aneri Shah","userId":"14009776940163552827"}},"outputId":"03a83d80-1efe-485d-d9e0-f687de3e612c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["2024-04-21 16:00:05.819493: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","2024-04-21 16:00:05.819544: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","2024-04-21 16:00:05.821140: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","2024-04-21 16:00:06.829005: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","2024-04-21 16:00:11.067734: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:47] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n","/usr/local/lib/python3.10/dist-packages/transformers/deepspeed.py:23: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/omegaconf/grammar_visitor.py:253: UserWarning: In the sequence `MMF_USER_DIR,` some elements are missing: please replace them with empty quoted strings. See https://github.com/omry/omegaconf/issues/572 for details.\n","  warnings.warn(\n","\u001b[32m2024-04-21T16:00:17 | mmf.utils.configuration: \u001b[0mOverriding option config to ./configs/defaults.yaml\n","\u001b[32m2024-04-21T16:00:17 | mmf.utils.configuration: \u001b[0mOverriding option model to visual_bert\n","\u001b[32m2024-04-21T16:00:17 | mmf.utils.configuration: \u001b[0mOverriding option datasets to hateful_memes\n","\u001b[32m2024-04-21T16:00:17 | mmf.utils.configuration: \u001b[0mOverriding option run_type to train_val\n","\u001b[32m2024-04-21T16:00:17 | mmf.utils.configuration: \u001b[0mOverriding option training.log_interval to 200\n","\u001b[32m2024-04-21T16:00:17 | mmf.utils.configuration: \u001b[0mOverriding option training.max_updates to 1500\n","\u001b[32m2024-04-21T16:00:17 | mmf.utils.configuration: \u001b[0mOverriding option training.batch_size to 64\n","\u001b[32m2024-04-21T16:00:17 | mmf.utils.configuration: \u001b[0mOverriding option training.evaluation_interval to 200\n","\u001b[32m2024-04-21T16:00:17 | mmf.utils.configuration: \u001b[0mOverriding option training.tensorboard to True\n","\u001b[32m2024-04-21T16:00:17 | mmf.utils.configuration: \u001b[0mOverriding option training.checkpoint_interval to 200\n","\u001b[32m2024-04-21T16:00:17 | mmf.utils.configuration: \u001b[0mOverriding option checkpoint.resume_pretrained to True\n","\u001b[32m2024-04-21T16:00:17 | mmf.utils.configuration: \u001b[0mOverriding option checkpoint.resume_zoo to visual_bert.pretrained.coco\n","\u001b[32m2024-04-21T16:00:17 | mmf.utils.configuration: \u001b[0mOverriding option dataset_config.hateful_memes.annotations.train[0] to hateful_memes/defaults/annotations/train.jsonl\n","\u001b[32m2024-04-21T16:00:17 | mmf.utils.configuration: \u001b[0mOverriding option dataset_config.hateful_memes.annotations.val[0] to hateful_memes/defaults/annotations/dev_unseen.jsonl\n","\u001b[32m2024-04-21T16:00:17 | mmf.utils.configuration: \u001b[0mOverriding option dataset_config.hateful_memes.annotations.test[0] to hateful_memes/defaults/annotations/test_unseen.jsonl\n","/usr/local/lib/python3.10/dist-packages/omegaconf/grammar_visitor.py:253: UserWarning: In the sequence `MMF_LOG_DIR,` some elements are missing: please replace them with empty quoted strings. See https://github.com/omry/omegaconf/issues/572 for details.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/omegaconf/grammar_visitor.py:253: UserWarning: In the sequence `MMF_REPORT_DIR,` some elements are missing: please replace them with empty quoted strings. See https://github.com/omry/omegaconf/issues/572 for details.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/omegaconf/grammar_visitor.py:253: UserWarning: In the sequence `MMF_TENSORBOARD_LOGDIR,` some elements are missing: please replace them with empty quoted strings. See https://github.com/omry/omegaconf/issues/572 for details.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/omegaconf/grammar_visitor.py:253: UserWarning: In the sequence `MMF_WANDB_LOGDIR,` some elements are missing: please replace them with empty quoted strings. See https://github.com/omry/omegaconf/issues/572 for details.\n","  warnings.warn(\n","\u001b[32m2024-04-21T16:00:17 | mmf: \u001b[0mLogging to: ./save/train.log\n","\u001b[32m2024-04-21T16:00:17 | mmf_cli.run: \u001b[0mNamespace(config_override=None, local_rank=None, opts=['config=./configs/defaults.yaml', 'model=visual_bert', 'dataset=hateful_memes', 'run_type=train_val', 'training.log_interval=200', 'training.max_updates=1500', 'training.batch_size=64', 'training.evaluation_interval=200', 'training.tensorboard=True', 'training.checkpoint_interval=200', 'checkpoint.resume_pretrained=True', 'checkpoint.resume_zoo=visual_bert.pretrained.coco', 'dataset_config.hateful_memes.annotations.train[0]=hateful_memes/defaults/annotations/train.jsonl', 'dataset_config.hateful_memes.annotations.val[0]=hateful_memes/defaults/annotations/dev_unseen.jsonl', 'dataset_config.hateful_memes.annotations.test[0]=hateful_memes/defaults/annotations/test_unseen.jsonl'])\n","\u001b[32m2024-04-21T16:00:17 | mmf_cli.run: \u001b[0mTorch version: 2.2.0+cu121\n","\u001b[32m2024-04-21T16:00:17 | mmf.utils.general: \u001b[0mCUDA Device 0 is: NVIDIA L4\n","\u001b[32m2024-04-21T16:00:17 | mmf_cli.run: \u001b[0mUsing seed 17565435\n","\u001b[32m2024-04-21T16:00:17 | mmf.trainers.mmf_trainer: \u001b[0mLoading datasets\n","[ Downloading: https://dl.fbaipublicfiles.com/mmf/data/datasets/hateful_memes/defaults/features/features_2020_10_01.tar.gz to /root/.cache/torch/mmf/data/datasets/hateful_memes/defaults/features/features.tar.gz ]\n","Downloading features.tar.gz: 100% 10.3G/10.3G [00:51<00:00, 201MB/s]\n","[ Starting checksum for features.tar.gz]\n","[ Checksum successful for features.tar.gz]\n","Unpacking features.tar.gz\n","tokenizer_config.json: 100% 48.0/48.0 [00:00<00:00, 348kB/s]\n","config.json: 100% 570/570 [00:00<00:00, 4.95MB/s]\n","loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--bert-base-uncased/snapshots/86b5e0934494bd15c9632b12f734a8a67f723594/config.json\n","Model config BertConfig {\n","  \"_name_or_path\": \"bert-base-uncased\",\n","  \"architectures\": [\n","    \"BertForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.38.2\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n","vocab.txt: 100% 232k/232k [00:00<00:00, 1.76MB/s]\n","tokenizer.json: 100% 466k/466k [00:00<00:00, 3.56MB/s]\n","loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--bert-base-uncased/snapshots/86b5e0934494bd15c9632b12f734a8a67f723594/vocab.txt\n","loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--bert-base-uncased/snapshots/86b5e0934494bd15c9632b12f734a8a67f723594/tokenizer.json\n","loading file added_tokens.json from cache at None\n","loading file special_tokens_map.json from cache at None\n","loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--bert-base-uncased/snapshots/86b5e0934494bd15c9632b12f734a8a67f723594/tokenizer_config.json\n","loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--bert-base-uncased/snapshots/86b5e0934494bd15c9632b12f734a8a67f723594/config.json\n","Model config BertConfig {\n","  \"_name_or_path\": \"bert-base-uncased\",\n","  \"architectures\": [\n","    \"BertForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.38.2\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n","\u001b[32m2024-04-21T16:03:45 | mmf.datasets.multi_datamodule: \u001b[0mMultitasking disabled by default for single dataset training\n","\u001b[32m2024-04-21T16:03:45 | mmf.datasets.multi_datamodule: \u001b[0mMultitasking disabled by default for single dataset training\n","\u001b[32m2024-04-21T16:03:45 | mmf.datasets.multi_datamodule: \u001b[0mMultitasking disabled by default for single dataset training\n","\u001b[32m2024-04-21T16:03:45 | mmf.trainers.mmf_trainer: \u001b[0mLoading model\n","Model config BertConfig {\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bert_model_name\": \"bert-base-uncased\",\n","  \"bypass_transformer\": false,\n","  \"classifier_dropout\": null,\n","  \"embedding_strategy\": \"plain\",\n","  \"finetune_lr_multiplier\": 1,\n","  \"freeze_base\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"losses\": [\n","    \"cross_entropy\"\n","  ],\n","  \"max_position_embeddings\": 512,\n","  \"model\": \"visual_bert\",\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"pooler_strategy\": \"default\",\n","  \"position_embedding_type\": \"absolute\",\n","  \"random_initialize\": false,\n","  \"special_visual_initialize\": true,\n","  \"training_head_type\": \"classification\",\n","  \"transformers_version\": \"4.38.2\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"visual_embedding_dim\": 2048,\n","  \"vocab_size\": 30522,\n","  \"zerobias\": false\n","}\n","\n","model.safetensors: 100% 440M/440M [00:01<00:00, 289MB/s]\n","loading weights file model.safetensors from cache at /root/.cache/torch/mmf/distributed_-1/models--bert-base-uncased/snapshots/86b5e0934494bd15c9632b12f734a8a67f723594/model.safetensors\n","Some weights of the model checkpoint at bert-base-uncased were not used when initializing VisualBERTBase: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n","- This IS expected if you are initializing VisualBERTBase from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing VisualBERTBase from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of VisualBERTBase were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['bert.embeddings.position_embeddings_visual.weight', 'bert.embeddings.projection.bias', 'bert.embeddings.projection.weight', 'bert.embeddings.token_type_embeddings_visual.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","\u001b[32m2024-04-21T16:03:47 | mmf.trainers.mmf_trainer: \u001b[0mLoading optimizer\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2024-04-21T16:03:47 | py.warnings: \u001b[0m/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n","\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2024-04-21T16:03:47 | py.warnings: \u001b[0m/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n","\n","\u001b[32m2024-04-21T16:03:47 | mmf.trainers.mmf_trainer: \u001b[0mLoading metrics\n","\u001b[32m2024-04-21T16:03:47 | mmf.utils.checkpoint: \u001b[0mLoading checkpoint\n","[ Downloading: https://dl.fbaipublicfiles.com/mmf/data/models/visual_bert/visual_bert.pretrained.coco_train_val.tar.gz to /root/.cache/torch/mmf/data/models/visual_bert.pretrained.coco.defaults/visual_bert.pretrained.coco_train_val.tar.gz ]\n","Downloading visual_bert.pretrained.coco_train_val.tar.gz: 100% 415M/415M [00:02<00:00, 144MB/s]\n","[ Starting checksum for visual_bert.pretrained.coco_train_val.tar.gz]\n","[ Checksum successful for visual_bert.pretrained.coco_train_val.tar.gz]\n","Unpacking visual_bert.pretrained.coco_train_val.tar.gz\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2024-04-21T16:03:56 | mmf: \u001b[0mKey data_parallel is not present in registry, returning default value of None\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2024-04-21T16:03:56 | mmf: \u001b[0mKey distributed is not present in registry, returning default value of None\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2024-04-21T16:03:56 | mmf: \u001b[0mKey data_parallel is not present in registry, returning default value of None\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2024-04-21T16:03:56 | mmf: \u001b[0mKey distributed is not present in registry, returning default value of None\n","\u001b[32m2024-04-21T16:03:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.word_embeddings.weight from model.bert.embeddings.word_embeddings.weight\n","\u001b[32m2024-04-21T16:03:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.position_embeddings.weight from model.bert.embeddings.position_embeddings.weight\n","\u001b[32m2024-04-21T16:03:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.token_type_embeddings.weight from model.bert.embeddings.token_type_embeddings.weight\n","\u001b[32m2024-04-21T16:03:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.LayerNorm.weight from model.bert.embeddings.LayerNorm.weight\n","\u001b[32m2024-04-21T16:03:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.LayerNorm.bias from model.bert.embeddings.LayerNorm.bias\n","\u001b[32m2024-04-21T16:03:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.token_type_embeddings_visual.weight from model.bert.embeddings.token_type_embeddings_visual.weight\n","\u001b[32m2024-04-21T16:03:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.position_embeddings_visual.weight from model.bert.embeddings.position_embeddings_visual.weight\n","\u001b[32m2024-04-21T16:03:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.projection.weight from model.bert.embeddings.projection.weight\n","\u001b[32m2024-04-21T16:03:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.projection.bias from model.bert.embeddings.projection.bias\n","\u001b[32m2024-04-21T16:03:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.self.query.weight from model.bert.encoder.layer.0.attention.self.query.weight\n","\u001b[32m2024-04-21T16:03:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.self.query.bias from model.bert.encoder.layer.0.attention.self.query.bias\n","\u001b[32m2024-04-21T16:03:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.self.key.weight from model.bert.encoder.layer.0.attention.self.key.weight\n","\u001b[32m2024-04-21T16:03:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.self.key.bias from model.bert.encoder.layer.0.attention.self.key.bias\n","\u001b[32m2024-04-21T16:03:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.self.value.weight from model.bert.encoder.layer.0.attention.self.value.weight\n","\u001b[32m2024-04-21T16:03:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.self.value.bias from model.bert.encoder.layer.0.attention.self.value.bias\n","\u001b[32m2024-04-21T16:03:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.output.dense.weight from model.bert.encoder.layer.0.attention.output.dense.weight\n","\u001b[32m2024-04-21T16:03:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.output.dense.bias from model.bert.encoder.layer.0.attention.output.dense.bias\n","\u001b[32m2024-04-21T16:03:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.output.LayerNorm.weight from model.bert.encoder.layer.0.attention.output.LayerNorm.weight\n","\u001b[32m2024-04-21T16:03:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.output.LayerNorm.bias from model.bert.encoder.layer.0.attention.output.LayerNorm.bias\n","\u001b[32m2024-04-21T16:03:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.intermediate.dense.weight from model.bert.encoder.layer.0.intermediate.dense.weight\n","\u001b[32m2024-04-21T16:03:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.intermediate.dense.bias from model.bert.encoder.layer.0.intermediate.dense.bias\n","\u001b[32m2024-04-21T16:03:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.output.dense.weight from model.bert.encoder.layer.0.output.dense.weight\n","\u001b[32m2024-04-21T16:03:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.output.dense.bias from model.bert.encoder.layer.0.output.dense.bias\n","\u001b[32m2024-04-21T16:03:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.output.LayerNorm.weight from model.bert.encoder.layer.0.output.LayerNorm.weight\n","\u001b[32m2024-04-21T16:03:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.output.LayerNorm.bias from model.bert.encoder.layer.0.output.LayerNorm.bias\n","\u001b[32m2024-04-21T16:03:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.self.query.weight from model.bert.encoder.layer.1.attention.self.query.weight\n","\u001b[32m2024-04-21T16:03:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.self.query.bias from model.bert.encoder.layer.1.attention.self.query.bias\n","\u001b[32m2024-04-21T16:03:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.self.key.weight from model.bert.encoder.layer.1.attention.self.key.weight\n","\u001b[32m2024-04-21T16:03:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.self.key.bias from model.bert.encoder.layer.1.attention.self.key.bias\n","\u001b[32m2024-04-21T16:03:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.self.value.weight from model.bert.encoder.layer.1.attention.self.value.weight\n","\u001b[32m2024-04-21T16:03:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.self.value.bias from model.bert.encoder.layer.1.attention.self.value.bias\n","\u001b[32m2024-04-21T16:03:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.output.dense.weight from model.bert.encoder.layer.1.attention.output.dense.weight\n","\u001b[32m2024-04-21T16:03:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.output.dense.bias from model.bert.encoder.layer.1.attention.output.dense.bias\n","\u001b[32m2024-04-21T16:03:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.output.LayerNorm.weight from model.bert.encoder.layer.1.attention.output.LayerNorm.weight\n","\u001b[32m2024-04-21T16:03:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.output.LayerNorm.bias from model.bert.encoder.layer.1.attention.output.LayerNorm.bias\n","\u001b[32m2024-04-21T16:03:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.intermediate.dense.weight from model.bert.encoder.layer.1.intermediate.dense.weight\n","\u001b[32m2024-04-21T16:03:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.intermediate.dense.bias from model.bert.encoder.layer.1.intermediate.dense.bias\n","\u001b[32m2024-04-21T16:03:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.output.dense.weight from model.bert.encoder.layer.1.output.dense.weight\n","\u001b[32m2024-04-21T16:03:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.output.dense.bias from model.bert.encoder.layer.1.output.dense.bias\n","\u001b[32m2024-04-21T16:03:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.output.LayerNorm.weight from model.bert.encoder.layer.1.output.LayerNorm.weight\n","\u001b[32m2024-04-21T16:03:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.output.LayerNorm.bias from model.bert.encoder.layer.1.output.LayerNorm.bias\n","\u001b[32m2024-04-21T16:03:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.self.query.weight from model.bert.encoder.layer.2.attention.self.query.weight\n","\u001b[32m2024-04-21T16:03:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.self.query.bias from model.bert.encoder.layer.2.attention.self.query.bias\n","\u001b[32m2024-04-21T16:03:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.self.key.weight from model.bert.encoder.layer.2.attention.self.key.weight\n","\u001b[32m2024-04-21T16:03:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.self.key.bias from model.bert.encoder.layer.2.attention.self.key.bias\n","\u001b[32m2024-04-21T16:03:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.self.value.weight from model.bert.encoder.layer.2.attention.self.value.weight\n","\u001b[32m2024-04-21T16:03:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.self.value.bias from model.bert.encoder.layer.2.attention.self.value.bias\n","\u001b[32m2024-04-21T16:03:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.output.dense.weight from model.bert.encoder.layer.2.attention.output.dense.weight\n","\u001b[32m2024-04-21T16:03:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.output.dense.bias from model.bert.encoder.layer.2.attention.output.dense.bias\n","\u001b[32m2024-04-21T16:03:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.output.LayerNorm.weight from model.bert.encoder.layer.2.attention.output.LayerNorm.weight\n","\u001b[32m2024-04-21T16:03:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.output.LayerNorm.bias from model.bert.encoder.layer.2.attention.output.LayerNorm.bias\n","\u001b[32m2024-04-21T16:03:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.intermediate.dense.weight from model.bert.encoder.layer.2.intermediate.dense.weight\n","\u001b[32m2024-04-21T16:03:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.intermediate.dense.bias from model.bert.encoder.layer.2.intermediate.dense.bias\n","\u001b[32m2024-04-21T16:03:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.output.dense.weight from model.bert.encoder.layer.2.output.dense.weight\n","\u001b[32m2024-04-21T16:03:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.output.dense.bias from model.bert.encoder.layer.2.output.dense.bias\n","\u001b[32m2024-04-21T16:03:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.output.LayerNorm.weight from model.bert.encoder.layer.2.output.LayerNorm.weight\n","\u001b[32m2024-04-21T16:03:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.output.LayerNorm.bias from model.bert.encoder.layer.2.output.LayerNorm.bias\n","\u001b[32m2024-04-21T16:03:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.self.query.weight from model.bert.encoder.layer.3.attention.self.query.weight\n","\u001b[32m2024-04-21T16:03:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.self.query.bias from model.bert.encoder.layer.3.attention.self.query.bias\n","\u001b[32m2024-04-21T16:03:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.self.key.weight from model.bert.encoder.layer.3.attention.self.key.weight\n","\u001b[32m2024-04-21T16:03:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.self.key.bias from model.bert.encoder.layer.3.attention.self.key.bias\n","\u001b[32m2024-04-21T16:03:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.self.value.weight from model.bert.encoder.layer.3.attention.self.value.weight\n","\u001b[32m2024-04-21T16:03:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.self.value.bias from model.bert.encoder.layer.3.attention.self.value.bias\n","\u001b[32m2024-04-21T16:03:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.output.dense.weight from model.bert.encoder.layer.3.attention.output.dense.weight\n","\u001b[32m2024-04-21T16:03:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.output.dense.bias from model.bert.encoder.layer.3.attention.output.dense.bias\n","\u001b[32m2024-04-21T16:03:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.output.LayerNorm.weight from model.bert.encoder.layer.3.attention.output.LayerNorm.weight\n","\u001b[32m2024-04-21T16:03:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.output.LayerNorm.bias from model.bert.encoder.layer.3.attention.output.LayerNorm.bias\n","\u001b[32m2024-04-21T16:03:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.intermediate.dense.weight from model.bert.encoder.layer.3.intermediate.dense.weight\n","\u001b[32m2024-04-21T16:03:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.intermediate.dense.bias from model.bert.encoder.layer.3.intermediate.dense.bias\n","\u001b[32m2024-04-21T16:03:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.output.dense.weight from model.bert.encoder.layer.3.output.dense.weight\n","\u001b[32m2024-04-21T16:03:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.output.dense.bias from model.bert.encoder.layer.3.output.dense.bias\n","\u001b[32m2024-04-21T16:03:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.output.LayerNorm.weight from model.bert.encoder.layer.3.output.LayerNorm.weight\n","\u001b[32m2024-04-21T16:03:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.output.LayerNorm.bias from model.bert.encoder.layer.3.output.LayerNorm.bias\n","\u001b[32m2024-04-21T16:03:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.self.query.weight from model.bert.encoder.layer.4.attention.self.query.weight\n","\u001b[32m2024-04-21T16:03:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.self.query.bias from model.bert.encoder.layer.4.attention.self.query.bias\n","\u001b[32m2024-04-21T16:03:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.self.key.weight from model.bert.encoder.layer.4.attention.self.key.weight\n","\u001b[32m2024-04-21T16:03:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.self.key.bias from model.bert.encoder.layer.4.attention.self.key.bias\n","\u001b[32m2024-04-21T16:03:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.self.value.weight from model.bert.encoder.layer.4.attention.self.value.weight\n","\u001b[32m2024-04-21T16:03:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.self.value.bias from model.bert.encoder.layer.4.attention.self.value.bias\n","\u001b[32m2024-04-21T16:03:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.output.dense.weight from model.bert.encoder.layer.4.attention.output.dense.weight\n","\u001b[32m2024-04-21T16:03:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.output.dense.bias from model.bert.encoder.layer.4.attention.output.dense.bias\n","\u001b[32m2024-04-21T16:03:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.output.LayerNorm.weight from model.bert.encoder.layer.4.attention.output.LayerNorm.weight\n","\u001b[32m2024-04-21T16:03:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.output.LayerNorm.bias from model.bert.encoder.layer.4.attention.output.LayerNorm.bias\n","\u001b[32m2024-04-21T16:03:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.intermediate.dense.weight from model.bert.encoder.layer.4.intermediate.dense.weight\n","\u001b[32m2024-04-21T16:03:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.intermediate.dense.bias from model.bert.encoder.layer.4.intermediate.dense.bias\n","\u001b[32m2024-04-21T16:03:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.output.dense.weight from model.bert.encoder.layer.4.output.dense.weight\n","\u001b[32m2024-04-21T16:03:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.output.dense.bias from model.bert.encoder.layer.4.output.dense.bias\n","\u001b[32m2024-04-21T16:03:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.output.LayerNorm.weight from model.bert.encoder.layer.4.output.LayerNorm.weight\n","\u001b[32m2024-04-21T16:03:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.output.LayerNorm.bias from model.bert.encoder.layer.4.output.LayerNorm.bias\n","\u001b[32m2024-04-21T16:03:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.self.query.weight from model.bert.encoder.layer.5.attention.self.query.weight\n","\u001b[32m2024-04-21T16:03:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.self.query.bias from model.bert.encoder.layer.5.attention.self.query.bias\n","\u001b[32m2024-04-21T16:03:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.self.key.weight from model.bert.encoder.layer.5.attention.self.key.weight\n","\u001b[32m2024-04-21T16:03:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.self.key.bias from model.bert.encoder.layer.5.attention.self.key.bias\n","\u001b[32m2024-04-21T16:03:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.self.value.weight from model.bert.encoder.layer.5.attention.self.value.weight\n","\u001b[32m2024-04-21T16:03:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.self.value.bias from model.bert.encoder.layer.5.attention.self.value.bias\n","\u001b[32m2024-04-21T16:03:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.output.dense.weight from model.bert.encoder.layer.5.attention.output.dense.weight\n","\u001b[32m2024-04-21T16:03:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.output.dense.bias from model.bert.encoder.layer.5.attention.output.dense.bias\n","\u001b[32m2024-04-21T16:03:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.output.LayerNorm.weight from model.bert.encoder.layer.5.attention.output.LayerNorm.weight\n","\u001b[32m2024-04-21T16:03:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.output.LayerNorm.bias from model.bert.encoder.layer.5.attention.output.LayerNorm.bias\n","\u001b[32m2024-04-21T16:03:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.intermediate.dense.weight from model.bert.encoder.layer.5.intermediate.dense.weight\n","\u001b[32m2024-04-21T16:03:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.intermediate.dense.bias from model.bert.encoder.layer.5.intermediate.dense.bias\n","\u001b[32m2024-04-21T16:03:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.output.dense.weight from model.bert.encoder.layer.5.output.dense.weight\n","\u001b[32m2024-04-21T16:03:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.output.dense.bias from model.bert.encoder.layer.5.output.dense.bias\n","\u001b[32m2024-04-21T16:03:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.output.LayerNorm.weight from model.bert.encoder.layer.5.output.LayerNorm.weight\n","\u001b[32m2024-04-21T16:03:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.output.LayerNorm.bias from model.bert.encoder.layer.5.output.LayerNorm.bias\n","\u001b[32m2024-04-21T16:03:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.self.query.weight from model.bert.encoder.layer.6.attention.self.query.weight\n","\u001b[32m2024-04-21T16:03:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.self.query.bias from model.bert.encoder.layer.6.attention.self.query.bias\n","\u001b[32m2024-04-21T16:03:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.self.key.weight from model.bert.encoder.layer.6.attention.self.key.weight\n","\u001b[32m2024-04-21T16:03:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.self.key.bias from model.bert.encoder.layer.6.attention.self.key.bias\n","\u001b[32m2024-04-21T16:03:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.self.value.weight from model.bert.encoder.layer.6.attention.self.value.weight\n","\u001b[32m2024-04-21T16:03:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.self.value.bias from model.bert.encoder.layer.6.attention.self.value.bias\n","\u001b[32m2024-04-21T16:03:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.output.dense.weight from model.bert.encoder.layer.6.attention.output.dense.weight\n","\u001b[32m2024-04-21T16:03:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.output.dense.bias from model.bert.encoder.layer.6.attention.output.dense.bias\n","\u001b[32m2024-04-21T16:03:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.output.LayerNorm.weight from model.bert.encoder.layer.6.attention.output.LayerNorm.weight\n","\u001b[32m2024-04-21T16:03:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.output.LayerNorm.bias from model.bert.encoder.layer.6.attention.output.LayerNorm.bias\n","\u001b[32m2024-04-21T16:03:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.intermediate.dense.weight from model.bert.encoder.layer.6.intermediate.dense.weight\n","\u001b[32m2024-04-21T16:03:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.intermediate.dense.bias from model.bert.encoder.layer.6.intermediate.dense.bias\n","\u001b[32m2024-04-21T16:03:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.output.dense.weight from model.bert.encoder.layer.6.output.dense.weight\n","\u001b[32m2024-04-21T16:03:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.output.dense.bias from model.bert.encoder.layer.6.output.dense.bias\n","\u001b[32m2024-04-21T16:03:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.output.LayerNorm.weight from model.bert.encoder.layer.6.output.LayerNorm.weight\n","\u001b[32m2024-04-21T16:03:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.output.LayerNorm.bias from model.bert.encoder.layer.6.output.LayerNorm.bias\n","\u001b[32m2024-04-21T16:03:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.self.query.weight from model.bert.encoder.layer.7.attention.self.query.weight\n","\u001b[32m2024-04-21T16:03:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.self.query.bias from model.bert.encoder.layer.7.attention.self.query.bias\n","\u001b[32m2024-04-21T16:03:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.self.key.weight from model.bert.encoder.layer.7.attention.self.key.weight\n","\u001b[32m2024-04-21T16:03:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.self.key.bias from model.bert.encoder.layer.7.attention.self.key.bias\n","\u001b[32m2024-04-21T16:03:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.self.value.weight from model.bert.encoder.layer.7.attention.self.value.weight\n","\u001b[32m2024-04-21T16:03:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.self.value.bias from model.bert.encoder.layer.7.attention.self.value.bias\n","\u001b[32m2024-04-21T16:03:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.output.dense.weight from model.bert.encoder.layer.7.attention.output.dense.weight\n","\u001b[32m2024-04-21T16:03:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.output.dense.bias from model.bert.encoder.layer.7.attention.output.dense.bias\n","\u001b[32m2024-04-21T16:03:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.output.LayerNorm.weight from model.bert.encoder.layer.7.attention.output.LayerNorm.weight\n","\u001b[32m2024-04-21T16:03:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.output.LayerNorm.bias from model.bert.encoder.layer.7.attention.output.LayerNorm.bias\n","\u001b[32m2024-04-21T16:03:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.intermediate.dense.weight from model.bert.encoder.layer.7.intermediate.dense.weight\n","\u001b[32m2024-04-21T16:03:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.intermediate.dense.bias from model.bert.encoder.layer.7.intermediate.dense.bias\n","\u001b[32m2024-04-21T16:03:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.output.dense.weight from model.bert.encoder.layer.7.output.dense.weight\n","\u001b[32m2024-04-21T16:03:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.output.dense.bias from model.bert.encoder.layer.7.output.dense.bias\n","\u001b[32m2024-04-21T16:03:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.output.LayerNorm.weight from model.bert.encoder.layer.7.output.LayerNorm.weight\n","\u001b[32m2024-04-21T16:03:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.output.LayerNorm.bias from model.bert.encoder.layer.7.output.LayerNorm.bias\n","\u001b[32m2024-04-21T16:03:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.self.query.weight from model.bert.encoder.layer.8.attention.self.query.weight\n","\u001b[32m2024-04-21T16:03:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.self.query.bias from model.bert.encoder.layer.8.attention.self.query.bias\n","\u001b[32m2024-04-21T16:03:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.self.key.weight from model.bert.encoder.layer.8.attention.self.key.weight\n","\u001b[32m2024-04-21T16:03:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.self.key.bias from model.bert.encoder.layer.8.attention.self.key.bias\n","\u001b[32m2024-04-21T16:03:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.self.value.weight from model.bert.encoder.layer.8.attention.self.value.weight\n","\u001b[32m2024-04-21T16:03:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.self.value.bias from model.bert.encoder.layer.8.attention.self.value.bias\n","\u001b[32m2024-04-21T16:03:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.output.dense.weight from model.bert.encoder.layer.8.attention.output.dense.weight\n","\u001b[32m2024-04-21T16:03:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.output.dense.bias from model.bert.encoder.layer.8.attention.output.dense.bias\n","\u001b[32m2024-04-21T16:03:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.output.LayerNorm.weight from model.bert.encoder.layer.8.attention.output.LayerNorm.weight\n","\u001b[32m2024-04-21T16:03:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.output.LayerNorm.bias from model.bert.encoder.layer.8.attention.output.LayerNorm.bias\n","\u001b[32m2024-04-21T16:03:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.intermediate.dense.weight from model.bert.encoder.layer.8.intermediate.dense.weight\n","\u001b[32m2024-04-21T16:03:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.intermediate.dense.bias from model.bert.encoder.layer.8.intermediate.dense.bias\n","\u001b[32m2024-04-21T16:03:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.output.dense.weight from model.bert.encoder.layer.8.output.dense.weight\n","\u001b[32m2024-04-21T16:03:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.output.dense.bias from model.bert.encoder.layer.8.output.dense.bias\n","\u001b[32m2024-04-21T16:03:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.output.LayerNorm.weight from model.bert.encoder.layer.8.output.LayerNorm.weight\n","\u001b[32m2024-04-21T16:03:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.output.LayerNorm.bias from model.bert.encoder.layer.8.output.LayerNorm.bias\n","\u001b[32m2024-04-21T16:03:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.self.query.weight from model.bert.encoder.layer.9.attention.self.query.weight\n","\u001b[32m2024-04-21T16:03:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.self.query.bias from model.bert.encoder.layer.9.attention.self.query.bias\n","\u001b[32m2024-04-21T16:03:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.self.key.weight from model.bert.encoder.layer.9.attention.self.key.weight\n","\u001b[32m2024-04-21T16:03:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.self.key.bias from model.bert.encoder.layer.9.attention.self.key.bias\n","\u001b[32m2024-04-21T16:03:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.self.value.weight from model.bert.encoder.layer.9.attention.self.value.weight\n","\u001b[32m2024-04-21T16:03:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.self.value.bias from model.bert.encoder.layer.9.attention.self.value.bias\n","\u001b[32m2024-04-21T16:03:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.output.dense.weight from model.bert.encoder.layer.9.attention.output.dense.weight\n","\u001b[32m2024-04-21T16:03:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.output.dense.bias from model.bert.encoder.layer.9.attention.output.dense.bias\n","\u001b[32m2024-04-21T16:03:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.output.LayerNorm.weight from model.bert.encoder.layer.9.attention.output.LayerNorm.weight\n","\u001b[32m2024-04-21T16:03:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.output.LayerNorm.bias from model.bert.encoder.layer.9.attention.output.LayerNorm.bias\n","\u001b[32m2024-04-21T16:03:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.intermediate.dense.weight from model.bert.encoder.layer.9.intermediate.dense.weight\n","\u001b[32m2024-04-21T16:03:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.intermediate.dense.bias from model.bert.encoder.layer.9.intermediate.dense.bias\n","\u001b[32m2024-04-21T16:03:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.output.dense.weight from model.bert.encoder.layer.9.output.dense.weight\n","\u001b[32m2024-04-21T16:03:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.output.dense.bias from model.bert.encoder.layer.9.output.dense.bias\n","\u001b[32m2024-04-21T16:03:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.output.LayerNorm.weight from model.bert.encoder.layer.9.output.LayerNorm.weight\n","\u001b[32m2024-04-21T16:03:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.output.LayerNorm.bias from model.bert.encoder.layer.9.output.LayerNorm.bias\n","\u001b[32m2024-04-21T16:03:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.self.query.weight from model.bert.encoder.layer.10.attention.self.query.weight\n","\u001b[32m2024-04-21T16:03:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.self.query.bias from model.bert.encoder.layer.10.attention.self.query.bias\n","\u001b[32m2024-04-21T16:03:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.self.key.weight from model.bert.encoder.layer.10.attention.self.key.weight\n","\u001b[32m2024-04-21T16:03:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.self.key.bias from model.bert.encoder.layer.10.attention.self.key.bias\n","\u001b[32m2024-04-21T16:03:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.self.value.weight from model.bert.encoder.layer.10.attention.self.value.weight\n","\u001b[32m2024-04-21T16:03:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.self.value.bias from model.bert.encoder.layer.10.attention.self.value.bias\n","\u001b[32m2024-04-21T16:03:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.output.dense.weight from model.bert.encoder.layer.10.attention.output.dense.weight\n","\u001b[32m2024-04-21T16:03:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.output.dense.bias from model.bert.encoder.layer.10.attention.output.dense.bias\n","\u001b[32m2024-04-21T16:03:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.output.LayerNorm.weight from model.bert.encoder.layer.10.attention.output.LayerNorm.weight\n","\u001b[32m2024-04-21T16:03:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.output.LayerNorm.bias from model.bert.encoder.layer.10.attention.output.LayerNorm.bias\n","\u001b[32m2024-04-21T16:03:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.intermediate.dense.weight from model.bert.encoder.layer.10.intermediate.dense.weight\n","\u001b[32m2024-04-21T16:03:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.intermediate.dense.bias from model.bert.encoder.layer.10.intermediate.dense.bias\n","\u001b[32m2024-04-21T16:03:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.output.dense.weight from model.bert.encoder.layer.10.output.dense.weight\n","\u001b[32m2024-04-21T16:03:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.output.dense.bias from model.bert.encoder.layer.10.output.dense.bias\n","\u001b[32m2024-04-21T16:03:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.output.LayerNorm.weight from model.bert.encoder.layer.10.output.LayerNorm.weight\n","\u001b[32m2024-04-21T16:03:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.output.LayerNorm.bias from model.bert.encoder.layer.10.output.LayerNorm.bias\n","\u001b[32m2024-04-21T16:03:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.self.query.weight from model.bert.encoder.layer.11.attention.self.query.weight\n","\u001b[32m2024-04-21T16:03:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.self.query.bias from model.bert.encoder.layer.11.attention.self.query.bias\n","\u001b[32m2024-04-21T16:03:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.self.key.weight from model.bert.encoder.layer.11.attention.self.key.weight\n","\u001b[32m2024-04-21T16:03:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.self.key.bias from model.bert.encoder.layer.11.attention.self.key.bias\n","\u001b[32m2024-04-21T16:03:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.self.value.weight from model.bert.encoder.layer.11.attention.self.value.weight\n","\u001b[32m2024-04-21T16:03:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.self.value.bias from model.bert.encoder.layer.11.attention.self.value.bias\n","\u001b[32m2024-04-21T16:03:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.output.dense.weight from model.bert.encoder.layer.11.attention.output.dense.weight\n","\u001b[32m2024-04-21T16:03:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.output.dense.bias from model.bert.encoder.layer.11.attention.output.dense.bias\n","\u001b[32m2024-04-21T16:03:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.output.LayerNorm.weight from model.bert.encoder.layer.11.attention.output.LayerNorm.weight\n","\u001b[32m2024-04-21T16:03:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.output.LayerNorm.bias from model.bert.encoder.layer.11.attention.output.LayerNorm.bias\n","\u001b[32m2024-04-21T16:03:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.intermediate.dense.weight from model.bert.encoder.layer.11.intermediate.dense.weight\n","\u001b[32m2024-04-21T16:03:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.intermediate.dense.bias from model.bert.encoder.layer.11.intermediate.dense.bias\n","\u001b[32m2024-04-21T16:03:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.output.dense.weight from model.bert.encoder.layer.11.output.dense.weight\n","\u001b[32m2024-04-21T16:03:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.output.dense.bias from model.bert.encoder.layer.11.output.dense.bias\n","\u001b[32m2024-04-21T16:03:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.output.LayerNorm.weight from model.bert.encoder.layer.11.output.LayerNorm.weight\n","\u001b[32m2024-04-21T16:03:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.output.LayerNorm.bias from model.bert.encoder.layer.11.output.LayerNorm.bias\n","\u001b[32m2024-04-21T16:03:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.pooler.dense.weight from model.bert.pooler.dense.weight\n","\u001b[32m2024-04-21T16:03:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.pooler.dense.bias from model.bert.pooler.dense.bias\n","\u001b[32m2024-04-21T16:03:56 | mmf.utils.checkpoint: \u001b[0mPretrained model loaded\n","\u001b[32m2024-04-21T16:03:56 | mmf.utils.checkpoint: \u001b[0mCheckpoint loaded.\n","\u001b[32m2024-04-21T16:03:56 | mmf.utils.checkpoint: \u001b[0mCurrent num updates: 0\n","\u001b[32m2024-04-21T16:03:56 | mmf.utils.checkpoint: \u001b[0mCurrent iteration: 0\n","\u001b[32m2024-04-21T16:03:56 | mmf.utils.checkpoint: \u001b[0mCurrent epoch: 0\n","\u001b[32m2024-04-21T16:03:56 | mmf.trainers.mmf_trainer: \u001b[0m===== Model =====\n","\u001b[32m2024-04-21T16:03:56 | mmf.trainers.mmf_trainer: \u001b[0mVisualBERT(\n","  (model): VisualBERTForClassification(\n","    (bert): VisualBERTBase(\n","      (embeddings): BertVisioLinguisticEmbeddings(\n","        (word_embeddings): Embedding(30522, 768, padding_idx=0)\n","        (position_embeddings): Embedding(512, 768)\n","        (token_type_embeddings): Embedding(2, 768)\n","        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","        (token_type_embeddings_visual): Embedding(2, 768)\n","        (position_embeddings_visual): Embedding(512, 768)\n","        (projection): Linear(in_features=2048, out_features=768, bias=True)\n","      )\n","      (encoder): BertEncoderJit(\n","        (layer): ModuleList(\n","          (0-11): 12 x BertLayerJit(\n","            (attention): BertAttentionJit(\n","              (self): BertSelfAttentionJit(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","              (intermediate_act_fn): GELUActivation()\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","        )\n","      )\n","      (pooler): BertPooler(\n","        (dense): Linear(in_features=768, out_features=768, bias=True)\n","        (activation): Tanh()\n","      )\n","    )\n","    (dropout): Dropout(p=0.1, inplace=False)\n","    (classifier): Sequential(\n","      (0): BertPredictionHeadTransform(\n","        (dense): Linear(in_features=768, out_features=768, bias=True)\n","        (transform_act_fn): GELUActivation()\n","        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      )\n","      (1): Linear(in_features=768, out_features=2, bias=True)\n","    )\n","  )\n","  (losses): Losses(\n","    (losses): ModuleList(\n","      (0): MMFLoss(\n","        (loss_criterion): CrossEntropyLoss(\n","          (loss_fn): CrossEntropyLoss()\n","        )\n","      )\n","    )\n","  )\n",")\n","\u001b[32m2024-04-21T16:03:56 | mmf.utils.general: \u001b[0mTotal Parameters: 112044290. Trained Parameters: 112044290\n","\u001b[32m2024-04-21T16:03:56 | mmf.trainers.core.training_loop: \u001b[0mStarting training...\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2024-04-21T16:03:56 | py.warnings: \u001b[0m/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n","  self.pid = os.fork()\n","\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2024-04-21T16:03:56 | py.warnings: \u001b[0m/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n","  self.pid = os.fork()\n","\n","\u001b[32m2024-04-21T16:08:00 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n","\u001b[32m2024-04-21T16:08:00 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2024-04-21T16:08:02 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2024-04-21T16:08:05 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2024-04-21T16:08:05 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 200/1500, train/hateful_memes/cross_entropy: 0.6300, train/hateful_memes/cross_entropy/avg: 0.6300, train/total_loss: 0.6300, train/total_loss/avg: 0.6300, max mem: 14322.0, experiment: run, epoch: 2, num_updates: 200, iterations: 200, max_updates: 1500, lr: 0.00001, ups: 0.81, time: 04m 08s 621ms, time_since_start: 04m 17s 749ms, eta: 28m 08s 764ms\n","\u001b[32m2024-04-21T16:08:05 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n","\u001b[32m2024-04-21T16:08:05 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n","\u001b[32m2024-04-21T16:08:10 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished evaluation inference. Loaded 9\n","\u001b[32m2024-04-21T16:08:10 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n","\u001b[32m2024-04-21T16:08:10 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2024-04-21T16:08:13 | mmf.utils.checkpoint: \u001b[0mSaving best checkpoint\n","\u001b[32m2024-04-21T16:08:17 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2024-04-21T16:08:22 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2024-04-21T16:08:22 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 200/1500, val/hateful_memes/cross_entropy: 0.6564, val/total_loss: 0.6564, val/hateful_memes/accuracy: 0.6241, val/hateful_memes/binary_f1: 0.1506, val/hateful_memes/roc_auc: 0.5858, num_updates: 200, epoch: 2, iterations: 200, max_updates: 1500, val_time: 17s 134ms, best_update: 200, best_iteration: 200, best_val/hateful_memes/roc_auc: 0.585809\n","\u001b[32m2024-04-21T16:12:25 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n","\u001b[32m2024-04-21T16:12:25 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2024-04-21T16:12:27 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2024-04-21T16:12:30 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2024-04-21T16:12:30 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 400/1500, train/hateful_memes/cross_entropy: 0.4974, train/hateful_memes/cross_entropy/avg: 0.5637, train/total_loss: 0.4974, train/total_loss/avg: 0.5637, max mem: 14371.0, experiment: run, epoch: 4, num_updates: 400, iterations: 400, max_updates: 1500, lr: 0.00001, ups: 0.81, time: 04m 07s 857ms, time_since_start: 08m 42s 745ms, eta: 23m 44s 558ms\n","\u001b[32m2024-04-21T16:12:30 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n","\u001b[32m2024-04-21T16:12:30 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2024-04-21T16:12:30 | py.warnings: \u001b[0m/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n","  self.pid = os.fork()\n","\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2024-04-21T16:12:30 | py.warnings: \u001b[0m/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n","  self.pid = os.fork()\n","\n","\u001b[32m2024-04-21T16:12:35 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished evaluation inference. Loaded 9\n","\u001b[32m2024-04-21T16:12:35 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n","\u001b[32m2024-04-21T16:12:35 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2024-04-21T16:12:37 | mmf.utils.checkpoint: \u001b[0mSaving best checkpoint\n","\u001b[32m2024-04-21T16:12:43 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2024-04-21T16:12:48 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2024-04-21T16:12:48 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 400/1500, val/hateful_memes/cross_entropy: 0.6681, val/total_loss: 0.6681, val/hateful_memes/accuracy: 0.6500, val/hateful_memes/binary_f1: 0.4038, val/hateful_memes/roc_auc: 0.6685, num_updates: 400, epoch: 4, iterations: 400, max_updates: 1500, val_time: 17s 638ms, best_update: 400, best_iteration: 400, best_val/hateful_memes/roc_auc: 0.668456\n","\u001b[32m2024-04-21T16:16:51 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n","\u001b[32m2024-04-21T16:16:51 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2024-04-21T16:16:53 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2024-04-21T16:16:55 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2024-04-21T16:16:55 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 600/1500, train/hateful_memes/cross_entropy: 0.4974, train/hateful_memes/cross_entropy/avg: 0.4854, train/total_loss: 0.4974, train/total_loss/avg: 0.4854, max mem: 14371.0, experiment: run, epoch: 5, num_updates: 600, iterations: 600, max_updates: 1500, lr: 0.00002, ups: 0.81, time: 04m 07s 794ms, time_since_start: 13m 08s 181ms, eta: 19m 25s 254ms\n","\u001b[32m2024-04-21T16:16:55 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n","\u001b[32m2024-04-21T16:16:56 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2024-04-21T16:16:56 | py.warnings: \u001b[0m/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n","  self.pid = os.fork()\n","\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2024-04-21T16:16:56 | py.warnings: \u001b[0m/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n","  self.pid = os.fork()\n","\n","\u001b[32m2024-04-21T16:17:00 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished evaluation inference. Loaded 9\n","\u001b[32m2024-04-21T16:17:00 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n","\u001b[32m2024-04-21T16:17:00 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2024-04-21T16:17:02 | mmf.utils.checkpoint: \u001b[0mSaving best checkpoint\n","\u001b[32m2024-04-21T16:17:05 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2024-04-21T16:17:10 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2024-04-21T16:17:10 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 600/1500, val/hateful_memes/cross_entropy: 0.7531, val/total_loss: 0.7531, val/hateful_memes/accuracy: 0.6759, val/hateful_memes/binary_f1: 0.5152, val/hateful_memes/roc_auc: 0.7097, num_updates: 600, epoch: 5, iterations: 600, max_updates: 1500, val_time: 14s 031ms, best_update: 600, best_iteration: 600, best_val/hateful_memes/roc_auc: 0.709706\n","\u001b[32m2024-04-21T16:21:12 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n","\u001b[32m2024-04-21T16:21:12 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2024-04-21T16:21:15 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2024-04-21T16:21:20 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2024-04-21T16:21:20 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 800/1500, train/hateful_memes/cross_entropy: 0.3289, train/hateful_memes/cross_entropy/avg: 0.3842, train/total_loss: 0.3289, train/total_loss/avg: 0.3842, max mem: 14371.0, experiment: run, epoch: 7, num_updates: 800, iterations: 800, max_updates: 1500, lr: 0.00002, ups: 0.80, time: 04m 10s 591ms, time_since_start: 17m 32s 805ms, eta: 15m 16s 538ms\n","\u001b[32m2024-04-21T16:21:20 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n","\u001b[32m2024-04-21T16:21:20 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2024-04-21T16:21:20 | py.warnings: \u001b[0m/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n","  self.pid = os.fork()\n","\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2024-04-21T16:21:20 | py.warnings: \u001b[0m/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n","  self.pid = os.fork()\n","\n","\u001b[32m2024-04-21T16:21:24 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished evaluation inference. Loaded 9\n","\u001b[32m2024-04-21T16:21:24 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n","\u001b[32m2024-04-21T16:21:25 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2024-04-21T16:21:27 | mmf.utils.checkpoint: \u001b[0mSaving best checkpoint\n","\u001b[32m2024-04-21T16:21:33 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2024-04-21T16:21:37 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2024-04-21T16:21:37 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 800/1500, val/hateful_memes/cross_entropy: 1.0136, val/total_loss: 1.0136, val/hateful_memes/accuracy: 0.6852, val/hateful_memes/binary_f1: 0.4516, val/hateful_memes/roc_auc: 0.7218, num_updates: 800, epoch: 7, iterations: 800, max_updates: 1500, val_time: 17s 036ms, best_update: 800, best_iteration: 800, best_val/hateful_memes/roc_auc: 0.721809\n","\u001b[32m2024-04-21T16:25:40 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n","\u001b[32m2024-04-21T16:25:40 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2024-04-21T16:25:43 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2024-04-21T16:25:48 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2024-04-21T16:25:48 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1000/1500, train/hateful_memes/cross_entropy: 0.3289, train/hateful_memes/cross_entropy/avg: 0.3262, train/total_loss: 0.3289, train/total_loss/avg: 0.3262, max mem: 14371.0, experiment: run, epoch: 8, num_updates: 1000, iterations: 1000, max_updates: 1500, lr: 0.00003, ups: 0.80, time: 04m 10s 627ms, time_since_start: 22m 472ms, eta: 10m 54s 764ms\n","\u001b[32m2024-04-21T16:25:48 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n","\u001b[32m2024-04-21T16:25:48 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2024-04-21T16:25:48 | py.warnings: \u001b[0m/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n","  self.pid = os.fork()\n","\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2024-04-21T16:25:48 | py.warnings: \u001b[0m/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n","  self.pid = os.fork()\n","\n","\u001b[32m2024-04-21T16:25:52 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished evaluation inference. Loaded 9\n","\u001b[32m2024-04-21T16:25:52 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n","\u001b[32m2024-04-21T16:25:52 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2024-04-21T16:25:55 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2024-04-21T16:25:59 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2024-04-21T16:25:59 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1000/1500, val/hateful_memes/cross_entropy: 1.3550, val/total_loss: 1.3550, val/hateful_memes/accuracy: 0.6704, val/hateful_memes/binary_f1: 0.3776, val/hateful_memes/roc_auc: 0.7106, num_updates: 1000, epoch: 8, iterations: 1000, max_updates: 1500, val_time: 11s 382ms, best_update: 800, best_iteration: 800, best_val/hateful_memes/roc_auc: 0.721809\n","\u001b[32m2024-04-21T16:30:02 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n","\u001b[32m2024-04-21T16:30:02 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2024-04-21T16:30:04 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2024-04-21T16:30:08 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2024-04-21T16:30:08 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1200/1500, train/hateful_memes/cross_entropy: 0.0944, train/hateful_memes/cross_entropy/avg: 0.2834, train/total_loss: 0.0944, train/total_loss/avg: 0.2834, max mem: 14371.0, experiment: run, epoch: 10, num_updates: 1200, iterations: 1200, max_updates: 1500, lr: 0.00003, ups: 0.81, time: 04m 08s 698ms, time_since_start: 26m 20s 555ms, eta: 06m 29s 834ms\n","\u001b[32m2024-04-21T16:30:08 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n","\u001b[32m2024-04-21T16:30:08 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2024-04-21T16:30:08 | py.warnings: \u001b[0m/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n","  self.pid = os.fork()\n","\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2024-04-21T16:30:08 | py.warnings: \u001b[0m/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n","  self.pid = os.fork()\n","\n","\u001b[32m2024-04-21T16:30:12 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished evaluation inference. Loaded 9\n","\u001b[32m2024-04-21T16:30:12 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n","\u001b[32m2024-04-21T16:30:12 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2024-04-21T16:30:15 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2024-04-21T16:30:21 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2024-04-21T16:30:21 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1200/1500, val/hateful_memes/cross_entropy: 1.2803, val/total_loss: 1.2803, val/hateful_memes/accuracy: 0.6944, val/hateful_memes/binary_f1: 0.5075, val/hateful_memes/roc_auc: 0.7089, num_updates: 1200, epoch: 10, iterations: 1200, max_updates: 1500, val_time: 13s 460ms, best_update: 800, best_iteration: 800, best_val/hateful_memes/roc_auc: 0.721809\n","\u001b[32m2024-04-21T16:34:24 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n","\u001b[32m2024-04-21T16:34:24 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2024-04-21T16:34:26 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2024-04-21T16:34:29 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2024-04-21T16:34:29 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1400/1500, train/hateful_memes/cross_entropy: 0.1277, train/hateful_memes/cross_entropy/avg: 0.2612, train/total_loss: 0.1277, train/total_loss/avg: 0.2612, max mem: 14371.0, experiment: run, epoch: 11, num_updates: 1400, iterations: 1400, max_updates: 1500, lr: 0.00003, ups: 0.81, time: 04m 08s 122ms, time_since_start: 30m 42s 140ms, eta: 02m 09s 643ms\n","\u001b[32m2024-04-21T16:34:29 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n","\u001b[32m2024-04-21T16:34:29 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2024-04-21T16:34:29 | py.warnings: \u001b[0m/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n","  self.pid = os.fork()\n","\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2024-04-21T16:34:29 | py.warnings: \u001b[0m/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n","  self.pid = os.fork()\n","\n","\u001b[32m2024-04-21T16:34:34 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished evaluation inference. Loaded 9\n","\u001b[32m2024-04-21T16:34:34 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n","\u001b[32m2024-04-21T16:34:34 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2024-04-21T16:34:37 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2024-04-21T16:34:42 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2024-04-21T16:34:42 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1400/1500, val/hateful_memes/cross_entropy: 1.5146, val/total_loss: 1.5146, val/hateful_memes/accuracy: 0.6833, val/hateful_memes/binary_f1: 0.4606, val/hateful_memes/roc_auc: 0.7076, num_updates: 1400, epoch: 11, iterations: 1400, max_updates: 1500, val_time: 12s 546ms, best_update: 800, best_iteration: 800, best_val/hateful_memes/roc_auc: 0.721809\n","\u001b[32m2024-04-21T16:36:44 | mmf.trainers.core.training_loop: \u001b[0mStepping into final validation check\n","\u001b[32m2024-04-21T16:36:44 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2024-04-21T16:36:44 | py.warnings: \u001b[0m/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n","  self.pid = os.fork()\n","\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2024-04-21T16:36:44 | py.warnings: \u001b[0m/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n","  self.pid = os.fork()\n","\n","\u001b[32m2024-04-21T16:36:48 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished evaluation inference. Loaded 9\n","\u001b[32m2024-04-21T16:36:48 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n","\u001b[32m2024-04-21T16:36:49 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1500/1500, val/hateful_memes/cross_entropy: 1.5592, val/total_loss: 1.5592, val/hateful_memes/accuracy: 0.6944, val/hateful_memes/binary_f1: 0.5553, val/hateful_memes/roc_auc: 0.7212, num_updates: 1500, epoch: 12, iterations: 1500, max_updates: 1500, val_time: 02m 19s 185ms, best_update: 800, best_iteration: 800, best_val/hateful_memes/roc_auc: 0.721809\n","\u001b[32m2024-04-21T16:36:49 | mmf.utils.checkpoint: \u001b[0mRestoring checkpoint\n","\u001b[32m2024-04-21T16:36:49 | mmf.utils.checkpoint: \u001b[0mLoading checkpoint\n","\u001b[32m2024-04-21T16:36:50 | mmf.utils.checkpoint: \u001b[0mCheckpoint loaded.\n","\u001b[32m2024-04-21T16:36:50 | mmf.utils.checkpoint: \u001b[0mCurrent num updates: 800\n","\u001b[32m2024-04-21T16:36:50 | mmf.utils.checkpoint: \u001b[0mCurrent iteration: 800\n","\u001b[32m2024-04-21T16:36:50 | mmf.utils.checkpoint: \u001b[0mCurrent epoch: 7\n","\u001b[32m2024-04-21T16:36:50 | mmf.trainers.mmf_trainer: \u001b[0mStarting inference on val set\n","\u001b[32m2024-04-21T16:36:50 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n","  0% 0/9 [00:00<?, ?it/s]\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2024-04-21T16:36:50 | py.warnings: \u001b[0m/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n","  self.pid = os.fork()\n","\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2024-04-21T16:36:50 | py.warnings: \u001b[0m/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n","  self.pid = os.fork()\n","\n","100% 9/9 [00:04<00:00,  2.08it/s]\n","\u001b[32m2024-04-21T16:36:55 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished evaluation inference. Loaded 9\n","\u001b[32m2024-04-21T16:36:55 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n","\u001b[32m2024-04-21T16:36:55 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 800/1500, val/hateful_memes/cross_entropy: 1.0136, val/total_loss: 1.0136, val/hateful_memes/accuracy: 0.6852, val/hateful_memes/binary_f1: 0.4516, val/hateful_memes/roc_auc: 0.7218\n","\u001b[32m2024-04-21T16:36:55 | mmf.trainers.callbacks.logistics: \u001b[0mFinished run in 33m 07s 520ms\n"]}]},{"cell_type":"markdown","source":["### Dataset: Appended with images from dev"],"metadata":{"id":"Og1oL2ALX00e"}},{"cell_type":"code","source":["!mmf_run config=\"./configs/defaults.yaml\" \\\n","  model=visual_bert \\\n","  dataset=hateful_memes \\\n","  run_type=train_val \\\n","  training.log_interval=200 \\\n","  training.batch_size=64 \\\n","  training.evaluation_interval=200 \\\n","  training.tensorboard=True \\\n","  training.checkpoint_interval=200 \\\n","  checkpoint.resume_pretrained=True \\\n","  checkpoint.resume_zoo=visual_bert.pretrained.coco \\\n","  dataset_config.hateful_memes.annotations.train[0]=\"hateful_memes/defaults/annotations/train_dev.jsonl\" \\\n","  dataset_config.hateful_memes.annotations.val[0]=\"hateful_memes/defaults/annotations/dev_unseen.jsonl\" \\\n","  dataset_config.hateful_memes.annotations.test[0]=\"hateful_memes/defaults/annotations/test_unseen.jsonl\""],"metadata":{"id":"6tBFsepoVj5n","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1713743374588,"user_tz":420,"elapsed":1149295,"user":{"displayName":"Aneri Shah","userId":"14009776940163552827"}},"outputId":"b78473b7-7597-4674-f0f0-a52cb9cf1b85"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["2024-04-21 23:16:01.895927: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","2024-04-21 23:16:01.895971: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","2024-04-21 23:16:01.897544: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","2024-04-21 23:16:02.913111: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","2024-04-21 23:16:07.044321: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:47] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n","/usr/local/lib/python3.10/dist-packages/transformers/deepspeed.py:23: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/omegaconf/grammar_visitor.py:253: UserWarning: In the sequence `MMF_USER_DIR,` some elements are missing: please replace them with empty quoted strings. See https://github.com/omry/omegaconf/issues/572 for details.\n","  warnings.warn(\n","\u001b[32m2024-04-21T23:16:11 | mmf.utils.configuration: \u001b[0mOverriding option config to ./configs/defaults.yaml\n","\u001b[32m2024-04-21T23:16:11 | mmf.utils.configuration: \u001b[0mOverriding option model to visual_bert\n","\u001b[32m2024-04-21T23:16:11 | mmf.utils.configuration: \u001b[0mOverriding option datasets to hateful_memes\n","\u001b[32m2024-04-21T23:16:11 | mmf.utils.configuration: \u001b[0mOverriding option run_type to train_val\n","\u001b[32m2024-04-21T23:16:11 | mmf.utils.configuration: \u001b[0mOverriding option training.log_interval to 200\n","\u001b[32m2024-04-21T23:16:11 | mmf.utils.configuration: \u001b[0mOverriding option training.batch_size to 64\n","\u001b[32m2024-04-21T23:16:11 | mmf.utils.configuration: \u001b[0mOverriding option training.evaluation_interval to 200\n","\u001b[32m2024-04-21T23:16:11 | mmf.utils.configuration: \u001b[0mOverriding option training.tensorboard to True\n","\u001b[32m2024-04-21T23:16:11 | mmf.utils.configuration: \u001b[0mOverriding option training.checkpoint_interval to 200\n","\u001b[32m2024-04-21T23:16:11 | mmf.utils.configuration: \u001b[0mOverriding option checkpoint.resume_pretrained to True\n","\u001b[32m2024-04-21T23:16:11 | mmf.utils.configuration: \u001b[0mOverriding option checkpoint.resume_zoo to visual_bert.pretrained.coco\n","\u001b[32m2024-04-21T23:16:11 | mmf.utils.configuration: \u001b[0mOverriding option dataset_config.hateful_memes.annotations.train[0] to hateful_memes/defaults/annotations/train_dev.jsonl\n","\u001b[32m2024-04-21T23:16:11 | mmf.utils.configuration: \u001b[0mOverriding option dataset_config.hateful_memes.annotations.val[0] to hateful_memes/defaults/annotations/dev_unseen.jsonl\n","\u001b[32m2024-04-21T23:16:11 | mmf.utils.configuration: \u001b[0mOverriding option dataset_config.hateful_memes.annotations.test[0] to hateful_memes/defaults/annotations/test_unseen.jsonl\n","/usr/local/lib/python3.10/dist-packages/omegaconf/grammar_visitor.py:253: UserWarning: In the sequence `MMF_LOG_DIR,` some elements are missing: please replace them with empty quoted strings. See https://github.com/omry/omegaconf/issues/572 for details.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/omegaconf/grammar_visitor.py:253: UserWarning: In the sequence `MMF_REPORT_DIR,` some elements are missing: please replace them with empty quoted strings. See https://github.com/omry/omegaconf/issues/572 for details.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/omegaconf/grammar_visitor.py:253: UserWarning: In the sequence `MMF_TENSORBOARD_LOGDIR,` some elements are missing: please replace them with empty quoted strings. See https://github.com/omry/omegaconf/issues/572 for details.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/omegaconf/grammar_visitor.py:253: UserWarning: In the sequence `MMF_WANDB_LOGDIR,` some elements are missing: please replace them with empty quoted strings. See https://github.com/omry/omegaconf/issues/572 for details.\n","  warnings.warn(\n","\u001b[32m2024-04-21T23:16:11 | mmf: \u001b[0mLogging to: ./save/train.log\n","\u001b[32m2024-04-21T23:16:11 | mmf_cli.run: \u001b[0mNamespace(config_override=None, local_rank=None, opts=['config=./configs/defaults.yaml', 'model=visual_bert', 'dataset=hateful_memes', 'run_type=train_val', 'training.log_interval=200', 'training.batch_size=64', 'training.evaluation_interval=200', 'training.tensorboard=True', 'training.checkpoint_interval=200', 'checkpoint.resume_pretrained=True', 'checkpoint.resume_zoo=visual_bert.pretrained.coco', 'dataset_config.hateful_memes.annotations.train[0]=hateful_memes/defaults/annotations/train_dev.jsonl', 'dataset_config.hateful_memes.annotations.val[0]=hateful_memes/defaults/annotations/dev_unseen.jsonl', 'dataset_config.hateful_memes.annotations.test[0]=hateful_memes/defaults/annotations/test_unseen.jsonl'])\n","\u001b[32m2024-04-21T23:16:11 | mmf_cli.run: \u001b[0mTorch version: 2.2.0+cu121\n","\u001b[32m2024-04-21T23:16:11 | mmf.utils.general: \u001b[0mCUDA Device 0 is: NVIDIA L4\n","\u001b[32m2024-04-21T23:16:11 | mmf_cli.run: \u001b[0mUsing seed 11773544\n","\u001b[32m2024-04-21T23:16:11 | mmf.trainers.mmf_trainer: \u001b[0mLoading datasets\n","loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--bert-base-uncased/snapshots/86b5e0934494bd15c9632b12f734a8a67f723594/config.json\n","Model config BertConfig {\n","  \"_name_or_path\": \"bert-base-uncased\",\n","  \"architectures\": [\n","    \"BertForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.38.2\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n","loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--bert-base-uncased/snapshots/86b5e0934494bd15c9632b12f734a8a67f723594/vocab.txt\n","loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--bert-base-uncased/snapshots/86b5e0934494bd15c9632b12f734a8a67f723594/tokenizer.json\n","loading file added_tokens.json from cache at None\n","loading file special_tokens_map.json from cache at None\n","loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--bert-base-uncased/snapshots/86b5e0934494bd15c9632b12f734a8a67f723594/tokenizer_config.json\n","loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--bert-base-uncased/snapshots/86b5e0934494bd15c9632b12f734a8a67f723594/config.json\n","Model config BertConfig {\n","  \"_name_or_path\": \"bert-base-uncased\",\n","  \"architectures\": [\n","    \"BertForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.38.2\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n","\u001b[32m2024-04-21T23:16:12 | mmf.datasets.multi_datamodule: \u001b[0mMultitasking disabled by default for single dataset training\n","\u001b[32m2024-04-21T23:16:12 | mmf.datasets.multi_datamodule: \u001b[0mMultitasking disabled by default for single dataset training\n","\u001b[32m2024-04-21T23:16:12 | mmf.datasets.multi_datamodule: \u001b[0mMultitasking disabled by default for single dataset training\n","\u001b[32m2024-04-21T23:16:12 | mmf.trainers.mmf_trainer: \u001b[0mLoading model\n","Model config BertConfig {\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bert_model_name\": \"bert-base-uncased\",\n","  \"bypass_transformer\": false,\n","  \"classifier_dropout\": null,\n","  \"embedding_strategy\": \"plain\",\n","  \"finetune_lr_multiplier\": 1,\n","  \"freeze_base\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"losses\": [\n","    \"cross_entropy\"\n","  ],\n","  \"max_position_embeddings\": 512,\n","  \"model\": \"visual_bert\",\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"pooler_strategy\": \"default\",\n","  \"position_embedding_type\": \"absolute\",\n","  \"random_initialize\": false,\n","  \"special_visual_initialize\": true,\n","  \"training_head_type\": \"classification\",\n","  \"transformers_version\": \"4.38.2\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"visual_embedding_dim\": 2048,\n","  \"vocab_size\": 30522,\n","  \"zerobias\": false\n","}\n","\n","loading weights file model.safetensors from cache at /root/.cache/torch/mmf/distributed_-1/models--bert-base-uncased/snapshots/86b5e0934494bd15c9632b12f734a8a67f723594/model.safetensors\n","Some weights of the model checkpoint at bert-base-uncased were not used when initializing VisualBERTBase: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n","- This IS expected if you are initializing VisualBERTBase from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing VisualBERTBase from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of VisualBERTBase were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['bert.embeddings.position_embeddings_visual.weight', 'bert.embeddings.projection.bias', 'bert.embeddings.projection.weight', 'bert.embeddings.token_type_embeddings_visual.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","\u001b[32m2024-04-21T23:16:12 | mmf.trainers.mmf_trainer: \u001b[0mLoading optimizer\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2024-04-21T23:16:12 | py.warnings: \u001b[0m/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n","\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2024-04-21T23:16:12 | py.warnings: \u001b[0m/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n","\n","\u001b[32m2024-04-21T23:16:12 | mmf.trainers.mmf_trainer: \u001b[0mLoading metrics\n","\u001b[32m2024-04-21T23:16:12 | mmf.utils.checkpoint: \u001b[0mLoading checkpoint\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2024-04-21T23:16:13 | mmf: \u001b[0mKey data_parallel is not present in registry, returning default value of None\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2024-04-21T23:16:13 | mmf: \u001b[0mKey distributed is not present in registry, returning default value of None\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2024-04-21T23:16:13 | mmf: \u001b[0mKey data_parallel is not present in registry, returning default value of None\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2024-04-21T23:16:13 | mmf: \u001b[0mKey distributed is not present in registry, returning default value of None\n","\u001b[32m2024-04-21T23:16:13 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.word_embeddings.weight from model.bert.embeddings.word_embeddings.weight\n","\u001b[32m2024-04-21T23:16:13 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.position_embeddings.weight from model.bert.embeddings.position_embeddings.weight\n","\u001b[32m2024-04-21T23:16:13 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.token_type_embeddings.weight from model.bert.embeddings.token_type_embeddings.weight\n","\u001b[32m2024-04-21T23:16:13 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.LayerNorm.weight from model.bert.embeddings.LayerNorm.weight\n","\u001b[32m2024-04-21T23:16:13 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.LayerNorm.bias from model.bert.embeddings.LayerNorm.bias\n","\u001b[32m2024-04-21T23:16:13 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.token_type_embeddings_visual.weight from model.bert.embeddings.token_type_embeddings_visual.weight\n","\u001b[32m2024-04-21T23:16:13 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.position_embeddings_visual.weight from model.bert.embeddings.position_embeddings_visual.weight\n","\u001b[32m2024-04-21T23:16:13 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.projection.weight from model.bert.embeddings.projection.weight\n","\u001b[32m2024-04-21T23:16:13 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.projection.bias from model.bert.embeddings.projection.bias\n","\u001b[32m2024-04-21T23:16:13 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.self.query.weight from model.bert.encoder.layer.0.attention.self.query.weight\n","\u001b[32m2024-04-21T23:16:13 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.self.query.bias from model.bert.encoder.layer.0.attention.self.query.bias\n","\u001b[32m2024-04-21T23:16:13 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.self.key.weight from model.bert.encoder.layer.0.attention.self.key.weight\n","\u001b[32m2024-04-21T23:16:13 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.self.key.bias from model.bert.encoder.layer.0.attention.self.key.bias\n","\u001b[32m2024-04-21T23:16:13 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.self.value.weight from model.bert.encoder.layer.0.attention.self.value.weight\n","\u001b[32m2024-04-21T23:16:13 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.self.value.bias from model.bert.encoder.layer.0.attention.self.value.bias\n","\u001b[32m2024-04-21T23:16:13 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.output.dense.weight from model.bert.encoder.layer.0.attention.output.dense.weight\n","\u001b[32m2024-04-21T23:16:13 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.output.dense.bias from model.bert.encoder.layer.0.attention.output.dense.bias\n","\u001b[32m2024-04-21T23:16:13 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.output.LayerNorm.weight from model.bert.encoder.layer.0.attention.output.LayerNorm.weight\n","\u001b[32m2024-04-21T23:16:13 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.output.LayerNorm.bias from model.bert.encoder.layer.0.attention.output.LayerNorm.bias\n","\u001b[32m2024-04-21T23:16:13 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.intermediate.dense.weight from model.bert.encoder.layer.0.intermediate.dense.weight\n","\u001b[32m2024-04-21T23:16:13 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.intermediate.dense.bias from model.bert.encoder.layer.0.intermediate.dense.bias\n","\u001b[32m2024-04-21T23:16:13 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.output.dense.weight from model.bert.encoder.layer.0.output.dense.weight\n","\u001b[32m2024-04-21T23:16:13 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.output.dense.bias from model.bert.encoder.layer.0.output.dense.bias\n","\u001b[32m2024-04-21T23:16:13 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.output.LayerNorm.weight from model.bert.encoder.layer.0.output.LayerNorm.weight\n","\u001b[32m2024-04-21T23:16:13 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.output.LayerNorm.bias from model.bert.encoder.layer.0.output.LayerNorm.bias\n","\u001b[32m2024-04-21T23:16:13 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.self.query.weight from model.bert.encoder.layer.1.attention.self.query.weight\n","\u001b[32m2024-04-21T23:16:13 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.self.query.bias from model.bert.encoder.layer.1.attention.self.query.bias\n","\u001b[32m2024-04-21T23:16:13 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.self.key.weight from model.bert.encoder.layer.1.attention.self.key.weight\n","\u001b[32m2024-04-21T23:16:13 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.self.key.bias from model.bert.encoder.layer.1.attention.self.key.bias\n","\u001b[32m2024-04-21T23:16:13 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.self.value.weight from model.bert.encoder.layer.1.attention.self.value.weight\n","\u001b[32m2024-04-21T23:16:13 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.self.value.bias from model.bert.encoder.layer.1.attention.self.value.bias\n","\u001b[32m2024-04-21T23:16:13 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.output.dense.weight from model.bert.encoder.layer.1.attention.output.dense.weight\n","\u001b[32m2024-04-21T23:16:13 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.output.dense.bias from model.bert.encoder.layer.1.attention.output.dense.bias\n","\u001b[32m2024-04-21T23:16:13 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.output.LayerNorm.weight from model.bert.encoder.layer.1.attention.output.LayerNorm.weight\n","\u001b[32m2024-04-21T23:16:13 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.output.LayerNorm.bias from model.bert.encoder.layer.1.attention.output.LayerNorm.bias\n","\u001b[32m2024-04-21T23:16:13 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.intermediate.dense.weight from model.bert.encoder.layer.1.intermediate.dense.weight\n","\u001b[32m2024-04-21T23:16:13 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.intermediate.dense.bias from model.bert.encoder.layer.1.intermediate.dense.bias\n","\u001b[32m2024-04-21T23:16:13 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.output.dense.weight from model.bert.encoder.layer.1.output.dense.weight\n","\u001b[32m2024-04-21T23:16:13 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.output.dense.bias from model.bert.encoder.layer.1.output.dense.bias\n","\u001b[32m2024-04-21T23:16:13 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.output.LayerNorm.weight from model.bert.encoder.layer.1.output.LayerNorm.weight\n","\u001b[32m2024-04-21T23:16:13 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.output.LayerNorm.bias from model.bert.encoder.layer.1.output.LayerNorm.bias\n","\u001b[32m2024-04-21T23:16:13 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.self.query.weight from model.bert.encoder.layer.2.attention.self.query.weight\n","\u001b[32m2024-04-21T23:16:13 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.self.query.bias from model.bert.encoder.layer.2.attention.self.query.bias\n","\u001b[32m2024-04-21T23:16:13 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.self.key.weight from model.bert.encoder.layer.2.attention.self.key.weight\n","\u001b[32m2024-04-21T23:16:13 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.self.key.bias from model.bert.encoder.layer.2.attention.self.key.bias\n","\u001b[32m2024-04-21T23:16:13 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.self.value.weight from model.bert.encoder.layer.2.attention.self.value.weight\n","\u001b[32m2024-04-21T23:16:13 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.self.value.bias from model.bert.encoder.layer.2.attention.self.value.bias\n","\u001b[32m2024-04-21T23:16:13 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.output.dense.weight from model.bert.encoder.layer.2.attention.output.dense.weight\n","\u001b[32m2024-04-21T23:16:13 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.output.dense.bias from model.bert.encoder.layer.2.attention.output.dense.bias\n","\u001b[32m2024-04-21T23:16:13 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.output.LayerNorm.weight from model.bert.encoder.layer.2.attention.output.LayerNorm.weight\n","\u001b[32m2024-04-21T23:16:13 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.output.LayerNorm.bias from model.bert.encoder.layer.2.attention.output.LayerNorm.bias\n","\u001b[32m2024-04-21T23:16:13 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.intermediate.dense.weight from model.bert.encoder.layer.2.intermediate.dense.weight\n","\u001b[32m2024-04-21T23:16:13 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.intermediate.dense.bias from model.bert.encoder.layer.2.intermediate.dense.bias\n","\u001b[32m2024-04-21T23:16:13 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.output.dense.weight from model.bert.encoder.layer.2.output.dense.weight\n","\u001b[32m2024-04-21T23:16:13 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.output.dense.bias from model.bert.encoder.layer.2.output.dense.bias\n","\u001b[32m2024-04-21T23:16:13 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.output.LayerNorm.weight from model.bert.encoder.layer.2.output.LayerNorm.weight\n","\u001b[32m2024-04-21T23:16:13 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.output.LayerNorm.bias from model.bert.encoder.layer.2.output.LayerNorm.bias\n","\u001b[32m2024-04-21T23:16:13 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.self.query.weight from model.bert.encoder.layer.3.attention.self.query.weight\n","\u001b[32m2024-04-21T23:16:13 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.self.query.bias from model.bert.encoder.layer.3.attention.self.query.bias\n","\u001b[32m2024-04-21T23:16:13 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.self.key.weight from model.bert.encoder.layer.3.attention.self.key.weight\n","\u001b[32m2024-04-21T23:16:13 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.self.key.bias from model.bert.encoder.layer.3.attention.self.key.bias\n","\u001b[32m2024-04-21T23:16:13 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.self.value.weight from model.bert.encoder.layer.3.attention.self.value.weight\n","\u001b[32m2024-04-21T23:16:13 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.self.value.bias from model.bert.encoder.layer.3.attention.self.value.bias\n","\u001b[32m2024-04-21T23:16:13 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.output.dense.weight from model.bert.encoder.layer.3.attention.output.dense.weight\n","\u001b[32m2024-04-21T23:16:13 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.output.dense.bias from model.bert.encoder.layer.3.attention.output.dense.bias\n","\u001b[32m2024-04-21T23:16:13 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.output.LayerNorm.weight from model.bert.encoder.layer.3.attention.output.LayerNorm.weight\n","\u001b[32m2024-04-21T23:16:13 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.output.LayerNorm.bias from model.bert.encoder.layer.3.attention.output.LayerNorm.bias\n","\u001b[32m2024-04-21T23:16:13 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.intermediate.dense.weight from model.bert.encoder.layer.3.intermediate.dense.weight\n","\u001b[32m2024-04-21T23:16:13 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.intermediate.dense.bias from model.bert.encoder.layer.3.intermediate.dense.bias\n","\u001b[32m2024-04-21T23:16:13 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.output.dense.weight from model.bert.encoder.layer.3.output.dense.weight\n","\u001b[32m2024-04-21T23:16:13 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.output.dense.bias from model.bert.encoder.layer.3.output.dense.bias\n","\u001b[32m2024-04-21T23:16:13 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.output.LayerNorm.weight from model.bert.encoder.layer.3.output.LayerNorm.weight\n","\u001b[32m2024-04-21T23:16:13 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.output.LayerNorm.bias from model.bert.encoder.layer.3.output.LayerNorm.bias\n","\u001b[32m2024-04-21T23:16:13 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.self.query.weight from model.bert.encoder.layer.4.attention.self.query.weight\n","\u001b[32m2024-04-21T23:16:13 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.self.query.bias from model.bert.encoder.layer.4.attention.self.query.bias\n","\u001b[32m2024-04-21T23:16:13 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.self.key.weight from model.bert.encoder.layer.4.attention.self.key.weight\n","\u001b[32m2024-04-21T23:16:13 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.self.key.bias from model.bert.encoder.layer.4.attention.self.key.bias\n","\u001b[32m2024-04-21T23:16:13 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.self.value.weight from model.bert.encoder.layer.4.attention.self.value.weight\n","\u001b[32m2024-04-21T23:16:13 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.self.value.bias from model.bert.encoder.layer.4.attention.self.value.bias\n","\u001b[32m2024-04-21T23:16:13 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.output.dense.weight from model.bert.encoder.layer.4.attention.output.dense.weight\n","\u001b[32m2024-04-21T23:16:13 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.output.dense.bias from model.bert.encoder.layer.4.attention.output.dense.bias\n","\u001b[32m2024-04-21T23:16:13 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.output.LayerNorm.weight from model.bert.encoder.layer.4.attention.output.LayerNorm.weight\n","\u001b[32m2024-04-21T23:16:13 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.output.LayerNorm.bias from model.bert.encoder.layer.4.attention.output.LayerNorm.bias\n","\u001b[32m2024-04-21T23:16:13 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.intermediate.dense.weight from model.bert.encoder.layer.4.intermediate.dense.weight\n","\u001b[32m2024-04-21T23:16:13 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.intermediate.dense.bias from model.bert.encoder.layer.4.intermediate.dense.bias\n","\u001b[32m2024-04-21T23:16:13 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.output.dense.weight from model.bert.encoder.layer.4.output.dense.weight\n","\u001b[32m2024-04-21T23:16:13 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.output.dense.bias from model.bert.encoder.layer.4.output.dense.bias\n","\u001b[32m2024-04-21T23:16:13 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.output.LayerNorm.weight from model.bert.encoder.layer.4.output.LayerNorm.weight\n","\u001b[32m2024-04-21T23:16:13 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.output.LayerNorm.bias from model.bert.encoder.layer.4.output.LayerNorm.bias\n","\u001b[32m2024-04-21T23:16:13 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.self.query.weight from model.bert.encoder.layer.5.attention.self.query.weight\n","\u001b[32m2024-04-21T23:16:13 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.self.query.bias from model.bert.encoder.layer.5.attention.self.query.bias\n","\u001b[32m2024-04-21T23:16:13 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.self.key.weight from model.bert.encoder.layer.5.attention.self.key.weight\n","\u001b[32m2024-04-21T23:16:13 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.self.key.bias from model.bert.encoder.layer.5.attention.self.key.bias\n","\u001b[32m2024-04-21T23:16:13 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.self.value.weight from model.bert.encoder.layer.5.attention.self.value.weight\n","\u001b[32m2024-04-21T23:16:13 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.self.value.bias from model.bert.encoder.layer.5.attention.self.value.bias\n","\u001b[32m2024-04-21T23:16:13 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.output.dense.weight from model.bert.encoder.layer.5.attention.output.dense.weight\n","\u001b[32m2024-04-21T23:16:13 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.output.dense.bias from model.bert.encoder.layer.5.attention.output.dense.bias\n","\u001b[32m2024-04-21T23:16:13 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.output.LayerNorm.weight from model.bert.encoder.layer.5.attention.output.LayerNorm.weight\n","\u001b[32m2024-04-21T23:16:13 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.output.LayerNorm.bias from model.bert.encoder.layer.5.attention.output.LayerNorm.bias\n","\u001b[32m2024-04-21T23:16:13 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.intermediate.dense.weight from model.bert.encoder.layer.5.intermediate.dense.weight\n","\u001b[32m2024-04-21T23:16:13 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.intermediate.dense.bias from model.bert.encoder.layer.5.intermediate.dense.bias\n","\u001b[32m2024-04-21T23:16:13 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.output.dense.weight from model.bert.encoder.layer.5.output.dense.weight\n","\u001b[32m2024-04-21T23:16:13 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.output.dense.bias from model.bert.encoder.layer.5.output.dense.bias\n","\u001b[32m2024-04-21T23:16:13 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.output.LayerNorm.weight from model.bert.encoder.layer.5.output.LayerNorm.weight\n","\u001b[32m2024-04-21T23:16:13 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.output.LayerNorm.bias from model.bert.encoder.layer.5.output.LayerNorm.bias\n","\u001b[32m2024-04-21T23:16:13 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.self.query.weight from model.bert.encoder.layer.6.attention.self.query.weight\n","\u001b[32m2024-04-21T23:16:13 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.self.query.bias from model.bert.encoder.layer.6.attention.self.query.bias\n","\u001b[32m2024-04-21T23:16:13 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.self.key.weight from model.bert.encoder.layer.6.attention.self.key.weight\n","\u001b[32m2024-04-21T23:16:13 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.self.key.bias from model.bert.encoder.layer.6.attention.self.key.bias\n","\u001b[32m2024-04-21T23:16:13 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.self.value.weight from model.bert.encoder.layer.6.attention.self.value.weight\n","\u001b[32m2024-04-21T23:16:13 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.self.value.bias from model.bert.encoder.layer.6.attention.self.value.bias\n","\u001b[32m2024-04-21T23:16:13 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.output.dense.weight from model.bert.encoder.layer.6.attention.output.dense.weight\n","\u001b[32m2024-04-21T23:16:13 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.output.dense.bias from model.bert.encoder.layer.6.attention.output.dense.bias\n","\u001b[32m2024-04-21T23:16:13 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.output.LayerNorm.weight from model.bert.encoder.layer.6.attention.output.LayerNorm.weight\n","\u001b[32m2024-04-21T23:16:13 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.output.LayerNorm.bias from model.bert.encoder.layer.6.attention.output.LayerNorm.bias\n","\u001b[32m2024-04-21T23:16:13 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.intermediate.dense.weight from model.bert.encoder.layer.6.intermediate.dense.weight\n","\u001b[32m2024-04-21T23:16:13 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.intermediate.dense.bias from model.bert.encoder.layer.6.intermediate.dense.bias\n","\u001b[32m2024-04-21T23:16:13 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.output.dense.weight from model.bert.encoder.layer.6.output.dense.weight\n","\u001b[32m2024-04-21T23:16:13 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.output.dense.bias from model.bert.encoder.layer.6.output.dense.bias\n","\u001b[32m2024-04-21T23:16:13 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.output.LayerNorm.weight from model.bert.encoder.layer.6.output.LayerNorm.weight\n","\u001b[32m2024-04-21T23:16:13 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.output.LayerNorm.bias from model.bert.encoder.layer.6.output.LayerNorm.bias\n","\u001b[32m2024-04-21T23:16:13 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.self.query.weight from model.bert.encoder.layer.7.attention.self.query.weight\n","\u001b[32m2024-04-21T23:16:13 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.self.query.bias from model.bert.encoder.layer.7.attention.self.query.bias\n","\u001b[32m2024-04-21T23:16:13 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.self.key.weight from model.bert.encoder.layer.7.attention.self.key.weight\n","\u001b[32m2024-04-21T23:16:13 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.self.key.bias from model.bert.encoder.layer.7.attention.self.key.bias\n","\u001b[32m2024-04-21T23:16:13 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.self.value.weight from model.bert.encoder.layer.7.attention.self.value.weight\n","\u001b[32m2024-04-21T23:16:13 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.self.value.bias from model.bert.encoder.layer.7.attention.self.value.bias\n","\u001b[32m2024-04-21T23:16:13 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.output.dense.weight from model.bert.encoder.layer.7.attention.output.dense.weight\n","\u001b[32m2024-04-21T23:16:13 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.output.dense.bias from model.bert.encoder.layer.7.attention.output.dense.bias\n","\u001b[32m2024-04-21T23:16:13 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.output.LayerNorm.weight from model.bert.encoder.layer.7.attention.output.LayerNorm.weight\n","\u001b[32m2024-04-21T23:16:13 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.output.LayerNorm.bias from model.bert.encoder.layer.7.attention.output.LayerNorm.bias\n","\u001b[32m2024-04-21T23:16:13 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.intermediate.dense.weight from model.bert.encoder.layer.7.intermediate.dense.weight\n","\u001b[32m2024-04-21T23:16:13 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.intermediate.dense.bias from model.bert.encoder.layer.7.intermediate.dense.bias\n","\u001b[32m2024-04-21T23:16:13 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.output.dense.weight from model.bert.encoder.layer.7.output.dense.weight\n","\u001b[32m2024-04-21T23:16:13 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.output.dense.bias from model.bert.encoder.layer.7.output.dense.bias\n","\u001b[32m2024-04-21T23:16:13 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.output.LayerNorm.weight from model.bert.encoder.layer.7.output.LayerNorm.weight\n","\u001b[32m2024-04-21T23:16:13 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.output.LayerNorm.bias from model.bert.encoder.layer.7.output.LayerNorm.bias\n","\u001b[32m2024-04-21T23:16:13 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.self.query.weight from model.bert.encoder.layer.8.attention.self.query.weight\n","\u001b[32m2024-04-21T23:16:13 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.self.query.bias from model.bert.encoder.layer.8.attention.self.query.bias\n","\u001b[32m2024-04-21T23:16:13 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.self.key.weight from model.bert.encoder.layer.8.attention.self.key.weight\n","\u001b[32m2024-04-21T23:16:13 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.self.key.bias from model.bert.encoder.layer.8.attention.self.key.bias\n","\u001b[32m2024-04-21T23:16:13 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.self.value.weight from model.bert.encoder.layer.8.attention.self.value.weight\n","\u001b[32m2024-04-21T23:16:13 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.self.value.bias from model.bert.encoder.layer.8.attention.self.value.bias\n","\u001b[32m2024-04-21T23:16:13 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.output.dense.weight from model.bert.encoder.layer.8.attention.output.dense.weight\n","\u001b[32m2024-04-21T23:16:13 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.output.dense.bias from model.bert.encoder.layer.8.attention.output.dense.bias\n","\u001b[32m2024-04-21T23:16:13 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.output.LayerNorm.weight from model.bert.encoder.layer.8.attention.output.LayerNorm.weight\n","\u001b[32m2024-04-21T23:16:13 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.output.LayerNorm.bias from model.bert.encoder.layer.8.attention.output.LayerNorm.bias\n","\u001b[32m2024-04-21T23:16:13 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.intermediate.dense.weight from model.bert.encoder.layer.8.intermediate.dense.weight\n","\u001b[32m2024-04-21T23:16:13 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.intermediate.dense.bias from model.bert.encoder.layer.8.intermediate.dense.bias\n","\u001b[32m2024-04-21T23:16:13 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.output.dense.weight from model.bert.encoder.layer.8.output.dense.weight\n","\u001b[32m2024-04-21T23:16:13 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.output.dense.bias from model.bert.encoder.layer.8.output.dense.bias\n","\u001b[32m2024-04-21T23:16:13 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.output.LayerNorm.weight from model.bert.encoder.layer.8.output.LayerNorm.weight\n","\u001b[32m2024-04-21T23:16:13 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.output.LayerNorm.bias from model.bert.encoder.layer.8.output.LayerNorm.bias\n","\u001b[32m2024-04-21T23:16:13 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.self.query.weight from model.bert.encoder.layer.9.attention.self.query.weight\n","\u001b[32m2024-04-21T23:16:13 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.self.query.bias from model.bert.encoder.layer.9.attention.self.query.bias\n","\u001b[32m2024-04-21T23:16:13 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.self.key.weight from model.bert.encoder.layer.9.attention.self.key.weight\n","\u001b[32m2024-04-21T23:16:13 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.self.key.bias from model.bert.encoder.layer.9.attention.self.key.bias\n","\u001b[32m2024-04-21T23:16:13 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.self.value.weight from model.bert.encoder.layer.9.attention.self.value.weight\n","\u001b[32m2024-04-21T23:16:13 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.self.value.bias from model.bert.encoder.layer.9.attention.self.value.bias\n","\u001b[32m2024-04-21T23:16:13 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.output.dense.weight from model.bert.encoder.layer.9.attention.output.dense.weight\n","\u001b[32m2024-04-21T23:16:13 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.output.dense.bias from model.bert.encoder.layer.9.attention.output.dense.bias\n","\u001b[32m2024-04-21T23:16:13 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.output.LayerNorm.weight from model.bert.encoder.layer.9.attention.output.LayerNorm.weight\n","\u001b[32m2024-04-21T23:16:13 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.output.LayerNorm.bias from model.bert.encoder.layer.9.attention.output.LayerNorm.bias\n","\u001b[32m2024-04-21T23:16:13 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.intermediate.dense.weight from model.bert.encoder.layer.9.intermediate.dense.weight\n","\u001b[32m2024-04-21T23:16:13 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.intermediate.dense.bias from model.bert.encoder.layer.9.intermediate.dense.bias\n","\u001b[32m2024-04-21T23:16:13 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.output.dense.weight from model.bert.encoder.layer.9.output.dense.weight\n","\u001b[32m2024-04-21T23:16:13 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.output.dense.bias from model.bert.encoder.layer.9.output.dense.bias\n","\u001b[32m2024-04-21T23:16:13 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.output.LayerNorm.weight from model.bert.encoder.layer.9.output.LayerNorm.weight\n","\u001b[32m2024-04-21T23:16:13 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.output.LayerNorm.bias from model.bert.encoder.layer.9.output.LayerNorm.bias\n","\u001b[32m2024-04-21T23:16:13 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.self.query.weight from model.bert.encoder.layer.10.attention.self.query.weight\n","\u001b[32m2024-04-21T23:16:13 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.self.query.bias from model.bert.encoder.layer.10.attention.self.query.bias\n","\u001b[32m2024-04-21T23:16:13 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.self.key.weight from model.bert.encoder.layer.10.attention.self.key.weight\n","\u001b[32m2024-04-21T23:16:13 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.self.key.bias from model.bert.encoder.layer.10.attention.self.key.bias\n","\u001b[32m2024-04-21T23:16:13 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.self.value.weight from model.bert.encoder.layer.10.attention.self.value.weight\n","\u001b[32m2024-04-21T23:16:13 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.self.value.bias from model.bert.encoder.layer.10.attention.self.value.bias\n","\u001b[32m2024-04-21T23:16:13 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.output.dense.weight from model.bert.encoder.layer.10.attention.output.dense.weight\n","\u001b[32m2024-04-21T23:16:13 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.output.dense.bias from model.bert.encoder.layer.10.attention.output.dense.bias\n","\u001b[32m2024-04-21T23:16:13 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.output.LayerNorm.weight from model.bert.encoder.layer.10.attention.output.LayerNorm.weight\n","\u001b[32m2024-04-21T23:16:13 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.output.LayerNorm.bias from model.bert.encoder.layer.10.attention.output.LayerNorm.bias\n","\u001b[32m2024-04-21T23:16:13 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.intermediate.dense.weight from model.bert.encoder.layer.10.intermediate.dense.weight\n","\u001b[32m2024-04-21T23:16:13 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.intermediate.dense.bias from model.bert.encoder.layer.10.intermediate.dense.bias\n","\u001b[32m2024-04-21T23:16:13 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.output.dense.weight from model.bert.encoder.layer.10.output.dense.weight\n","\u001b[32m2024-04-21T23:16:13 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.output.dense.bias from model.bert.encoder.layer.10.output.dense.bias\n","\u001b[32m2024-04-21T23:16:13 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.output.LayerNorm.weight from model.bert.encoder.layer.10.output.LayerNorm.weight\n","\u001b[32m2024-04-21T23:16:13 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.output.LayerNorm.bias from model.bert.encoder.layer.10.output.LayerNorm.bias\n","\u001b[32m2024-04-21T23:16:13 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.self.query.weight from model.bert.encoder.layer.11.attention.self.query.weight\n","\u001b[32m2024-04-21T23:16:13 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.self.query.bias from model.bert.encoder.layer.11.attention.self.query.bias\n","\u001b[32m2024-04-21T23:16:13 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.self.key.weight from model.bert.encoder.layer.11.attention.self.key.weight\n","\u001b[32m2024-04-21T23:16:13 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.self.key.bias from model.bert.encoder.layer.11.attention.self.key.bias\n","\u001b[32m2024-04-21T23:16:13 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.self.value.weight from model.bert.encoder.layer.11.attention.self.value.weight\n","\u001b[32m2024-04-21T23:16:13 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.self.value.bias from model.bert.encoder.layer.11.attention.self.value.bias\n","\u001b[32m2024-04-21T23:16:13 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.output.dense.weight from model.bert.encoder.layer.11.attention.output.dense.weight\n","\u001b[32m2024-04-21T23:16:13 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.output.dense.bias from model.bert.encoder.layer.11.attention.output.dense.bias\n","\u001b[32m2024-04-21T23:16:13 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.output.LayerNorm.weight from model.bert.encoder.layer.11.attention.output.LayerNorm.weight\n","\u001b[32m2024-04-21T23:16:13 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.output.LayerNorm.bias from model.bert.encoder.layer.11.attention.output.LayerNorm.bias\n","\u001b[32m2024-04-21T23:16:13 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.intermediate.dense.weight from model.bert.encoder.layer.11.intermediate.dense.weight\n","\u001b[32m2024-04-21T23:16:13 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.intermediate.dense.bias from model.bert.encoder.layer.11.intermediate.dense.bias\n","\u001b[32m2024-04-21T23:16:13 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.output.dense.weight from model.bert.encoder.layer.11.output.dense.weight\n","\u001b[32m2024-04-21T23:16:13 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.output.dense.bias from model.bert.encoder.layer.11.output.dense.bias\n","\u001b[32m2024-04-21T23:16:13 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.output.LayerNorm.weight from model.bert.encoder.layer.11.output.LayerNorm.weight\n","\u001b[32m2024-04-21T23:16:13 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.output.LayerNorm.bias from model.bert.encoder.layer.11.output.LayerNorm.bias\n","\u001b[32m2024-04-21T23:16:13 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.pooler.dense.weight from model.bert.pooler.dense.weight\n","\u001b[32m2024-04-21T23:16:13 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.pooler.dense.bias from model.bert.pooler.dense.bias\n","\u001b[32m2024-04-21T23:16:13 | mmf.utils.checkpoint: \u001b[0mPretrained model loaded\n","\u001b[32m2024-04-21T23:16:13 | mmf.utils.checkpoint: \u001b[0mCheckpoint loaded.\n","\u001b[32m2024-04-21T23:16:13 | mmf.utils.checkpoint: \u001b[0mCurrent num updates: 0\n","\u001b[32m2024-04-21T23:16:13 | mmf.utils.checkpoint: \u001b[0mCurrent iteration: 0\n","\u001b[32m2024-04-21T23:16:13 | mmf.utils.checkpoint: \u001b[0mCurrent epoch: 0\n","\u001b[32m2024-04-21T23:16:13 | mmf.trainers.mmf_trainer: \u001b[0m===== Model =====\n","\u001b[32m2024-04-21T23:16:13 | mmf.trainers.mmf_trainer: \u001b[0mVisualBERT(\n","  (model): VisualBERTForClassification(\n","    (bert): VisualBERTBase(\n","      (embeddings): BertVisioLinguisticEmbeddings(\n","        (word_embeddings): Embedding(30522, 768, padding_idx=0)\n","        (position_embeddings): Embedding(512, 768)\n","        (token_type_embeddings): Embedding(2, 768)\n","        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","        (token_type_embeddings_visual): Embedding(2, 768)\n","        (position_embeddings_visual): Embedding(512, 768)\n","        (projection): Linear(in_features=2048, out_features=768, bias=True)\n","      )\n","      (encoder): BertEncoderJit(\n","        (layer): ModuleList(\n","          (0-11): 12 x BertLayerJit(\n","            (attention): BertAttentionJit(\n","              (self): BertSelfAttentionJit(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","              (intermediate_act_fn): GELUActivation()\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","        )\n","      )\n","      (pooler): BertPooler(\n","        (dense): Linear(in_features=768, out_features=768, bias=True)\n","        (activation): Tanh()\n","      )\n","    )\n","    (dropout): Dropout(p=0.1, inplace=False)\n","    (classifier): Sequential(\n","      (0): BertPredictionHeadTransform(\n","        (dense): Linear(in_features=768, out_features=768, bias=True)\n","        (transform_act_fn): GELUActivation()\n","        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      )\n","      (1): Linear(in_features=768, out_features=2, bias=True)\n","    )\n","  )\n","  (losses): Losses(\n","    (losses): ModuleList(\n","      (0): MMFLoss(\n","        (loss_criterion): CrossEntropyLoss(\n","          (loss_fn): CrossEntropyLoss()\n","        )\n","      )\n","    )\n","  )\n",")\n","\u001b[32m2024-04-21T23:16:13 | mmf.utils.general: \u001b[0mTotal Parameters: 112044290. Trained Parameters: 112044290\n","\u001b[32m2024-04-21T23:16:13 | mmf.trainers.core.training_loop: \u001b[0mStarting training...\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2024-04-21T23:16:13 | py.warnings: \u001b[0m/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n","  self.pid = os.fork()\n","\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2024-04-21T23:16:13 | py.warnings: \u001b[0m/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n","  self.pid = os.fork()\n","\n","\u001b[32m2024-04-21T23:20:15 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n","\u001b[32m2024-04-21T23:20:15 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2024-04-21T23:20:17 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2024-04-21T23:20:23 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2024-04-21T23:20:23 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 200/1500, train/hateful_memes/cross_entropy: 0.6065, train/hateful_memes/cross_entropy/avg: 0.6065, train/total_loss: 0.6065, train/total_loss/avg: 0.6065, max mem: 14322.0, experiment: run, epoch: 2, num_updates: 200, iterations: 200, max_updates: 1500, lr: 0.00001, ups: 0.80, time: 04m 10s 096ms, time_since_start: 04m 10s 748ms, eta: 28m 18s 783ms\n","\u001b[32m2024-04-21T23:20:23 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n","\u001b[32m2024-04-21T23:20:23 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n","\u001b[32m2024-04-21T23:20:27 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished evaluation inference. Loaded 9\n","\u001b[32m2024-04-21T23:20:27 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n","\u001b[32m2024-04-21T23:20:28 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2024-04-21T23:20:30 | mmf.utils.checkpoint: \u001b[0mSaving best checkpoint\n","\u001b[32m2024-04-21T23:20:34 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2024-04-21T23:20:37 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2024-04-21T23:20:37 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 200/1500, val/hateful_memes/cross_entropy: 0.6626, val/total_loss: 0.6626, val/hateful_memes/accuracy: 0.6000, val/hateful_memes/binary_f1: 0.2286, val/hateful_memes/roc_auc: 0.5712, num_updates: 200, epoch: 2, iterations: 200, max_updates: 1500, val_time: 14s 438ms, best_update: 200, best_iteration: 200, best_val/hateful_memes/roc_auc: 0.571235\n","\u001b[32m2024-04-21T23:24:38 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n","\u001b[32m2024-04-21T23:24:38 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2024-04-21T23:24:40 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2024-04-21T23:24:43 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2024-04-21T23:24:43 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 400/1500, train/hateful_memes/cross_entropy: 0.3559, train/hateful_memes/cross_entropy/avg: 0.4812, train/total_loss: 0.3559, train/total_loss/avg: 0.4812, max mem: 14371.0, experiment: run, epoch: 3, num_updates: 400, iterations: 400, max_updates: 1500, lr: 0.00001, ups: 0.82, time: 04m 05s 449ms, time_since_start: 08m 30s 646ms, eta: 23m 30s 722ms\n","\u001b[32m2024-04-21T23:24:43 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n","\u001b[32m2024-04-21T23:24:43 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2024-04-21T23:24:43 | py.warnings: \u001b[0m/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n","  self.pid = os.fork()\n","\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2024-04-21T23:24:43 | py.warnings: \u001b[0m/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n","  self.pid = os.fork()\n","\n","\u001b[32m2024-04-21T23:24:47 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished evaluation inference. Loaded 9\n","\u001b[32m2024-04-21T23:24:47 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n","\u001b[32m2024-04-21T23:24:47 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2024-04-21T23:24:50 | mmf.utils.checkpoint: \u001b[0mSaving best checkpoint\n","\u001b[32m2024-04-21T23:24:55 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2024-04-21T23:24:57 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2024-04-21T23:24:57 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 400/1500, val/hateful_memes/cross_entropy: 0.7296, val/total_loss: 0.7296, val/hateful_memes/accuracy: 0.6407, val/hateful_memes/binary_f1: 0.3782, val/hateful_memes/roc_auc: 0.6395, num_updates: 400, epoch: 3, iterations: 400, max_updates: 1500, val_time: 14s 572ms, best_update: 400, best_iteration: 400, best_val/hateful_memes/roc_auc: 0.639471\n","\u001b[32m2024-04-21T23:28:58 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n","\u001b[32m2024-04-21T23:28:58 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2024-04-21T23:29:00 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2024-04-21T23:29:03 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2024-04-21T23:29:03 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 600/1500, train/hateful_memes/cross_entropy: 0.3559, train/hateful_memes/cross_entropy/avg: 0.3829, train/total_loss: 0.3559, train/total_loss/avg: 0.3829, max mem: 14371.0, experiment: run, epoch: 5, num_updates: 600, iterations: 600, max_updates: 1500, lr: 0.00002, ups: 0.81, time: 04m 06s 039ms, time_since_start: 12m 51s 259ms, eta: 19m 16s 999ms\n","\u001b[32m2024-04-21T23:29:03 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n","\u001b[32m2024-04-21T23:29:03 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2024-04-21T23:29:03 | py.warnings: \u001b[0m/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n","  self.pid = os.fork()\n","\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2024-04-21T23:29:03 | py.warnings: \u001b[0m/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n","  self.pid = os.fork()\n","\n","\u001b[32m2024-04-21T23:29:08 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished evaluation inference. Loaded 9\n","\u001b[32m2024-04-21T23:29:08 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n","\u001b[32m2024-04-21T23:29:08 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2024-04-21T23:29:10 | mmf.utils.checkpoint: \u001b[0mSaving best checkpoint\n","\u001b[32m2024-04-21T23:29:14 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2024-04-21T23:29:21 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2024-04-21T23:29:21 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 600/1500, val/hateful_memes/cross_entropy: 0.7499, val/total_loss: 0.7499, val/hateful_memes/accuracy: 0.6870, val/hateful_memes/binary_f1: 0.4925, val/hateful_memes/roc_auc: 0.7039, num_updates: 600, epoch: 5, iterations: 600, max_updates: 1500, val_time: 17s 352ms, best_update: 600, best_iteration: 600, best_val/hateful_memes/roc_auc: 0.703868\n","\u001b[32m2024-04-21T23:33:21 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n","\u001b[32m2024-04-21T23:33:21 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2024-04-21T23:33:23 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2024-04-21T23:33:29 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2024-04-21T23:33:29 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 800/1500, train/hateful_memes/cross_entropy: 0.3559, train/hateful_memes/cross_entropy/avg: 0.3960, train/total_loss: 0.3559, train/total_loss/avg: 0.3960, max mem: 14372.0, experiment: run, epoch: 6, num_updates: 800, iterations: 800, max_updates: 1500, lr: 0.00002, ups: 0.81, time: 04m 08s 489ms, time_since_start: 17m 17s 102ms, eta: 15m 08s 848ms\n","\u001b[32m2024-04-21T23:33:29 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n","\u001b[32m2024-04-21T23:33:29 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2024-04-21T23:33:29 | py.warnings: \u001b[0m/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n","  self.pid = os.fork()\n","\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2024-04-21T23:33:29 | py.warnings: \u001b[0m/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n","  self.pid = os.fork()\n","\n","\u001b[32m2024-04-21T23:33:34 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished evaluation inference. Loaded 9\n","\u001b[32m2024-04-21T23:33:34 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n","\u001b[32m2024-04-21T23:33:34 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2024-04-21T23:33:36 | mmf.utils.checkpoint: \u001b[0mSaving best checkpoint\n","\u001b[32m2024-04-21T23:33:42 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2024-04-21T23:33:47 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2024-04-21T23:33:47 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 800/1500, val/hateful_memes/cross_entropy: 0.9320, val/total_loss: 0.9320, val/hateful_memes/accuracy: 0.6685, val/hateful_memes/binary_f1: 0.4657, val/hateful_memes/roc_auc: 0.7047, num_updates: 800, epoch: 6, iterations: 800, max_updates: 1500, val_time: 17s 243ms, best_update: 800, best_iteration: 800, best_val/hateful_memes/roc_auc: 0.704676\n","\u001b[32m2024-04-21T23:37:47 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n","\u001b[32m2024-04-21T23:37:47 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2024-04-21T23:37:49 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2024-04-21T23:37:53 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2024-04-21T23:37:53 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1000/1500, train/hateful_memes/cross_entropy: 0.3559, train/hateful_memes/cross_entropy/avg: 0.3245, train/total_loss: 0.3559, train/total_loss/avg: 0.3245, max mem: 14372.0, experiment: run, epoch: 8, num_updates: 1000, iterations: 1000, max_updates: 1500, lr: 0.00003, ups: 0.81, time: 04m 06s 796ms, time_since_start: 21m 41s 144ms, eta: 10m 44s 755ms\n","\u001b[32m2024-04-21T23:37:53 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n","\u001b[32m2024-04-21T23:37:53 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2024-04-21T23:37:53 | py.warnings: \u001b[0m/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n","  self.pid = os.fork()\n","\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2024-04-21T23:37:53 | py.warnings: \u001b[0m/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n","  self.pid = os.fork()\n","\n","\u001b[32m2024-04-21T23:37:57 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished evaluation inference. Loaded 9\n","\u001b[32m2024-04-21T23:37:58 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n","\u001b[32m2024-04-21T23:37:58 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2024-04-21T23:38:10 | mmf.utils.checkpoint: \u001b[0mSaving best checkpoint\n","\u001b[32m2024-04-21T23:38:14 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2024-04-21T23:38:22 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2024-04-21T23:38:22 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1000/1500, val/hateful_memes/cross_entropy: 1.1172, val/total_loss: 1.1172, val/hateful_memes/accuracy: 0.6870, val/hateful_memes/binary_f1: 0.5130, val/hateful_memes/roc_auc: 0.7203, num_updates: 1000, epoch: 8, iterations: 1000, max_updates: 1500, val_time: 29s 118ms, best_update: 1000, best_iteration: 1000, best_val/hateful_memes/roc_auc: 0.720309\n","\u001b[32m2024-04-21T23:42:23 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n","\u001b[32m2024-04-21T23:42:23 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2024-04-21T23:42:26 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2024-04-21T23:42:30 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2024-04-21T23:42:30 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1200/1500, train/hateful_memes/cross_entropy: 0.1865, train/hateful_memes/cross_entropy/avg: 0.2928, train/total_loss: 0.1865, train/total_loss/avg: 0.2928, max mem: 14372.0, experiment: run, epoch: 9, num_updates: 1200, iterations: 1200, max_updates: 1500, lr: 0.00003, ups: 0.81, time: 04m 07s 369ms, time_since_start: 26m 17s 635ms, eta: 06m 27s 752ms\n","\u001b[32m2024-04-21T23:42:30 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n","\u001b[32m2024-04-21T23:42:30 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2024-04-21T23:42:30 | py.warnings: \u001b[0m/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n","  self.pid = os.fork()\n","\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2024-04-21T23:42:30 | py.warnings: \u001b[0m/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n","  self.pid = os.fork()\n","\n","\u001b[32m2024-04-21T23:42:34 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished evaluation inference. Loaded 9\n","\u001b[32m2024-04-21T23:42:34 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n","\u001b[32m2024-04-21T23:42:34 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2024-04-21T23:42:37 | mmf.utils.checkpoint: \u001b[0mSaving best checkpoint\n","\u001b[32m2024-04-21T23:42:42 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2024-04-21T23:42:51 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2024-04-21T23:42:51 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1200/1500, val/hateful_memes/cross_entropy: 1.3820, val/total_loss: 1.3820, val/hateful_memes/accuracy: 0.6907, val/hateful_memes/binary_f1: 0.4452, val/hateful_memes/roc_auc: 0.7315, num_updates: 1200, epoch: 9, iterations: 1200, max_updates: 1500, val_time: 21s 009ms, best_update: 1200, best_iteration: 1200, best_val/hateful_memes/roc_auc: 0.731544\n","\u001b[32m2024-04-21T23:46:51 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n","\u001b[32m2024-04-21T23:46:51 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2024-04-21T23:46:53 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2024-04-21T23:46:57 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2024-04-21T23:46:57 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1400/1500, train/hateful_memes/cross_entropy: 0.1865, train/hateful_memes/cross_entropy/avg: 0.2702, train/total_loss: 0.1865, train/total_loss/avg: 0.2702, max mem: 14372.0, experiment: run, epoch: 11, num_updates: 1400, iterations: 1400, max_updates: 1500, lr: 0.00003, ups: 0.81, time: 04m 06s 550ms, time_since_start: 30m 45s 196ms, eta: 02m 08s 822ms\n","\u001b[32m2024-04-21T23:46:57 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n","\u001b[32m2024-04-21T23:46:57 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2024-04-21T23:46:57 | py.warnings: \u001b[0m/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n","  self.pid = os.fork()\n","\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2024-04-21T23:46:57 | py.warnings: \u001b[0m/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n","  self.pid = os.fork()\n","\n","\u001b[32m2024-04-21T23:47:02 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished evaluation inference. Loaded 9\n","\u001b[32m2024-04-21T23:47:02 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n","\u001b[32m2024-04-21T23:47:02 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2024-04-21T23:47:16 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2024-04-21T23:47:20 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2024-04-21T23:47:20 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1400/1500, val/hateful_memes/cross_entropy: 1.4057, val/total_loss: 1.4057, val/hateful_memes/accuracy: 0.6796, val/hateful_memes/binary_f1: 0.4136, val/hateful_memes/roc_auc: 0.7041, num_updates: 1400, epoch: 11, iterations: 1400, max_updates: 1500, val_time: 23s 038ms, best_update: 1200, best_iteration: 1200, best_val/hateful_memes/roc_auc: 0.731544\n","\u001b[32m2024-04-21T23:49:20 | mmf.trainers.core.training_loop: \u001b[0mStepping into final validation check\n","\u001b[32m2024-04-21T23:49:20 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2024-04-21T23:49:20 | py.warnings: \u001b[0m/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n","  self.pid = os.fork()\n","\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2024-04-21T23:49:20 | py.warnings: \u001b[0m/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n","  self.pid = os.fork()\n","\n","\u001b[32m2024-04-21T23:49:25 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished evaluation inference. Loaded 9\n","\u001b[32m2024-04-21T23:49:25 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n","\u001b[32m2024-04-21T23:49:25 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1500/1500, val/hateful_memes/cross_entropy: 1.3213, val/total_loss: 1.3213, val/hateful_memes/accuracy: 0.6907, val/hateful_memes/binary_f1: 0.4732, val/hateful_memes/roc_auc: 0.7360, num_updates: 1500, epoch: 12, iterations: 1500, max_updates: 1500, val_time: 02m 27s 629ms, best_update: 1200, best_iteration: 1200, best_val/hateful_memes/roc_auc: 0.731544\n","\u001b[32m2024-04-21T23:49:25 | mmf.utils.checkpoint: \u001b[0mRestoring checkpoint\n","\u001b[32m2024-04-21T23:49:25 | mmf.utils.checkpoint: \u001b[0mLoading checkpoint\n","\u001b[32m2024-04-21T23:49:26 | mmf.utils.checkpoint: \u001b[0mCheckpoint loaded.\n","\u001b[32m2024-04-21T23:49:26 | mmf.utils.checkpoint: \u001b[0mCurrent num updates: 1200\n","\u001b[32m2024-04-21T23:49:26 | mmf.utils.checkpoint: \u001b[0mCurrent iteration: 1200\n","\u001b[32m2024-04-21T23:49:26 | mmf.utils.checkpoint: \u001b[0mCurrent epoch: 9\n","\u001b[32m2024-04-21T23:49:27 | mmf.trainers.mmf_trainer: \u001b[0mStarting inference on val set\n","\u001b[32m2024-04-21T23:49:27 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n","  0% 0/9 [00:00<?, ?it/s]\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2024-04-21T23:49:27 | py.warnings: \u001b[0m/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n","  self.pid = os.fork()\n","\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2024-04-21T23:49:27 | py.warnings: \u001b[0m/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n","  self.pid = os.fork()\n","\n","100% 9/9 [00:04<00:00,  2.13it/s]\n","\u001b[32m2024-04-21T23:49:31 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished evaluation inference. Loaded 9\n","\u001b[32m2024-04-21T23:49:31 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n","\u001b[32m2024-04-21T23:49:31 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1200/1500, val/hateful_memes/cross_entropy: 1.3820, val/total_loss: 1.3820, val/hateful_memes/accuracy: 0.6907, val/hateful_memes/binary_f1: 0.4452, val/hateful_memes/roc_auc: 0.7315\n","\u001b[32m2024-04-21T23:49:31 | mmf.trainers.callbacks.logistics: \u001b[0mFinished run in 33m 18s 839ms\n"]}]},{"cell_type":"markdown","source":["### Dataset: Augmented with partial transformations"],"metadata":{"id":"afXZ1LIaYASl"}},{"cell_type":"code","source":["!mmf_run config=\"./configs/defaults.yaml\" \\\n","  model=visual_bert \\\n","  dataset=hateful_memes \\\n","  run_type=train_val \\\n","  training.log_interval=200 \\\n","  training.batch_size=64 \\\n","  training.evaluation_interval=200 \\\n","  training.tensorboard=True \\\n","  training.checkpoint_interval=200 \\\n","  checkpoint.resume_pretrained=True \\\n","  checkpoint.resume_zoo=visual_bert.pretrained.coco \\\n","  dataset_config.hateful_memes.annotations.train[0]=\"hateful_memes/defaults/annotations/train_dev_transform_half.jsonl\" \\\n","  dataset_config.hateful_memes.annotations.val[0]=\"hateful_memes/defaults/annotations/dev_unseen.jsonl\" \\\n","  dataset_config.hateful_memes.annotations.test[0]=\"hateful_memes/defaults/annotations/test_unseen.jsonl\""],"metadata":{"id":"UrnxjaO1VoXn","colab":{"base_uri":"https://localhost:8080/"},"outputId":"5f3ba37b-572c-4846-c184-c0b9cf94ba32"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["2024-04-22 13:56:53.939928: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","2024-04-22 13:56:53.939983: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","2024-04-22 13:56:53.941702: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","2024-04-22 13:56:55.124889: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","2024-04-22 13:57:01.451812: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:47] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n","/usr/local/lib/python3.10/dist-packages/transformers/deepspeed.py:23: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/omegaconf/grammar_visitor.py:253: UserWarning: In the sequence `MMF_USER_DIR,` some elements are missing: please replace them with empty quoted strings. See https://github.com/omry/omegaconf/issues/572 for details.\n","  warnings.warn(\n","\u001b[32m2024-04-22T13:57:07 | mmf.utils.configuration: \u001b[0mOverriding option config to ./configs/defaults.yaml\n","\u001b[32m2024-04-22T13:57:07 | mmf.utils.configuration: \u001b[0mOverriding option model to visual_bert\n","\u001b[32m2024-04-22T13:57:07 | mmf.utils.configuration: \u001b[0mOverriding option datasets to hateful_memes\n","\u001b[32m2024-04-22T13:57:07 | mmf.utils.configuration: \u001b[0mOverriding option run_type to train_val\n","\u001b[32m2024-04-22T13:57:07 | mmf.utils.configuration: \u001b[0mOverriding option training.log_interval to 200\n","\u001b[32m2024-04-22T13:57:07 | mmf.utils.configuration: \u001b[0mOverriding option training.batch_size to 64\n","\u001b[32m2024-04-22T13:57:07 | mmf.utils.configuration: \u001b[0mOverriding option training.evaluation_interval to 200\n","\u001b[32m2024-04-22T13:57:07 | mmf.utils.configuration: \u001b[0mOverriding option training.tensorboard to True\n","\u001b[32m2024-04-22T13:57:07 | mmf.utils.configuration: \u001b[0mOverriding option training.checkpoint_interval to 200\n","\u001b[32m2024-04-22T13:57:07 | mmf.utils.configuration: \u001b[0mOverriding option checkpoint.resume_pretrained to True\n","\u001b[32m2024-04-22T13:57:07 | mmf.utils.configuration: \u001b[0mOverriding option checkpoint.resume_zoo to visual_bert.pretrained.coco\n","\u001b[32m2024-04-22T13:57:07 | mmf.utils.configuration: \u001b[0mOverriding option dataset_config.hateful_memes.annotations.train[0] to hateful_memes/defaults/annotations/train_dev_transform_half.jsonl\n","\u001b[32m2024-04-22T13:57:07 | mmf.utils.configuration: \u001b[0mOverriding option dataset_config.hateful_memes.annotations.val[0] to hateful_memes/defaults/annotations/dev_unseen.jsonl\n","\u001b[32m2024-04-22T13:57:07 | mmf.utils.configuration: \u001b[0mOverriding option dataset_config.hateful_memes.annotations.test[0] to hateful_memes/defaults/annotations/test_unseen.jsonl\n","/usr/local/lib/python3.10/dist-packages/omegaconf/grammar_visitor.py:253: UserWarning: In the sequence `MMF_LOG_DIR,` some elements are missing: please replace them with empty quoted strings. See https://github.com/omry/omegaconf/issues/572 for details.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/omegaconf/grammar_visitor.py:253: UserWarning: In the sequence `MMF_REPORT_DIR,` some elements are missing: please replace them with empty quoted strings. See https://github.com/omry/omegaconf/issues/572 for details.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/omegaconf/grammar_visitor.py:253: UserWarning: In the sequence `MMF_TENSORBOARD_LOGDIR,` some elements are missing: please replace them with empty quoted strings. See https://github.com/omry/omegaconf/issues/572 for details.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/omegaconf/grammar_visitor.py:253: UserWarning: In the sequence `MMF_WANDB_LOGDIR,` some elements are missing: please replace them with empty quoted strings. See https://github.com/omry/omegaconf/issues/572 for details.\n","  warnings.warn(\n","\u001b[32m2024-04-22T13:57:07 | mmf: \u001b[0mLogging to: ./save/train.log\n","\u001b[32m2024-04-22T13:57:07 | mmf_cli.run: \u001b[0mNamespace(config_override=None, local_rank=None, opts=['config=./configs/defaults.yaml', 'model=visual_bert', 'dataset=hateful_memes', 'run_type=train_val', 'training.log_interval=200', 'training.batch_size=64', 'training.evaluation_interval=200', 'training.tensorboard=True', 'training.checkpoint_interval=200', 'checkpoint.resume_pretrained=True', 'checkpoint.resume_zoo=visual_bert.pretrained.coco', 'dataset_config.hateful_memes.annotations.train[0]=hateful_memes/defaults/annotations/train_dev_transform_half.jsonl', 'dataset_config.hateful_memes.annotations.val[0]=hateful_memes/defaults/annotations/dev_unseen.jsonl', 'dataset_config.hateful_memes.annotations.test[0]=hateful_memes/defaults/annotations/test_unseen.jsonl'])\n","\u001b[32m2024-04-22T13:57:07 | mmf_cli.run: \u001b[0mTorch version: 2.2.0+cu121\n","\u001b[32m2024-04-22T13:57:07 | mmf.utils.general: \u001b[0mCUDA Device 0 is: NVIDIA L4\n","\u001b[32m2024-04-22T13:57:07 | mmf_cli.run: \u001b[0mUsing seed 7528337\n","\u001b[32m2024-04-22T13:57:07 | mmf.trainers.mmf_trainer: \u001b[0mLoading datasets\n","tokenizer_config.json: 100% 48.0/48.0 [00:00<00:00, 291kB/s]\n","config.json: 100% 570/570 [00:00<00:00, 4.38MB/s]\n","loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--bert-base-uncased/snapshots/86b5e0934494bd15c9632b12f734a8a67f723594/config.json\n","Model config BertConfig {\n","  \"_name_or_path\": \"bert-base-uncased\",\n","  \"architectures\": [\n","    \"BertForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.38.2\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n","vocab.txt: 100% 232k/232k [00:00<00:00, 3.23MB/s]\n","tokenizer.json: 100% 466k/466k [00:00<00:00, 3.56MB/s]\n","loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--bert-base-uncased/snapshots/86b5e0934494bd15c9632b12f734a8a67f723594/vocab.txt\n","loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--bert-base-uncased/snapshots/86b5e0934494bd15c9632b12f734a8a67f723594/tokenizer.json\n","loading file added_tokens.json from cache at None\n","loading file special_tokens_map.json from cache at None\n","loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--bert-base-uncased/snapshots/86b5e0934494bd15c9632b12f734a8a67f723594/tokenizer_config.json\n","loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--bert-base-uncased/snapshots/86b5e0934494bd15c9632b12f734a8a67f723594/config.json\n","Model config BertConfig {\n","  \"_name_or_path\": \"bert-base-uncased\",\n","  \"architectures\": [\n","    \"BertForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.38.2\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n","\u001b[32m2024-04-22T13:57:08 | mmf.datasets.multi_datamodule: \u001b[0mMultitasking disabled by default for single dataset training\n","\u001b[32m2024-04-22T13:57:08 | mmf.datasets.multi_datamodule: \u001b[0mMultitasking disabled by default for single dataset training\n","\u001b[32m2024-04-22T13:57:08 | mmf.datasets.multi_datamodule: \u001b[0mMultitasking disabled by default for single dataset training\n","\u001b[32m2024-04-22T13:57:08 | mmf.trainers.mmf_trainer: \u001b[0mLoading model\n","Model config BertConfig {\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bert_model_name\": \"bert-base-uncased\",\n","  \"bypass_transformer\": false,\n","  \"classifier_dropout\": null,\n","  \"embedding_strategy\": \"plain\",\n","  \"finetune_lr_multiplier\": 1,\n","  \"freeze_base\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"losses\": [\n","    \"cross_entropy\"\n","  ],\n","  \"max_position_embeddings\": 512,\n","  \"model\": \"visual_bert\",\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"pooler_strategy\": \"default\",\n","  \"position_embedding_type\": \"absolute\",\n","  \"random_initialize\": false,\n","  \"special_visual_initialize\": true,\n","  \"training_head_type\": \"classification\",\n","  \"transformers_version\": \"4.38.2\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"visual_embedding_dim\": 2048,\n","  \"vocab_size\": 30522,\n","  \"zerobias\": false\n","}\n","\n","model.safetensors: 100% 440M/440M [00:01<00:00, 299MB/s]\n","loading weights file model.safetensors from cache at /root/.cache/torch/mmf/distributed_-1/models--bert-base-uncased/snapshots/86b5e0934494bd15c9632b12f734a8a67f723594/model.safetensors\n","Some weights of the model checkpoint at bert-base-uncased were not used when initializing VisualBERTBase: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n","- This IS expected if you are initializing VisualBERTBase from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing VisualBERTBase from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of VisualBERTBase were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['bert.embeddings.position_embeddings_visual.weight', 'bert.embeddings.projection.bias', 'bert.embeddings.projection.weight', 'bert.embeddings.token_type_embeddings_visual.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","\u001b[32m2024-04-22T13:57:11 | mmf.trainers.mmf_trainer: \u001b[0mLoading optimizer\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2024-04-22T13:57:11 | py.warnings: \u001b[0m/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n","\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2024-04-22T13:57:11 | py.warnings: \u001b[0m/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n","\n","\u001b[32m2024-04-22T13:57:11 | mmf.trainers.mmf_trainer: \u001b[0mLoading metrics\n","\u001b[32m2024-04-22T13:57:11 | mmf.utils.checkpoint: \u001b[0mLoading checkpoint\n","[ Downloading: https://dl.fbaipublicfiles.com/mmf/data/models/visual_bert/visual_bert.pretrained.coco_train_val.tar.gz to /root/.cache/torch/mmf/data/models/visual_bert.pretrained.coco.defaults/visual_bert.pretrained.coco_train_val.tar.gz ]\n","Downloading visual_bert.pretrained.coco_train_val.tar.gz: 100% 415M/415M [00:02<00:00, 169MB/s]\n","[ Starting checksum for visual_bert.pretrained.coco_train_val.tar.gz]\n","[ Checksum successful for visual_bert.pretrained.coco_train_val.tar.gz]\n","Unpacking visual_bert.pretrained.coco_train_val.tar.gz\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2024-04-22T13:57:19 | mmf: \u001b[0mKey data_parallel is not present in registry, returning default value of None\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2024-04-22T13:57:19 | mmf: \u001b[0mKey distributed is not present in registry, returning default value of None\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2024-04-22T13:57:19 | mmf: \u001b[0mKey data_parallel is not present in registry, returning default value of None\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2024-04-22T13:57:19 | mmf: \u001b[0mKey distributed is not present in registry, returning default value of None\n","\u001b[32m2024-04-22T13:57:19 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.word_embeddings.weight from model.bert.embeddings.word_embeddings.weight\n","\u001b[32m2024-04-22T13:57:19 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.position_embeddings.weight from model.bert.embeddings.position_embeddings.weight\n","\u001b[32m2024-04-22T13:57:19 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.token_type_embeddings.weight from model.bert.embeddings.token_type_embeddings.weight\n","\u001b[32m2024-04-22T13:57:19 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.LayerNorm.weight from model.bert.embeddings.LayerNorm.weight\n","\u001b[32m2024-04-22T13:57:19 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.LayerNorm.bias from model.bert.embeddings.LayerNorm.bias\n","\u001b[32m2024-04-22T13:57:19 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.token_type_embeddings_visual.weight from model.bert.embeddings.token_type_embeddings_visual.weight\n","\u001b[32m2024-04-22T13:57:19 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.position_embeddings_visual.weight from model.bert.embeddings.position_embeddings_visual.weight\n","\u001b[32m2024-04-22T13:57:19 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.projection.weight from model.bert.embeddings.projection.weight\n","\u001b[32m2024-04-22T13:57:19 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.projection.bias from model.bert.embeddings.projection.bias\n","\u001b[32m2024-04-22T13:57:19 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.self.query.weight from model.bert.encoder.layer.0.attention.self.query.weight\n","\u001b[32m2024-04-22T13:57:19 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.self.query.bias from model.bert.encoder.layer.0.attention.self.query.bias\n","\u001b[32m2024-04-22T13:57:19 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.self.key.weight from model.bert.encoder.layer.0.attention.self.key.weight\n","\u001b[32m2024-04-22T13:57:19 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.self.key.bias from model.bert.encoder.layer.0.attention.self.key.bias\n","\u001b[32m2024-04-22T13:57:19 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.self.value.weight from model.bert.encoder.layer.0.attention.self.value.weight\n","\u001b[32m2024-04-22T13:57:19 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.self.value.bias from model.bert.encoder.layer.0.attention.self.value.bias\n","\u001b[32m2024-04-22T13:57:19 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.output.dense.weight from model.bert.encoder.layer.0.attention.output.dense.weight\n","\u001b[32m2024-04-22T13:57:19 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.output.dense.bias from model.bert.encoder.layer.0.attention.output.dense.bias\n","\u001b[32m2024-04-22T13:57:19 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.output.LayerNorm.weight from model.bert.encoder.layer.0.attention.output.LayerNorm.weight\n","\u001b[32m2024-04-22T13:57:19 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.output.LayerNorm.bias from model.bert.encoder.layer.0.attention.output.LayerNorm.bias\n","\u001b[32m2024-04-22T13:57:19 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.intermediate.dense.weight from model.bert.encoder.layer.0.intermediate.dense.weight\n","\u001b[32m2024-04-22T13:57:19 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.intermediate.dense.bias from model.bert.encoder.layer.0.intermediate.dense.bias\n","\u001b[32m2024-04-22T13:57:19 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.output.dense.weight from model.bert.encoder.layer.0.output.dense.weight\n","\u001b[32m2024-04-22T13:57:19 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.output.dense.bias from model.bert.encoder.layer.0.output.dense.bias\n","\u001b[32m2024-04-22T13:57:19 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.output.LayerNorm.weight from model.bert.encoder.layer.0.output.LayerNorm.weight\n","\u001b[32m2024-04-22T13:57:19 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.output.LayerNorm.bias from model.bert.encoder.layer.0.output.LayerNorm.bias\n","\u001b[32m2024-04-22T13:57:19 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.self.query.weight from model.bert.encoder.layer.1.attention.self.query.weight\n","\u001b[32m2024-04-22T13:57:19 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.self.query.bias from model.bert.encoder.layer.1.attention.self.query.bias\n","\u001b[32m2024-04-22T13:57:19 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.self.key.weight from model.bert.encoder.layer.1.attention.self.key.weight\n","\u001b[32m2024-04-22T13:57:19 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.self.key.bias from model.bert.encoder.layer.1.attention.self.key.bias\n","\u001b[32m2024-04-22T13:57:19 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.self.value.weight from model.bert.encoder.layer.1.attention.self.value.weight\n","\u001b[32m2024-04-22T13:57:19 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.self.value.bias from model.bert.encoder.layer.1.attention.self.value.bias\n","\u001b[32m2024-04-22T13:57:19 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.output.dense.weight from model.bert.encoder.layer.1.attention.output.dense.weight\n","\u001b[32m2024-04-22T13:57:19 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.output.dense.bias from model.bert.encoder.layer.1.attention.output.dense.bias\n","\u001b[32m2024-04-22T13:57:19 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.output.LayerNorm.weight from model.bert.encoder.layer.1.attention.output.LayerNorm.weight\n","\u001b[32m2024-04-22T13:57:19 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.output.LayerNorm.bias from model.bert.encoder.layer.1.attention.output.LayerNorm.bias\n","\u001b[32m2024-04-22T13:57:19 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.intermediate.dense.weight from model.bert.encoder.layer.1.intermediate.dense.weight\n","\u001b[32m2024-04-22T13:57:19 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.intermediate.dense.bias from model.bert.encoder.layer.1.intermediate.dense.bias\n","\u001b[32m2024-04-22T13:57:19 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.output.dense.weight from model.bert.encoder.layer.1.output.dense.weight\n","\u001b[32m2024-04-22T13:57:19 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.output.dense.bias from model.bert.encoder.layer.1.output.dense.bias\n","\u001b[32m2024-04-22T13:57:19 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.output.LayerNorm.weight from model.bert.encoder.layer.1.output.LayerNorm.weight\n","\u001b[32m2024-04-22T13:57:19 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.output.LayerNorm.bias from model.bert.encoder.layer.1.output.LayerNorm.bias\n","\u001b[32m2024-04-22T13:57:19 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.self.query.weight from model.bert.encoder.layer.2.attention.self.query.weight\n","\u001b[32m2024-04-22T13:57:19 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.self.query.bias from model.bert.encoder.layer.2.attention.self.query.bias\n","\u001b[32m2024-04-22T13:57:19 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.self.key.weight from model.bert.encoder.layer.2.attention.self.key.weight\n","\u001b[32m2024-04-22T13:57:19 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.self.key.bias from model.bert.encoder.layer.2.attention.self.key.bias\n","\u001b[32m2024-04-22T13:57:19 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.self.value.weight from model.bert.encoder.layer.2.attention.self.value.weight\n","\u001b[32m2024-04-22T13:57:19 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.self.value.bias from model.bert.encoder.layer.2.attention.self.value.bias\n","\u001b[32m2024-04-22T13:57:19 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.output.dense.weight from model.bert.encoder.layer.2.attention.output.dense.weight\n","\u001b[32m2024-04-22T13:57:19 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.output.dense.bias from model.bert.encoder.layer.2.attention.output.dense.bias\n","\u001b[32m2024-04-22T13:57:19 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.output.LayerNorm.weight from model.bert.encoder.layer.2.attention.output.LayerNorm.weight\n","\u001b[32m2024-04-22T13:57:19 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.output.LayerNorm.bias from model.bert.encoder.layer.2.attention.output.LayerNorm.bias\n","\u001b[32m2024-04-22T13:57:19 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.intermediate.dense.weight from model.bert.encoder.layer.2.intermediate.dense.weight\n","\u001b[32m2024-04-22T13:57:19 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.intermediate.dense.bias from model.bert.encoder.layer.2.intermediate.dense.bias\n","\u001b[32m2024-04-22T13:57:19 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.output.dense.weight from model.bert.encoder.layer.2.output.dense.weight\n","\u001b[32m2024-04-22T13:57:19 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.output.dense.bias from model.bert.encoder.layer.2.output.dense.bias\n","\u001b[32m2024-04-22T13:57:19 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.output.LayerNorm.weight from model.bert.encoder.layer.2.output.LayerNorm.weight\n","\u001b[32m2024-04-22T13:57:19 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.output.LayerNorm.bias from model.bert.encoder.layer.2.output.LayerNorm.bias\n","\u001b[32m2024-04-22T13:57:19 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.self.query.weight from model.bert.encoder.layer.3.attention.self.query.weight\n","\u001b[32m2024-04-22T13:57:19 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.self.query.bias from model.bert.encoder.layer.3.attention.self.query.bias\n","\u001b[32m2024-04-22T13:57:19 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.self.key.weight from model.bert.encoder.layer.3.attention.self.key.weight\n","\u001b[32m2024-04-22T13:57:19 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.self.key.bias from model.bert.encoder.layer.3.attention.self.key.bias\n","\u001b[32m2024-04-22T13:57:19 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.self.value.weight from model.bert.encoder.layer.3.attention.self.value.weight\n","\u001b[32m2024-04-22T13:57:19 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.self.value.bias from model.bert.encoder.layer.3.attention.self.value.bias\n","\u001b[32m2024-04-22T13:57:19 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.output.dense.weight from model.bert.encoder.layer.3.attention.output.dense.weight\n","\u001b[32m2024-04-22T13:57:19 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.output.dense.bias from model.bert.encoder.layer.3.attention.output.dense.bias\n","\u001b[32m2024-04-22T13:57:19 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.output.LayerNorm.weight from model.bert.encoder.layer.3.attention.output.LayerNorm.weight\n","\u001b[32m2024-04-22T13:57:19 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.output.LayerNorm.bias from model.bert.encoder.layer.3.attention.output.LayerNorm.bias\n","\u001b[32m2024-04-22T13:57:19 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.intermediate.dense.weight from model.bert.encoder.layer.3.intermediate.dense.weight\n","\u001b[32m2024-04-22T13:57:19 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.intermediate.dense.bias from model.bert.encoder.layer.3.intermediate.dense.bias\n","\u001b[32m2024-04-22T13:57:19 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.output.dense.weight from model.bert.encoder.layer.3.output.dense.weight\n","\u001b[32m2024-04-22T13:57:19 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.output.dense.bias from model.bert.encoder.layer.3.output.dense.bias\n","\u001b[32m2024-04-22T13:57:19 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.output.LayerNorm.weight from model.bert.encoder.layer.3.output.LayerNorm.weight\n","\u001b[32m2024-04-22T13:57:19 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.output.LayerNorm.bias from model.bert.encoder.layer.3.output.LayerNorm.bias\n","\u001b[32m2024-04-22T13:57:19 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.self.query.weight from model.bert.encoder.layer.4.attention.self.query.weight\n","\u001b[32m2024-04-22T13:57:19 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.self.query.bias from model.bert.encoder.layer.4.attention.self.query.bias\n","\u001b[32m2024-04-22T13:57:19 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.self.key.weight from model.bert.encoder.layer.4.attention.self.key.weight\n","\u001b[32m2024-04-22T13:57:19 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.self.key.bias from model.bert.encoder.layer.4.attention.self.key.bias\n","\u001b[32m2024-04-22T13:57:19 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.self.value.weight from model.bert.encoder.layer.4.attention.self.value.weight\n","\u001b[32m2024-04-22T13:57:19 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.self.value.bias from model.bert.encoder.layer.4.attention.self.value.bias\n","\u001b[32m2024-04-22T13:57:19 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.output.dense.weight from model.bert.encoder.layer.4.attention.output.dense.weight\n","\u001b[32m2024-04-22T13:57:19 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.output.dense.bias from model.bert.encoder.layer.4.attention.output.dense.bias\n","\u001b[32m2024-04-22T13:57:19 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.output.LayerNorm.weight from model.bert.encoder.layer.4.attention.output.LayerNorm.weight\n","\u001b[32m2024-04-22T13:57:19 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.output.LayerNorm.bias from model.bert.encoder.layer.4.attention.output.LayerNorm.bias\n","\u001b[32m2024-04-22T13:57:19 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.intermediate.dense.weight from model.bert.encoder.layer.4.intermediate.dense.weight\n","\u001b[32m2024-04-22T13:57:19 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.intermediate.dense.bias from model.bert.encoder.layer.4.intermediate.dense.bias\n","\u001b[32m2024-04-22T13:57:19 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.output.dense.weight from model.bert.encoder.layer.4.output.dense.weight\n","\u001b[32m2024-04-22T13:57:19 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.output.dense.bias from model.bert.encoder.layer.4.output.dense.bias\n","\u001b[32m2024-04-22T13:57:19 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.output.LayerNorm.weight from model.bert.encoder.layer.4.output.LayerNorm.weight\n","\u001b[32m2024-04-22T13:57:19 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.output.LayerNorm.bias from model.bert.encoder.layer.4.output.LayerNorm.bias\n","\u001b[32m2024-04-22T13:57:19 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.self.query.weight from model.bert.encoder.layer.5.attention.self.query.weight\n","\u001b[32m2024-04-22T13:57:19 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.self.query.bias from model.bert.encoder.layer.5.attention.self.query.bias\n","\u001b[32m2024-04-22T13:57:19 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.self.key.weight from model.bert.encoder.layer.5.attention.self.key.weight\n","\u001b[32m2024-04-22T13:57:19 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.self.key.bias from model.bert.encoder.layer.5.attention.self.key.bias\n","\u001b[32m2024-04-22T13:57:19 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.self.value.weight from model.bert.encoder.layer.5.attention.self.value.weight\n","\u001b[32m2024-04-22T13:57:19 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.self.value.bias from model.bert.encoder.layer.5.attention.self.value.bias\n","\u001b[32m2024-04-22T13:57:19 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.output.dense.weight from model.bert.encoder.layer.5.attention.output.dense.weight\n","\u001b[32m2024-04-22T13:57:19 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.output.dense.bias from model.bert.encoder.layer.5.attention.output.dense.bias\n","\u001b[32m2024-04-22T13:57:19 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.output.LayerNorm.weight from model.bert.encoder.layer.5.attention.output.LayerNorm.weight\n","\u001b[32m2024-04-22T13:57:19 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.output.LayerNorm.bias from model.bert.encoder.layer.5.attention.output.LayerNorm.bias\n","\u001b[32m2024-04-22T13:57:19 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.intermediate.dense.weight from model.bert.encoder.layer.5.intermediate.dense.weight\n","\u001b[32m2024-04-22T13:57:19 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.intermediate.dense.bias from model.bert.encoder.layer.5.intermediate.dense.bias\n","\u001b[32m2024-04-22T13:57:19 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.output.dense.weight from model.bert.encoder.layer.5.output.dense.weight\n","\u001b[32m2024-04-22T13:57:19 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.output.dense.bias from model.bert.encoder.layer.5.output.dense.bias\n","\u001b[32m2024-04-22T13:57:19 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.output.LayerNorm.weight from model.bert.encoder.layer.5.output.LayerNorm.weight\n","\u001b[32m2024-04-22T13:57:19 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.output.LayerNorm.bias from model.bert.encoder.layer.5.output.LayerNorm.bias\n","\u001b[32m2024-04-22T13:57:19 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.self.query.weight from model.bert.encoder.layer.6.attention.self.query.weight\n","\u001b[32m2024-04-22T13:57:19 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.self.query.bias from model.bert.encoder.layer.6.attention.self.query.bias\n","\u001b[32m2024-04-22T13:57:19 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.self.key.weight from model.bert.encoder.layer.6.attention.self.key.weight\n","\u001b[32m2024-04-22T13:57:19 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.self.key.bias from model.bert.encoder.layer.6.attention.self.key.bias\n","\u001b[32m2024-04-22T13:57:19 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.self.value.weight from model.bert.encoder.layer.6.attention.self.value.weight\n","\u001b[32m2024-04-22T13:57:19 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.self.value.bias from model.bert.encoder.layer.6.attention.self.value.bias\n","\u001b[32m2024-04-22T13:57:19 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.output.dense.weight from model.bert.encoder.layer.6.attention.output.dense.weight\n","\u001b[32m2024-04-22T13:57:19 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.output.dense.bias from model.bert.encoder.layer.6.attention.output.dense.bias\n","\u001b[32m2024-04-22T13:57:19 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.output.LayerNorm.weight from model.bert.encoder.layer.6.attention.output.LayerNorm.weight\n","\u001b[32m2024-04-22T13:57:19 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.output.LayerNorm.bias from model.bert.encoder.layer.6.attention.output.LayerNorm.bias\n","\u001b[32m2024-04-22T13:57:19 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.intermediate.dense.weight from model.bert.encoder.layer.6.intermediate.dense.weight\n","\u001b[32m2024-04-22T13:57:19 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.intermediate.dense.bias from model.bert.encoder.layer.6.intermediate.dense.bias\n","\u001b[32m2024-04-22T13:57:19 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.output.dense.weight from model.bert.encoder.layer.6.output.dense.weight\n","\u001b[32m2024-04-22T13:57:19 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.output.dense.bias from model.bert.encoder.layer.6.output.dense.bias\n","\u001b[32m2024-04-22T13:57:19 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.output.LayerNorm.weight from model.bert.encoder.layer.6.output.LayerNorm.weight\n","\u001b[32m2024-04-22T13:57:19 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.output.LayerNorm.bias from model.bert.encoder.layer.6.output.LayerNorm.bias\n","\u001b[32m2024-04-22T13:57:19 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.self.query.weight from model.bert.encoder.layer.7.attention.self.query.weight\n","\u001b[32m2024-04-22T13:57:19 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.self.query.bias from model.bert.encoder.layer.7.attention.self.query.bias\n","\u001b[32m2024-04-22T13:57:19 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.self.key.weight from model.bert.encoder.layer.7.attention.self.key.weight\n","\u001b[32m2024-04-22T13:57:19 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.self.key.bias from model.bert.encoder.layer.7.attention.self.key.bias\n","\u001b[32m2024-04-22T13:57:19 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.self.value.weight from model.bert.encoder.layer.7.attention.self.value.weight\n","\u001b[32m2024-04-22T13:57:19 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.self.value.bias from model.bert.encoder.layer.7.attention.self.value.bias\n","\u001b[32m2024-04-22T13:57:19 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.output.dense.weight from model.bert.encoder.layer.7.attention.output.dense.weight\n","\u001b[32m2024-04-22T13:57:19 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.output.dense.bias from model.bert.encoder.layer.7.attention.output.dense.bias\n","\u001b[32m2024-04-22T13:57:19 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.output.LayerNorm.weight from model.bert.encoder.layer.7.attention.output.LayerNorm.weight\n","\u001b[32m2024-04-22T13:57:19 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.output.LayerNorm.bias from model.bert.encoder.layer.7.attention.output.LayerNorm.bias\n","\u001b[32m2024-04-22T13:57:19 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.intermediate.dense.weight from model.bert.encoder.layer.7.intermediate.dense.weight\n","\u001b[32m2024-04-22T13:57:19 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.intermediate.dense.bias from model.bert.encoder.layer.7.intermediate.dense.bias\n","\u001b[32m2024-04-22T13:57:19 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.output.dense.weight from model.bert.encoder.layer.7.output.dense.weight\n","\u001b[32m2024-04-22T13:57:19 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.output.dense.bias from model.bert.encoder.layer.7.output.dense.bias\n","\u001b[32m2024-04-22T13:57:19 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.output.LayerNorm.weight from model.bert.encoder.layer.7.output.LayerNorm.weight\n","\u001b[32m2024-04-22T13:57:19 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.output.LayerNorm.bias from model.bert.encoder.layer.7.output.LayerNorm.bias\n","\u001b[32m2024-04-22T13:57:19 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.self.query.weight from model.bert.encoder.layer.8.attention.self.query.weight\n","\u001b[32m2024-04-22T13:57:19 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.self.query.bias from model.bert.encoder.layer.8.attention.self.query.bias\n","\u001b[32m2024-04-22T13:57:19 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.self.key.weight from model.bert.encoder.layer.8.attention.self.key.weight\n","\u001b[32m2024-04-22T13:57:19 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.self.key.bias from model.bert.encoder.layer.8.attention.self.key.bias\n","\u001b[32m2024-04-22T13:57:19 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.self.value.weight from model.bert.encoder.layer.8.attention.self.value.weight\n","\u001b[32m2024-04-22T13:57:19 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.self.value.bias from model.bert.encoder.layer.8.attention.self.value.bias\n","\u001b[32m2024-04-22T13:57:19 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.output.dense.weight from model.bert.encoder.layer.8.attention.output.dense.weight\n","\u001b[32m2024-04-22T13:57:19 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.output.dense.bias from model.bert.encoder.layer.8.attention.output.dense.bias\n","\u001b[32m2024-04-22T13:57:19 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.output.LayerNorm.weight from model.bert.encoder.layer.8.attention.output.LayerNorm.weight\n","\u001b[32m2024-04-22T13:57:19 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.output.LayerNorm.bias from model.bert.encoder.layer.8.attention.output.LayerNorm.bias\n","\u001b[32m2024-04-22T13:57:19 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.intermediate.dense.weight from model.bert.encoder.layer.8.intermediate.dense.weight\n","\u001b[32m2024-04-22T13:57:19 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.intermediate.dense.bias from model.bert.encoder.layer.8.intermediate.dense.bias\n","\u001b[32m2024-04-22T13:57:19 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.output.dense.weight from model.bert.encoder.layer.8.output.dense.weight\n","\u001b[32m2024-04-22T13:57:19 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.output.dense.bias from model.bert.encoder.layer.8.output.dense.bias\n","\u001b[32m2024-04-22T13:57:19 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.output.LayerNorm.weight from model.bert.encoder.layer.8.output.LayerNorm.weight\n","\u001b[32m2024-04-22T13:57:19 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.output.LayerNorm.bias from model.bert.encoder.layer.8.output.LayerNorm.bias\n","\u001b[32m2024-04-22T13:57:19 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.self.query.weight from model.bert.encoder.layer.9.attention.self.query.weight\n","\u001b[32m2024-04-22T13:57:19 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.self.query.bias from model.bert.encoder.layer.9.attention.self.query.bias\n","\u001b[32m2024-04-22T13:57:19 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.self.key.weight from model.bert.encoder.layer.9.attention.self.key.weight\n","\u001b[32m2024-04-22T13:57:19 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.self.key.bias from model.bert.encoder.layer.9.attention.self.key.bias\n","\u001b[32m2024-04-22T13:57:19 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.self.value.weight from model.bert.encoder.layer.9.attention.self.value.weight\n","\u001b[32m2024-04-22T13:57:19 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.self.value.bias from model.bert.encoder.layer.9.attention.self.value.bias\n","\u001b[32m2024-04-22T13:57:19 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.output.dense.weight from model.bert.encoder.layer.9.attention.output.dense.weight\n","\u001b[32m2024-04-22T13:57:19 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.output.dense.bias from model.bert.encoder.layer.9.attention.output.dense.bias\n","\u001b[32m2024-04-22T13:57:19 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.output.LayerNorm.weight from model.bert.encoder.layer.9.attention.output.LayerNorm.weight\n","\u001b[32m2024-04-22T13:57:19 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.output.LayerNorm.bias from model.bert.encoder.layer.9.attention.output.LayerNorm.bias\n","\u001b[32m2024-04-22T13:57:19 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.intermediate.dense.weight from model.bert.encoder.layer.9.intermediate.dense.weight\n","\u001b[32m2024-04-22T13:57:19 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.intermediate.dense.bias from model.bert.encoder.layer.9.intermediate.dense.bias\n","\u001b[32m2024-04-22T13:57:19 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.output.dense.weight from model.bert.encoder.layer.9.output.dense.weight\n","\u001b[32m2024-04-22T13:57:19 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.output.dense.bias from model.bert.encoder.layer.9.output.dense.bias\n","\u001b[32m2024-04-22T13:57:19 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.output.LayerNorm.weight from model.bert.encoder.layer.9.output.LayerNorm.weight\n","\u001b[32m2024-04-22T13:57:19 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.output.LayerNorm.bias from model.bert.encoder.layer.9.output.LayerNorm.bias\n","\u001b[32m2024-04-22T13:57:19 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.self.query.weight from model.bert.encoder.layer.10.attention.self.query.weight\n","\u001b[32m2024-04-22T13:57:19 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.self.query.bias from model.bert.encoder.layer.10.attention.self.query.bias\n","\u001b[32m2024-04-22T13:57:19 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.self.key.weight from model.bert.encoder.layer.10.attention.self.key.weight\n","\u001b[32m2024-04-22T13:57:19 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.self.key.bias from model.bert.encoder.layer.10.attention.self.key.bias\n","\u001b[32m2024-04-22T13:57:19 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.self.value.weight from model.bert.encoder.layer.10.attention.self.value.weight\n","\u001b[32m2024-04-22T13:57:19 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.self.value.bias from model.bert.encoder.layer.10.attention.self.value.bias\n","\u001b[32m2024-04-22T13:57:19 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.output.dense.weight from model.bert.encoder.layer.10.attention.output.dense.weight\n","\u001b[32m2024-04-22T13:57:19 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.output.dense.bias from model.bert.encoder.layer.10.attention.output.dense.bias\n","\u001b[32m2024-04-22T13:57:19 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.output.LayerNorm.weight from model.bert.encoder.layer.10.attention.output.LayerNorm.weight\n","\u001b[32m2024-04-22T13:57:19 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.output.LayerNorm.bias from model.bert.encoder.layer.10.attention.output.LayerNorm.bias\n","\u001b[32m2024-04-22T13:57:19 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.intermediate.dense.weight from model.bert.encoder.layer.10.intermediate.dense.weight\n","\u001b[32m2024-04-22T13:57:19 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.intermediate.dense.bias from model.bert.encoder.layer.10.intermediate.dense.bias\n","\u001b[32m2024-04-22T13:57:19 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.output.dense.weight from model.bert.encoder.layer.10.output.dense.weight\n","\u001b[32m2024-04-22T13:57:19 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.output.dense.bias from model.bert.encoder.layer.10.output.dense.bias\n","\u001b[32m2024-04-22T13:57:19 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.output.LayerNorm.weight from model.bert.encoder.layer.10.output.LayerNorm.weight\n","\u001b[32m2024-04-22T13:57:19 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.output.LayerNorm.bias from model.bert.encoder.layer.10.output.LayerNorm.bias\n","\u001b[32m2024-04-22T13:57:19 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.self.query.weight from model.bert.encoder.layer.11.attention.self.query.weight\n","\u001b[32m2024-04-22T13:57:19 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.self.query.bias from model.bert.encoder.layer.11.attention.self.query.bias\n","\u001b[32m2024-04-22T13:57:19 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.self.key.weight from model.bert.encoder.layer.11.attention.self.key.weight\n","\u001b[32m2024-04-22T13:57:19 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.self.key.bias from model.bert.encoder.layer.11.attention.self.key.bias\n","\u001b[32m2024-04-22T13:57:19 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.self.value.weight from model.bert.encoder.layer.11.attention.self.value.weight\n","\u001b[32m2024-04-22T13:57:19 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.self.value.bias from model.bert.encoder.layer.11.attention.self.value.bias\n","\u001b[32m2024-04-22T13:57:19 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.output.dense.weight from model.bert.encoder.layer.11.attention.output.dense.weight\n","\u001b[32m2024-04-22T13:57:19 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.output.dense.bias from model.bert.encoder.layer.11.attention.output.dense.bias\n","\u001b[32m2024-04-22T13:57:19 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.output.LayerNorm.weight from model.bert.encoder.layer.11.attention.output.LayerNorm.weight\n","\u001b[32m2024-04-22T13:57:19 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.output.LayerNorm.bias from model.bert.encoder.layer.11.attention.output.LayerNorm.bias\n","\u001b[32m2024-04-22T13:57:19 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.intermediate.dense.weight from model.bert.encoder.layer.11.intermediate.dense.weight\n","\u001b[32m2024-04-22T13:57:19 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.intermediate.dense.bias from model.bert.encoder.layer.11.intermediate.dense.bias\n","\u001b[32m2024-04-22T13:57:19 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.output.dense.weight from model.bert.encoder.layer.11.output.dense.weight\n","\u001b[32m2024-04-22T13:57:19 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.output.dense.bias from model.bert.encoder.layer.11.output.dense.bias\n","\u001b[32m2024-04-22T13:57:19 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.output.LayerNorm.weight from model.bert.encoder.layer.11.output.LayerNorm.weight\n","\u001b[32m2024-04-22T13:57:19 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.output.LayerNorm.bias from model.bert.encoder.layer.11.output.LayerNorm.bias\n","\u001b[32m2024-04-22T13:57:19 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.pooler.dense.weight from model.bert.pooler.dense.weight\n","\u001b[32m2024-04-22T13:57:19 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.pooler.dense.bias from model.bert.pooler.dense.bias\n","\u001b[32m2024-04-22T13:57:19 | mmf.utils.checkpoint: \u001b[0mPretrained model loaded\n","\u001b[32m2024-04-22T13:57:19 | mmf.utils.checkpoint: \u001b[0mCheckpoint loaded.\n","\u001b[32m2024-04-22T13:57:19 | mmf.utils.checkpoint: \u001b[0mCurrent num updates: 0\n","\u001b[32m2024-04-22T13:57:19 | mmf.utils.checkpoint: \u001b[0mCurrent iteration: 0\n","\u001b[32m2024-04-22T13:57:19 | mmf.utils.checkpoint: \u001b[0mCurrent epoch: 0\n","\u001b[32m2024-04-22T13:57:19 | mmf.trainers.mmf_trainer: \u001b[0m===== Model =====\n","\u001b[32m2024-04-22T13:57:19 | mmf.trainers.mmf_trainer: \u001b[0mVisualBERT(\n","  (model): VisualBERTForClassification(\n","    (bert): VisualBERTBase(\n","      (embeddings): BertVisioLinguisticEmbeddings(\n","        (word_embeddings): Embedding(30522, 768, padding_idx=0)\n","        (position_embeddings): Embedding(512, 768)\n","        (token_type_embeddings): Embedding(2, 768)\n","        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","        (token_type_embeddings_visual): Embedding(2, 768)\n","        (position_embeddings_visual): Embedding(512, 768)\n","        (projection): Linear(in_features=2048, out_features=768, bias=True)\n","      )\n","      (encoder): BertEncoderJit(\n","        (layer): ModuleList(\n","          (0-11): 12 x BertLayerJit(\n","            (attention): BertAttentionJit(\n","              (self): BertSelfAttentionJit(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","              (intermediate_act_fn): GELUActivation()\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","        )\n","      )\n","      (pooler): BertPooler(\n","        (dense): Linear(in_features=768, out_features=768, bias=True)\n","        (activation): Tanh()\n","      )\n","    )\n","    (dropout): Dropout(p=0.1, inplace=False)\n","    (classifier): Sequential(\n","      (0): BertPredictionHeadTransform(\n","        (dense): Linear(in_features=768, out_features=768, bias=True)\n","        (transform_act_fn): GELUActivation()\n","        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      )\n","      (1): Linear(in_features=768, out_features=2, bias=True)\n","    )\n","  )\n","  (losses): Losses(\n","    (losses): ModuleList(\n","      (0): MMFLoss(\n","        (loss_criterion): CrossEntropyLoss(\n","          (loss_fn): CrossEntropyLoss()\n","        )\n","      )\n","    )\n","  )\n",")\n","\u001b[32m2024-04-22T13:57:19 | mmf.utils.general: \u001b[0mTotal Parameters: 112044290. Trained Parameters: 112044290\n","\u001b[32m2024-04-22T13:57:19 | mmf.trainers.core.training_loop: \u001b[0mStarting training...\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2024-04-22T13:57:19 | py.warnings: \u001b[0m/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n","  self.pid = os.fork()\n","\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2024-04-22T13:57:19 | py.warnings: \u001b[0m/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n","  self.pid = os.fork()\n","\n","\u001b[32m2024-04-22T14:01:25 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n","\u001b[32m2024-04-22T14:01:25 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2024-04-22T14:01:28 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2024-04-22T14:01:31 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2024-04-22T14:01:31 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 200/1500, train/hateful_memes/cross_entropy: 0.5569, train/hateful_memes/cross_entropy/avg: 0.5569, train/total_loss: 0.5569, train/total_loss/avg: 0.5569, max mem: 14322.0, experiment: run, epoch: 1, num_updates: 200, iterations: 200, max_updates: 1500, lr: 0.00001, ups: 0.80, time: 04m 11s 311ms, time_since_start: 04m 20s 069ms, eta: 28m 27s 033ms\n","\u001b[32m2024-04-22T14:01:31 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n","\u001b[32m2024-04-22T14:01:31 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n","\u001b[32m2024-04-22T14:01:36 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished evaluation inference. Loaded 9\n","\u001b[32m2024-04-22T14:01:36 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n","\u001b[32m2024-04-22T14:01:36 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2024-04-22T14:01:39 | mmf.utils.checkpoint: \u001b[0mSaving best checkpoint\n","\u001b[32m2024-04-22T14:01:42 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2024-04-22T14:01:48 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2024-04-22T14:01:48 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 200/1500, val/hateful_memes/cross_entropy: 0.6666, val/total_loss: 0.6666, val/hateful_memes/accuracy: 0.5981, val/hateful_memes/binary_f1: 0.3673, val/hateful_memes/roc_auc: 0.5905, num_updates: 200, epoch: 1, iterations: 200, max_updates: 1500, val_time: 17s 758ms, best_update: 200, best_iteration: 200, best_val/hateful_memes/roc_auc: 0.590515\n","\u001b[32m2024-04-22T14:05:54 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n","\u001b[32m2024-04-22T14:05:54 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2024-04-22T14:05:56 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2024-04-22T14:06:02 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2024-04-22T14:06:02 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 400/1500, train/hateful_memes/cross_entropy: 0.4182, train/hateful_memes/cross_entropy/avg: 0.4876, train/total_loss: 0.4182, train/total_loss/avg: 0.4876, max mem: 14371.0, experiment: run, epoch: 2, num_updates: 400, iterations: 400, max_updates: 1500, lr: 0.00001, ups: 0.79, time: 04m 13s 578ms, time_since_start: 08m 51s 412ms, eta: 24m 17s 440ms\n","\u001b[32m2024-04-22T14:06:02 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n","\u001b[32m2024-04-22T14:06:02 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2024-04-22T14:06:02 | py.warnings: \u001b[0m/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n","  self.pid = os.fork()\n","\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2024-04-22T14:06:02 | py.warnings: \u001b[0m/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n","  self.pid = os.fork()\n","\n","\u001b[32m2024-04-22T14:06:06 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished evaluation inference. Loaded 9\n","\u001b[32m2024-04-22T14:06:06 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n","\u001b[32m2024-04-22T14:06:07 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2024-04-22T14:06:09 | mmf.utils.checkpoint: \u001b[0mSaving best checkpoint\n","\u001b[32m2024-04-22T14:06:15 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2024-04-22T14:06:18 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2024-04-22T14:06:18 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 400/1500, val/hateful_memes/cross_entropy: 0.8046, val/total_loss: 0.8046, val/hateful_memes/accuracy: 0.6537, val/hateful_memes/binary_f1: 0.3661, val/hateful_memes/roc_auc: 0.6584, num_updates: 400, epoch: 2, iterations: 400, max_updates: 1500, val_time: 15s 484ms, best_update: 400, best_iteration: 400, best_val/hateful_memes/roc_auc: 0.658397\n","\u001b[32m2024-04-22T14:10:25 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n","\u001b[32m2024-04-22T14:10:25 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2024-04-22T14:10:27 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2024-04-22T14:10:31 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2024-04-22T14:10:31 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 600/1500, train/hateful_memes/cross_entropy: 0.4182, train/hateful_memes/cross_entropy/avg: 0.4118, train/total_loss: 0.4182, train/total_loss/avg: 0.4118, max mem: 14371.0, experiment: run, epoch: 3, num_updates: 600, iterations: 600, max_updates: 1500, lr: 0.00002, ups: 0.79, time: 04m 13s 612ms, time_since_start: 13m 20s 511ms, eta: 19m 52s 611ms\n","\u001b[32m2024-04-22T14:10:31 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n","\u001b[32m2024-04-22T14:10:31 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2024-04-22T14:10:31 | py.warnings: \u001b[0m/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n","  self.pid = os.fork()\n","\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2024-04-22T14:10:31 | py.warnings: \u001b[0m/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n","  self.pid = os.fork()\n","\n","\u001b[32m2024-04-22T14:10:35 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished evaluation inference. Loaded 9\n","\u001b[32m2024-04-22T14:10:35 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n","\u001b[32m2024-04-22T14:10:36 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2024-04-22T14:10:38 | mmf.utils.checkpoint: \u001b[0mSaving best checkpoint\n","\u001b[32m2024-04-22T14:10:41 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2024-04-22T14:10:47 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2024-04-22T14:10:47 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 600/1500, val/hateful_memes/cross_entropy: 0.7754, val/total_loss: 0.7754, val/hateful_memes/accuracy: 0.6833, val/hateful_memes/binary_f1: 0.5315, val/hateful_memes/roc_auc: 0.7207, num_updates: 600, epoch: 3, iterations: 600, max_updates: 1500, val_time: 16s 103ms, best_update: 600, best_iteration: 600, best_val/hateful_memes/roc_auc: 0.720706\n","\u001b[32m2024-04-22T14:14:56 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n","\u001b[32m2024-04-22T14:14:56 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2024-04-22T14:14:58 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2024-04-22T14:15:04 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2024-04-22T14:15:04 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 800/1500, train/hateful_memes/cross_entropy: 0.2602, train/hateful_memes/cross_entropy/avg: 0.3470, train/total_loss: 0.2602, train/total_loss/avg: 0.3470, max mem: 14371.0, experiment: run, epoch: 3, num_updates: 800, iterations: 800, max_updates: 1500, lr: 0.00002, ups: 0.78, time: 04m 16s 746ms, time_since_start: 17m 53s 363ms, eta: 15m 39s 050ms\n","\u001b[32m2024-04-22T14:15:04 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n","\u001b[32m2024-04-22T14:15:04 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2024-04-22T14:15:04 | py.warnings: \u001b[0m/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n","  self.pid = os.fork()\n","\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2024-04-22T14:15:04 | py.warnings: \u001b[0m/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n","  self.pid = os.fork()\n","\n","\u001b[32m2024-04-22T14:15:08 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished evaluation inference. Loaded 9\n","\u001b[32m2024-04-22T14:15:08 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n","\u001b[32m2024-04-22T14:15:08 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2024-04-22T14:15:11 | mmf.utils.checkpoint: \u001b[0mSaving best checkpoint\n","\u001b[32m2024-04-22T14:15:16 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2024-04-22T14:15:21 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2024-04-22T14:15:21 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 800/1500, val/hateful_memes/cross_entropy: 0.8285, val/total_loss: 0.8285, val/hateful_memes/accuracy: 0.6852, val/hateful_memes/binary_f1: 0.5596, val/hateful_memes/roc_auc: 0.7248, num_updates: 800, epoch: 3, iterations: 800, max_updates: 1500, val_time: 17s 236ms, best_update: 800, best_iteration: 800, best_val/hateful_memes/roc_auc: 0.724827\n","\u001b[32m2024-04-22T14:19:27 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n","\u001b[32m2024-04-22T14:19:27 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2024-04-22T14:19:30 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2024-04-22T14:19:33 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2024-04-22T14:19:33 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1000/1500, train/hateful_memes/cross_entropy: 0.2602, train/hateful_memes/cross_entropy/avg: 0.3061, train/total_loss: 0.2602, train/total_loss/avg: 0.3061, max mem: 14371.0, experiment: run, epoch: 4, num_updates: 1000, iterations: 1000, max_updates: 1500, lr: 0.00003, ups: 0.80, time: 04m 11s 343ms, time_since_start: 22m 21s 945ms, eta: 10m 56s 635ms\n","\u001b[32m2024-04-22T14:19:33 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n","\u001b[32m2024-04-22T14:19:33 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2024-04-22T14:19:33 | py.warnings: \u001b[0m/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n","  self.pid = os.fork()\n","\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2024-04-22T14:19:33 | py.warnings: \u001b[0m/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n","  self.pid = os.fork()\n","\n","\u001b[32m2024-04-22T14:19:37 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished evaluation inference. Loaded 9\n","\u001b[32m2024-04-22T14:19:37 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n","\u001b[32m2024-04-22T14:19:37 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2024-04-22T14:19:40 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2024-04-22T14:19:45 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2024-04-22T14:19:45 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1000/1500, val/hateful_memes/cross_entropy: 0.9913, val/total_loss: 0.9913, val/hateful_memes/accuracy: 0.6722, val/hateful_memes/binary_f1: 0.4309, val/hateful_memes/roc_auc: 0.7199, num_updates: 1000, epoch: 4, iterations: 1000, max_updates: 1500, val_time: 12s 463ms, best_update: 800, best_iteration: 800, best_val/hateful_memes/roc_auc: 0.724827\n","\u001b[32m2024-04-22T14:23:53 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n","\u001b[32m2024-04-22T14:23:53 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2024-04-22T14:23:55 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2024-04-22T14:24:01 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2024-04-22T14:24:01 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1200/1500, train/hateful_memes/cross_entropy: 0.1527, train/hateful_memes/cross_entropy/avg: 0.2777, train/total_loss: 0.1527, train/total_loss/avg: 0.2777, max mem: 14372.0, experiment: run, epoch: 5, num_updates: 1200, iterations: 1200, max_updates: 1500, lr: 0.00003, ups: 0.78, time: 04m 15s 956ms, time_since_start: 26m 50s 366ms, eta: 06m 41s 211ms\n","\u001b[32m2024-04-22T14:24:01 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n","\u001b[32m2024-04-22T14:24:01 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2024-04-22T14:24:01 | py.warnings: \u001b[0m/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n","  self.pid = os.fork()\n","\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2024-04-22T14:24:01 | py.warnings: \u001b[0m/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n","  self.pid = os.fork()\n","\n","\u001b[32m2024-04-22T14:24:05 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished evaluation inference. Loaded 9\n","\u001b[32m2024-04-22T14:24:05 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n","\u001b[32m2024-04-22T14:24:06 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2024-04-22T14:24:08 | mmf.utils.checkpoint: \u001b[0mSaving best checkpoint\n","\u001b[32m2024-04-22T14:24:14 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2024-04-22T14:24:22 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2024-04-22T14:24:22 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1200/1500, val/hateful_memes/cross_entropy: 1.4570, val/total_loss: 1.4570, val/hateful_memes/accuracy: 0.6815, val/hateful_memes/binary_f1: 0.4028, val/hateful_memes/roc_auc: 0.7349, num_updates: 1200, epoch: 5, iterations: 1200, max_updates: 1500, val_time: 20s 650ms, best_update: 1200, best_iteration: 1200, best_val/hateful_memes/roc_auc: 0.734853\n"]}]},{"cell_type":"markdown","source":["### Dataset: Augmented with complete transformations"],"metadata":{"id":"w_ydFyhUYFwz"}},{"cell_type":"code","source":["!mmf_run config=\"./configs/defaults.yaml\" \\\n","  model=visual_bert \\\n","  dataset=hateful_memes \\\n","  run_type=train_val \\\n","  training.log_interval=200 \\\n","  training.batch_size=64 \\\n","  training.evaluation_interval=200 \\\n","  training.tensorboard=True \\\n","  training.checkpoint_interval=200 \\\n","  checkpoint.resume_pretrained=True \\\n","  checkpoint.resume_zoo=visual_bert.pretrained.coco \\\n","  dataset_config.hateful_memes.annotations.train[0]=\"hateful_memes/defaults/annotations/train_dev_transform_all.jsonl\" \\\n","  dataset_config.hateful_memes.annotations.val[0]=\"hateful_memes/defaults/annotations/dev_unseen.jsonl\" \\\n","  dataset_config.hateful_memes.annotations.test[0]=\"hateful_memes/defaults/annotations/test_unseen.jsonl\""],"metadata":{"id":"nsINJJQX8SA8","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1713981938293,"user_tz":420,"elapsed":2225064,"user":{"displayName":"Aneri Shah","userId":"14009776940163552827"}},"outputId":"ceda2c22-cb84-40a4-81a1-0b88c96c5926"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["2024-04-24 17:28:37.229214: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","2024-04-24 17:28:37.229279: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","2024-04-24 17:28:37.230849: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","2024-04-24 17:28:38.333522: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","2024-04-24 17:28:43.061966: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:47] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n","/usr/local/lib/python3.10/dist-packages/transformers/deepspeed.py:23: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/omegaconf/grammar_visitor.py:253: UserWarning: In the sequence `MMF_USER_DIR,` some elements are missing: please replace them with empty quoted strings. See https://github.com/omry/omegaconf/issues/572 for details.\n","  warnings.warn(\n","\u001b[32m2024-04-24T17:28:50 | mmf.utils.configuration: \u001b[0mOverriding option config to ./configs/defaults.yaml\n","\u001b[32m2024-04-24T17:28:50 | mmf.utils.configuration: \u001b[0mOverriding option model to visual_bert\n","\u001b[32m2024-04-24T17:28:50 | mmf.utils.configuration: \u001b[0mOverriding option datasets to hateful_memes\n","\u001b[32m2024-04-24T17:28:50 | mmf.utils.configuration: \u001b[0mOverriding option run_type to train_val\n","\u001b[32m2024-04-24T17:28:50 | mmf.utils.configuration: \u001b[0mOverriding option training.log_interval to 200\n","\u001b[32m2024-04-24T17:28:50 | mmf.utils.configuration: \u001b[0mOverriding option training.batch_size to 64\n","\u001b[32m2024-04-24T17:28:50 | mmf.utils.configuration: \u001b[0mOverriding option training.evaluation_interval to 200\n","\u001b[32m2024-04-24T17:28:50 | mmf.utils.configuration: \u001b[0mOverriding option training.tensorboard to True\n","\u001b[32m2024-04-24T17:28:50 | mmf.utils.configuration: \u001b[0mOverriding option training.checkpoint_interval to 200\n","\u001b[32m2024-04-24T17:28:50 | mmf.utils.configuration: \u001b[0mOverriding option checkpoint.resume_pretrained to True\n","\u001b[32m2024-04-24T17:28:50 | mmf.utils.configuration: \u001b[0mOverriding option checkpoint.resume_zoo to visual_bert.pretrained.coco\n","\u001b[32m2024-04-24T17:28:50 | mmf.utils.configuration: \u001b[0mOverriding option dataset_config.hateful_memes.annotations.train[0] to hateful_memes/defaults/annotations/train_dev_transform_all.jsonl\n","\u001b[32m2024-04-24T17:28:50 | mmf.utils.configuration: \u001b[0mOverriding option dataset_config.hateful_memes.annotations.val[0] to hateful_memes/defaults/annotations/dev_unseen.jsonl\n","\u001b[32m2024-04-24T17:28:50 | mmf.utils.configuration: \u001b[0mOverriding option dataset_config.hateful_memes.annotations.test[0] to hateful_memes/defaults/annotations/test_unseen.jsonl\n","/usr/local/lib/python3.10/dist-packages/omegaconf/grammar_visitor.py:253: UserWarning: In the sequence `MMF_LOG_DIR,` some elements are missing: please replace them with empty quoted strings. See https://github.com/omry/omegaconf/issues/572 for details.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/omegaconf/grammar_visitor.py:253: UserWarning: In the sequence `MMF_REPORT_DIR,` some elements are missing: please replace them with empty quoted strings. See https://github.com/omry/omegaconf/issues/572 for details.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/omegaconf/grammar_visitor.py:253: UserWarning: In the sequence `MMF_TENSORBOARD_LOGDIR,` some elements are missing: please replace them with empty quoted strings. See https://github.com/omry/omegaconf/issues/572 for details.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/omegaconf/grammar_visitor.py:253: UserWarning: In the sequence `MMF_WANDB_LOGDIR,` some elements are missing: please replace them with empty quoted strings. See https://github.com/omry/omegaconf/issues/572 for details.\n","  warnings.warn(\n","\u001b[32m2024-04-24T17:28:50 | mmf: \u001b[0mLogging to: ./save/train.log\n","\u001b[32m2024-04-24T17:28:50 | mmf_cli.run: \u001b[0mNamespace(config_override=None, local_rank=None, opts=['config=./configs/defaults.yaml', 'model=visual_bert', 'dataset=hateful_memes', 'run_type=train_val', 'training.log_interval=200', 'training.batch_size=64', 'training.evaluation_interval=200', 'training.tensorboard=True', 'training.checkpoint_interval=200', 'checkpoint.resume_pretrained=True', 'checkpoint.resume_zoo=visual_bert.pretrained.coco', 'dataset_config.hateful_memes.annotations.train[0]=hateful_memes/defaults/annotations/train_dev_transform_all.jsonl', 'dataset_config.hateful_memes.annotations.val[0]=hateful_memes/defaults/annotations/dev_unseen.jsonl', 'dataset_config.hateful_memes.annotations.test[0]=hateful_memes/defaults/annotations/test_unseen.jsonl'])\n","\u001b[32m2024-04-24T17:28:50 | mmf_cli.run: \u001b[0mTorch version: 2.2.0+cu121\n","\u001b[32m2024-04-24T17:28:50 | mmf.utils.general: \u001b[0mCUDA Device 0 is: NVIDIA L4\n","\u001b[32m2024-04-24T17:28:50 | mmf_cli.run: \u001b[0mUsing seed 50243609\n","\u001b[32m2024-04-24T17:28:50 | mmf.trainers.mmf_trainer: \u001b[0mLoading datasets\n","[ Downloading: https://dl.fbaipublicfiles.com/mmf/data/datasets/hateful_memes/defaults/features/features_2020_10_01.tar.gz to /root/.cache/torch/mmf/data/datasets/hateful_memes/defaults/features/features.tar.gz ]\n","Downloading features.tar.gz: 100% 10.3G/10.3G [00:53<00:00, 192MB/s]\n","[ Starting checksum for features.tar.gz]\n","[ Checksum successful for features.tar.gz]\n","Unpacking features.tar.gz\n","tokenizer_config.json: 100% 48.0/48.0 [00:00<00:00, 178kB/s]\n","config.json: 100% 570/570 [00:00<00:00, 4.13MB/s]\n","loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--bert-base-uncased/snapshots/86b5e0934494bd15c9632b12f734a8a67f723594/config.json\n","Model config BertConfig {\n","  \"_name_or_path\": \"bert-base-uncased\",\n","  \"architectures\": [\n","    \"BertForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.39.2\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n","vocab.txt: 100% 232k/232k [00:00<00:00, 3.20MB/s]\n","tokenizer.json: 100% 466k/466k [00:00<00:00, 3.59MB/s]\n","loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--bert-base-uncased/snapshots/86b5e0934494bd15c9632b12f734a8a67f723594/vocab.txt\n","loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--bert-base-uncased/snapshots/86b5e0934494bd15c9632b12f734a8a67f723594/tokenizer.json\n","loading file added_tokens.json from cache at None\n","loading file special_tokens_map.json from cache at None\n","loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--bert-base-uncased/snapshots/86b5e0934494bd15c9632b12f734a8a67f723594/tokenizer_config.json\n","loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--bert-base-uncased/snapshots/86b5e0934494bd15c9632b12f734a8a67f723594/config.json\n","Model config BertConfig {\n","  \"_name_or_path\": \"bert-base-uncased\",\n","  \"architectures\": [\n","    \"BertForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.39.2\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n","\u001b[32m2024-04-24T17:32:20 | mmf.datasets.multi_datamodule: \u001b[0mMultitasking disabled by default for single dataset training\n","\u001b[32m2024-04-24T17:32:20 | mmf.datasets.multi_datamodule: \u001b[0mMultitasking disabled by default for single dataset training\n","\u001b[32m2024-04-24T17:32:20 | mmf.datasets.multi_datamodule: \u001b[0mMultitasking disabled by default for single dataset training\n","\u001b[32m2024-04-24T17:32:20 | mmf.trainers.mmf_trainer: \u001b[0mLoading model\n","Model config BertConfig {\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bert_model_name\": \"bert-base-uncased\",\n","  \"bypass_transformer\": false,\n","  \"classifier_dropout\": null,\n","  \"embedding_strategy\": \"plain\",\n","  \"finetune_lr_multiplier\": 1,\n","  \"freeze_base\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"losses\": [\n","    \"cross_entropy\"\n","  ],\n","  \"max_position_embeddings\": 512,\n","  \"model\": \"visual_bert\",\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"pooler_strategy\": \"default\",\n","  \"position_embedding_type\": \"absolute\",\n","  \"random_initialize\": false,\n","  \"special_visual_initialize\": true,\n","  \"training_head_type\": \"classification\",\n","  \"transformers_version\": \"4.39.2\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"visual_embedding_dim\": 2048,\n","  \"vocab_size\": 30522,\n","  \"zerobias\": false\n","}\n","\n","model.safetensors: 100% 440M/440M [00:01<00:00, 298MB/s]\n","loading weights file model.safetensors from cache at /root/.cache/torch/mmf/distributed_-1/models--bert-base-uncased/snapshots/86b5e0934494bd15c9632b12f734a8a67f723594/model.safetensors\n","Some weights of the model checkpoint at bert-base-uncased were not used when initializing VisualBERTBase: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n","- This IS expected if you are initializing VisualBERTBase from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing VisualBERTBase from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of VisualBERTBase were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['bert.embeddings.position_embeddings_visual.weight', 'bert.embeddings.projection.bias', 'bert.embeddings.projection.weight', 'bert.embeddings.token_type_embeddings_visual.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","\u001b[32m2024-04-24T17:32:22 | mmf.trainers.mmf_trainer: \u001b[0mLoading optimizer\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2024-04-24T17:32:22 | py.warnings: \u001b[0m/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:457: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n","\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2024-04-24T17:32:22 | py.warnings: \u001b[0m/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:457: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n","\n","\u001b[32m2024-04-24T17:32:22 | mmf.trainers.mmf_trainer: \u001b[0mLoading metrics\n","\u001b[32m2024-04-24T17:32:22 | mmf.utils.checkpoint: \u001b[0mLoading checkpoint\n","[ Downloading: https://dl.fbaipublicfiles.com/mmf/data/models/visual_bert/visual_bert.pretrained.coco_train_val.tar.gz to /root/.cache/torch/mmf/data/models/visual_bert.pretrained.coco.defaults/visual_bert.pretrained.coco_train_val.tar.gz ]\n","Downloading visual_bert.pretrained.coco_train_val.tar.gz: 100% 415M/415M [00:02<00:00, 149MB/s]\n","[ Starting checksum for visual_bert.pretrained.coco_train_val.tar.gz]\n","[ Checksum successful for visual_bert.pretrained.coco_train_val.tar.gz]\n","Unpacking visual_bert.pretrained.coco_train_val.tar.gz\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2024-04-24T17:32:31 | mmf: \u001b[0mKey data_parallel is not present in registry, returning default value of None\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2024-04-24T17:32:31 | mmf: \u001b[0mKey distributed is not present in registry, returning default value of None\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2024-04-24T17:32:31 | mmf: \u001b[0mKey data_parallel is not present in registry, returning default value of None\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2024-04-24T17:32:31 | mmf: \u001b[0mKey distributed is not present in registry, returning default value of None\n","\u001b[32m2024-04-24T17:32:31 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.word_embeddings.weight from model.bert.embeddings.word_embeddings.weight\n","\u001b[32m2024-04-24T17:32:31 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.position_embeddings.weight from model.bert.embeddings.position_embeddings.weight\n","\u001b[32m2024-04-24T17:32:31 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.token_type_embeddings.weight from model.bert.embeddings.token_type_embeddings.weight\n","\u001b[32m2024-04-24T17:32:31 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.LayerNorm.weight from model.bert.embeddings.LayerNorm.weight\n","\u001b[32m2024-04-24T17:32:31 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.LayerNorm.bias from model.bert.embeddings.LayerNorm.bias\n","\u001b[32m2024-04-24T17:32:31 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.token_type_embeddings_visual.weight from model.bert.embeddings.token_type_embeddings_visual.weight\n","\u001b[32m2024-04-24T17:32:31 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.position_embeddings_visual.weight from model.bert.embeddings.position_embeddings_visual.weight\n","\u001b[32m2024-04-24T17:32:31 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.projection.weight from model.bert.embeddings.projection.weight\n","\u001b[32m2024-04-24T17:32:31 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.projection.bias from model.bert.embeddings.projection.bias\n","\u001b[32m2024-04-24T17:32:31 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.self.query.weight from model.bert.encoder.layer.0.attention.self.query.weight\n","\u001b[32m2024-04-24T17:32:31 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.self.query.bias from model.bert.encoder.layer.0.attention.self.query.bias\n","\u001b[32m2024-04-24T17:32:31 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.self.key.weight from model.bert.encoder.layer.0.attention.self.key.weight\n","\u001b[32m2024-04-24T17:32:31 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.self.key.bias from model.bert.encoder.layer.0.attention.self.key.bias\n","\u001b[32m2024-04-24T17:32:31 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.self.value.weight from model.bert.encoder.layer.0.attention.self.value.weight\n","\u001b[32m2024-04-24T17:32:31 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.self.value.bias from model.bert.encoder.layer.0.attention.self.value.bias\n","\u001b[32m2024-04-24T17:32:31 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.output.dense.weight from model.bert.encoder.layer.0.attention.output.dense.weight\n","\u001b[32m2024-04-24T17:32:31 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.output.dense.bias from model.bert.encoder.layer.0.attention.output.dense.bias\n","\u001b[32m2024-04-24T17:32:31 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.output.LayerNorm.weight from model.bert.encoder.layer.0.attention.output.LayerNorm.weight\n","\u001b[32m2024-04-24T17:32:31 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.output.LayerNorm.bias from model.bert.encoder.layer.0.attention.output.LayerNorm.bias\n","\u001b[32m2024-04-24T17:32:31 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.intermediate.dense.weight from model.bert.encoder.layer.0.intermediate.dense.weight\n","\u001b[32m2024-04-24T17:32:31 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.intermediate.dense.bias from model.bert.encoder.layer.0.intermediate.dense.bias\n","\u001b[32m2024-04-24T17:32:31 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.output.dense.weight from model.bert.encoder.layer.0.output.dense.weight\n","\u001b[32m2024-04-24T17:32:31 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.output.dense.bias from model.bert.encoder.layer.0.output.dense.bias\n","\u001b[32m2024-04-24T17:32:31 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.output.LayerNorm.weight from model.bert.encoder.layer.0.output.LayerNorm.weight\n","\u001b[32m2024-04-24T17:32:31 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.output.LayerNorm.bias from model.bert.encoder.layer.0.output.LayerNorm.bias\n","\u001b[32m2024-04-24T17:32:31 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.self.query.weight from model.bert.encoder.layer.1.attention.self.query.weight\n","\u001b[32m2024-04-24T17:32:31 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.self.query.bias from model.bert.encoder.layer.1.attention.self.query.bias\n","\u001b[32m2024-04-24T17:32:31 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.self.key.weight from model.bert.encoder.layer.1.attention.self.key.weight\n","\u001b[32m2024-04-24T17:32:31 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.self.key.bias from model.bert.encoder.layer.1.attention.self.key.bias\n","\u001b[32m2024-04-24T17:32:31 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.self.value.weight from model.bert.encoder.layer.1.attention.self.value.weight\n","\u001b[32m2024-04-24T17:32:31 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.self.value.bias from model.bert.encoder.layer.1.attention.self.value.bias\n","\u001b[32m2024-04-24T17:32:31 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.output.dense.weight from model.bert.encoder.layer.1.attention.output.dense.weight\n","\u001b[32m2024-04-24T17:32:31 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.output.dense.bias from model.bert.encoder.layer.1.attention.output.dense.bias\n","\u001b[32m2024-04-24T17:32:31 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.output.LayerNorm.weight from model.bert.encoder.layer.1.attention.output.LayerNorm.weight\n","\u001b[32m2024-04-24T17:32:31 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.output.LayerNorm.bias from model.bert.encoder.layer.1.attention.output.LayerNorm.bias\n","\u001b[32m2024-04-24T17:32:31 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.intermediate.dense.weight from model.bert.encoder.layer.1.intermediate.dense.weight\n","\u001b[32m2024-04-24T17:32:31 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.intermediate.dense.bias from model.bert.encoder.layer.1.intermediate.dense.bias\n","\u001b[32m2024-04-24T17:32:31 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.output.dense.weight from model.bert.encoder.layer.1.output.dense.weight\n","\u001b[32m2024-04-24T17:32:31 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.output.dense.bias from model.bert.encoder.layer.1.output.dense.bias\n","\u001b[32m2024-04-24T17:32:31 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.output.LayerNorm.weight from model.bert.encoder.layer.1.output.LayerNorm.weight\n","\u001b[32m2024-04-24T17:32:31 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.output.LayerNorm.bias from model.bert.encoder.layer.1.output.LayerNorm.bias\n","\u001b[32m2024-04-24T17:32:31 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.self.query.weight from model.bert.encoder.layer.2.attention.self.query.weight\n","\u001b[32m2024-04-24T17:32:31 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.self.query.bias from model.bert.encoder.layer.2.attention.self.query.bias\n","\u001b[32m2024-04-24T17:32:31 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.self.key.weight from model.bert.encoder.layer.2.attention.self.key.weight\n","\u001b[32m2024-04-24T17:32:31 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.self.key.bias from model.bert.encoder.layer.2.attention.self.key.bias\n","\u001b[32m2024-04-24T17:32:31 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.self.value.weight from model.bert.encoder.layer.2.attention.self.value.weight\n","\u001b[32m2024-04-24T17:32:31 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.self.value.bias from model.bert.encoder.layer.2.attention.self.value.bias\n","\u001b[32m2024-04-24T17:32:31 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.output.dense.weight from model.bert.encoder.layer.2.attention.output.dense.weight\n","\u001b[32m2024-04-24T17:32:31 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.output.dense.bias from model.bert.encoder.layer.2.attention.output.dense.bias\n","\u001b[32m2024-04-24T17:32:31 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.output.LayerNorm.weight from model.bert.encoder.layer.2.attention.output.LayerNorm.weight\n","\u001b[32m2024-04-24T17:32:31 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.output.LayerNorm.bias from model.bert.encoder.layer.2.attention.output.LayerNorm.bias\n","\u001b[32m2024-04-24T17:32:31 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.intermediate.dense.weight from model.bert.encoder.layer.2.intermediate.dense.weight\n","\u001b[32m2024-04-24T17:32:31 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.intermediate.dense.bias from model.bert.encoder.layer.2.intermediate.dense.bias\n","\u001b[32m2024-04-24T17:32:31 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.output.dense.weight from model.bert.encoder.layer.2.output.dense.weight\n","\u001b[32m2024-04-24T17:32:31 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.output.dense.bias from model.bert.encoder.layer.2.output.dense.bias\n","\u001b[32m2024-04-24T17:32:31 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.output.LayerNorm.weight from model.bert.encoder.layer.2.output.LayerNorm.weight\n","\u001b[32m2024-04-24T17:32:31 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.output.LayerNorm.bias from model.bert.encoder.layer.2.output.LayerNorm.bias\n","\u001b[32m2024-04-24T17:32:31 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.self.query.weight from model.bert.encoder.layer.3.attention.self.query.weight\n","\u001b[32m2024-04-24T17:32:31 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.self.query.bias from model.bert.encoder.layer.3.attention.self.query.bias\n","\u001b[32m2024-04-24T17:32:31 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.self.key.weight from model.bert.encoder.layer.3.attention.self.key.weight\n","\u001b[32m2024-04-24T17:32:31 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.self.key.bias from model.bert.encoder.layer.3.attention.self.key.bias\n","\u001b[32m2024-04-24T17:32:31 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.self.value.weight from model.bert.encoder.layer.3.attention.self.value.weight\n","\u001b[32m2024-04-24T17:32:31 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.self.value.bias from model.bert.encoder.layer.3.attention.self.value.bias\n","\u001b[32m2024-04-24T17:32:31 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.output.dense.weight from model.bert.encoder.layer.3.attention.output.dense.weight\n","\u001b[32m2024-04-24T17:32:31 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.output.dense.bias from model.bert.encoder.layer.3.attention.output.dense.bias\n","\u001b[32m2024-04-24T17:32:31 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.output.LayerNorm.weight from model.bert.encoder.layer.3.attention.output.LayerNorm.weight\n","\u001b[32m2024-04-24T17:32:31 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.output.LayerNorm.bias from model.bert.encoder.layer.3.attention.output.LayerNorm.bias\n","\u001b[32m2024-04-24T17:32:31 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.intermediate.dense.weight from model.bert.encoder.layer.3.intermediate.dense.weight\n","\u001b[32m2024-04-24T17:32:31 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.intermediate.dense.bias from model.bert.encoder.layer.3.intermediate.dense.bias\n","\u001b[32m2024-04-24T17:32:31 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.output.dense.weight from model.bert.encoder.layer.3.output.dense.weight\n","\u001b[32m2024-04-24T17:32:31 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.output.dense.bias from model.bert.encoder.layer.3.output.dense.bias\n","\u001b[32m2024-04-24T17:32:31 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.output.LayerNorm.weight from model.bert.encoder.layer.3.output.LayerNorm.weight\n","\u001b[32m2024-04-24T17:32:31 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.output.LayerNorm.bias from model.bert.encoder.layer.3.output.LayerNorm.bias\n","\u001b[32m2024-04-24T17:32:31 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.self.query.weight from model.bert.encoder.layer.4.attention.self.query.weight\n","\u001b[32m2024-04-24T17:32:31 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.self.query.bias from model.bert.encoder.layer.4.attention.self.query.bias\n","\u001b[32m2024-04-24T17:32:31 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.self.key.weight from model.bert.encoder.layer.4.attention.self.key.weight\n","\u001b[32m2024-04-24T17:32:31 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.self.key.bias from model.bert.encoder.layer.4.attention.self.key.bias\n","\u001b[32m2024-04-24T17:32:31 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.self.value.weight from model.bert.encoder.layer.4.attention.self.value.weight\n","\u001b[32m2024-04-24T17:32:31 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.self.value.bias from model.bert.encoder.layer.4.attention.self.value.bias\n","\u001b[32m2024-04-24T17:32:31 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.output.dense.weight from model.bert.encoder.layer.4.attention.output.dense.weight\n","\u001b[32m2024-04-24T17:32:31 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.output.dense.bias from model.bert.encoder.layer.4.attention.output.dense.bias\n","\u001b[32m2024-04-24T17:32:31 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.output.LayerNorm.weight from model.bert.encoder.layer.4.attention.output.LayerNorm.weight\n","\u001b[32m2024-04-24T17:32:31 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.output.LayerNorm.bias from model.bert.encoder.layer.4.attention.output.LayerNorm.bias\n","\u001b[32m2024-04-24T17:32:31 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.intermediate.dense.weight from model.bert.encoder.layer.4.intermediate.dense.weight\n","\u001b[32m2024-04-24T17:32:31 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.intermediate.dense.bias from model.bert.encoder.layer.4.intermediate.dense.bias\n","\u001b[32m2024-04-24T17:32:31 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.output.dense.weight from model.bert.encoder.layer.4.output.dense.weight\n","\u001b[32m2024-04-24T17:32:31 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.output.dense.bias from model.bert.encoder.layer.4.output.dense.bias\n","\u001b[32m2024-04-24T17:32:31 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.output.LayerNorm.weight from model.bert.encoder.layer.4.output.LayerNorm.weight\n","\u001b[32m2024-04-24T17:32:31 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.output.LayerNorm.bias from model.bert.encoder.layer.4.output.LayerNorm.bias\n","\u001b[32m2024-04-24T17:32:31 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.self.query.weight from model.bert.encoder.layer.5.attention.self.query.weight\n","\u001b[32m2024-04-24T17:32:31 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.self.query.bias from model.bert.encoder.layer.5.attention.self.query.bias\n","\u001b[32m2024-04-24T17:32:31 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.self.key.weight from model.bert.encoder.layer.5.attention.self.key.weight\n","\u001b[32m2024-04-24T17:32:31 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.self.key.bias from model.bert.encoder.layer.5.attention.self.key.bias\n","\u001b[32m2024-04-24T17:32:31 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.self.value.weight from model.bert.encoder.layer.5.attention.self.value.weight\n","\u001b[32m2024-04-24T17:32:31 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.self.value.bias from model.bert.encoder.layer.5.attention.self.value.bias\n","\u001b[32m2024-04-24T17:32:31 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.output.dense.weight from model.bert.encoder.layer.5.attention.output.dense.weight\n","\u001b[32m2024-04-24T17:32:31 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.output.dense.bias from model.bert.encoder.layer.5.attention.output.dense.bias\n","\u001b[32m2024-04-24T17:32:31 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.output.LayerNorm.weight from model.bert.encoder.layer.5.attention.output.LayerNorm.weight\n","\u001b[32m2024-04-24T17:32:31 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.output.LayerNorm.bias from model.bert.encoder.layer.5.attention.output.LayerNorm.bias\n","\u001b[32m2024-04-24T17:32:31 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.intermediate.dense.weight from model.bert.encoder.layer.5.intermediate.dense.weight\n","\u001b[32m2024-04-24T17:32:31 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.intermediate.dense.bias from model.bert.encoder.layer.5.intermediate.dense.bias\n","\u001b[32m2024-04-24T17:32:31 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.output.dense.weight from model.bert.encoder.layer.5.output.dense.weight\n","\u001b[32m2024-04-24T17:32:31 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.output.dense.bias from model.bert.encoder.layer.5.output.dense.bias\n","\u001b[32m2024-04-24T17:32:31 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.output.LayerNorm.weight from model.bert.encoder.layer.5.output.LayerNorm.weight\n","\u001b[32m2024-04-24T17:32:31 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.output.LayerNorm.bias from model.bert.encoder.layer.5.output.LayerNorm.bias\n","\u001b[32m2024-04-24T17:32:31 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.self.query.weight from model.bert.encoder.layer.6.attention.self.query.weight\n","\u001b[32m2024-04-24T17:32:31 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.self.query.bias from model.bert.encoder.layer.6.attention.self.query.bias\n","\u001b[32m2024-04-24T17:32:31 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.self.key.weight from model.bert.encoder.layer.6.attention.self.key.weight\n","\u001b[32m2024-04-24T17:32:31 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.self.key.bias from model.bert.encoder.layer.6.attention.self.key.bias\n","\u001b[32m2024-04-24T17:32:31 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.self.value.weight from model.bert.encoder.layer.6.attention.self.value.weight\n","\u001b[32m2024-04-24T17:32:31 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.self.value.bias from model.bert.encoder.layer.6.attention.self.value.bias\n","\u001b[32m2024-04-24T17:32:31 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.output.dense.weight from model.bert.encoder.layer.6.attention.output.dense.weight\n","\u001b[32m2024-04-24T17:32:31 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.output.dense.bias from model.bert.encoder.layer.6.attention.output.dense.bias\n","\u001b[32m2024-04-24T17:32:31 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.output.LayerNorm.weight from model.bert.encoder.layer.6.attention.output.LayerNorm.weight\n","\u001b[32m2024-04-24T17:32:31 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.output.LayerNorm.bias from model.bert.encoder.layer.6.attention.output.LayerNorm.bias\n","\u001b[32m2024-04-24T17:32:31 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.intermediate.dense.weight from model.bert.encoder.layer.6.intermediate.dense.weight\n","\u001b[32m2024-04-24T17:32:31 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.intermediate.dense.bias from model.bert.encoder.layer.6.intermediate.dense.bias\n","\u001b[32m2024-04-24T17:32:31 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.output.dense.weight from model.bert.encoder.layer.6.output.dense.weight\n","\u001b[32m2024-04-24T17:32:31 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.output.dense.bias from model.bert.encoder.layer.6.output.dense.bias\n","\u001b[32m2024-04-24T17:32:31 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.output.LayerNorm.weight from model.bert.encoder.layer.6.output.LayerNorm.weight\n","\u001b[32m2024-04-24T17:32:31 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.output.LayerNorm.bias from model.bert.encoder.layer.6.output.LayerNorm.bias\n","\u001b[32m2024-04-24T17:32:31 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.self.query.weight from model.bert.encoder.layer.7.attention.self.query.weight\n","\u001b[32m2024-04-24T17:32:31 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.self.query.bias from model.bert.encoder.layer.7.attention.self.query.bias\n","\u001b[32m2024-04-24T17:32:31 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.self.key.weight from model.bert.encoder.layer.7.attention.self.key.weight\n","\u001b[32m2024-04-24T17:32:31 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.self.key.bias from model.bert.encoder.layer.7.attention.self.key.bias\n","\u001b[32m2024-04-24T17:32:31 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.self.value.weight from model.bert.encoder.layer.7.attention.self.value.weight\n","\u001b[32m2024-04-24T17:32:31 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.self.value.bias from model.bert.encoder.layer.7.attention.self.value.bias\n","\u001b[32m2024-04-24T17:32:31 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.output.dense.weight from model.bert.encoder.layer.7.attention.output.dense.weight\n","\u001b[32m2024-04-24T17:32:31 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.output.dense.bias from model.bert.encoder.layer.7.attention.output.dense.bias\n","\u001b[32m2024-04-24T17:32:31 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.output.LayerNorm.weight from model.bert.encoder.layer.7.attention.output.LayerNorm.weight\n","\u001b[32m2024-04-24T17:32:31 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.output.LayerNorm.bias from model.bert.encoder.layer.7.attention.output.LayerNorm.bias\n","\u001b[32m2024-04-24T17:32:31 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.intermediate.dense.weight from model.bert.encoder.layer.7.intermediate.dense.weight\n","\u001b[32m2024-04-24T17:32:31 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.intermediate.dense.bias from model.bert.encoder.layer.7.intermediate.dense.bias\n","\u001b[32m2024-04-24T17:32:31 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.output.dense.weight from model.bert.encoder.layer.7.output.dense.weight\n","\u001b[32m2024-04-24T17:32:31 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.output.dense.bias from model.bert.encoder.layer.7.output.dense.bias\n","\u001b[32m2024-04-24T17:32:31 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.output.LayerNorm.weight from model.bert.encoder.layer.7.output.LayerNorm.weight\n","\u001b[32m2024-04-24T17:32:31 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.output.LayerNorm.bias from model.bert.encoder.layer.7.output.LayerNorm.bias\n","\u001b[32m2024-04-24T17:32:31 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.self.query.weight from model.bert.encoder.layer.8.attention.self.query.weight\n","\u001b[32m2024-04-24T17:32:31 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.self.query.bias from model.bert.encoder.layer.8.attention.self.query.bias\n","\u001b[32m2024-04-24T17:32:31 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.self.key.weight from model.bert.encoder.layer.8.attention.self.key.weight\n","\u001b[32m2024-04-24T17:32:31 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.self.key.bias from model.bert.encoder.layer.8.attention.self.key.bias\n","\u001b[32m2024-04-24T17:32:31 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.self.value.weight from model.bert.encoder.layer.8.attention.self.value.weight\n","\u001b[32m2024-04-24T17:32:31 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.self.value.bias from model.bert.encoder.layer.8.attention.self.value.bias\n","\u001b[32m2024-04-24T17:32:31 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.output.dense.weight from model.bert.encoder.layer.8.attention.output.dense.weight\n","\u001b[32m2024-04-24T17:32:31 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.output.dense.bias from model.bert.encoder.layer.8.attention.output.dense.bias\n","\u001b[32m2024-04-24T17:32:31 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.output.LayerNorm.weight from model.bert.encoder.layer.8.attention.output.LayerNorm.weight\n","\u001b[32m2024-04-24T17:32:31 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.output.LayerNorm.bias from model.bert.encoder.layer.8.attention.output.LayerNorm.bias\n","\u001b[32m2024-04-24T17:32:31 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.intermediate.dense.weight from model.bert.encoder.layer.8.intermediate.dense.weight\n","\u001b[32m2024-04-24T17:32:31 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.intermediate.dense.bias from model.bert.encoder.layer.8.intermediate.dense.bias\n","\u001b[32m2024-04-24T17:32:31 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.output.dense.weight from model.bert.encoder.layer.8.output.dense.weight\n","\u001b[32m2024-04-24T17:32:31 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.output.dense.bias from model.bert.encoder.layer.8.output.dense.bias\n","\u001b[32m2024-04-24T17:32:31 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.output.LayerNorm.weight from model.bert.encoder.layer.8.output.LayerNorm.weight\n","\u001b[32m2024-04-24T17:32:31 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.output.LayerNorm.bias from model.bert.encoder.layer.8.output.LayerNorm.bias\n","\u001b[32m2024-04-24T17:32:31 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.self.query.weight from model.bert.encoder.layer.9.attention.self.query.weight\n","\u001b[32m2024-04-24T17:32:31 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.self.query.bias from model.bert.encoder.layer.9.attention.self.query.bias\n","\u001b[32m2024-04-24T17:32:31 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.self.key.weight from model.bert.encoder.layer.9.attention.self.key.weight\n","\u001b[32m2024-04-24T17:32:31 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.self.key.bias from model.bert.encoder.layer.9.attention.self.key.bias\n","\u001b[32m2024-04-24T17:32:31 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.self.value.weight from model.bert.encoder.layer.9.attention.self.value.weight\n","\u001b[32m2024-04-24T17:32:31 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.self.value.bias from model.bert.encoder.layer.9.attention.self.value.bias\n","\u001b[32m2024-04-24T17:32:31 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.output.dense.weight from model.bert.encoder.layer.9.attention.output.dense.weight\n","\u001b[32m2024-04-24T17:32:31 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.output.dense.bias from model.bert.encoder.layer.9.attention.output.dense.bias\n","\u001b[32m2024-04-24T17:32:31 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.output.LayerNorm.weight from model.bert.encoder.layer.9.attention.output.LayerNorm.weight\n","\u001b[32m2024-04-24T17:32:31 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.output.LayerNorm.bias from model.bert.encoder.layer.9.attention.output.LayerNorm.bias\n","\u001b[32m2024-04-24T17:32:31 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.intermediate.dense.weight from model.bert.encoder.layer.9.intermediate.dense.weight\n","\u001b[32m2024-04-24T17:32:31 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.intermediate.dense.bias from model.bert.encoder.layer.9.intermediate.dense.bias\n","\u001b[32m2024-04-24T17:32:31 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.output.dense.weight from model.bert.encoder.layer.9.output.dense.weight\n","\u001b[32m2024-04-24T17:32:31 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.output.dense.bias from model.bert.encoder.layer.9.output.dense.bias\n","\u001b[32m2024-04-24T17:32:31 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.output.LayerNorm.weight from model.bert.encoder.layer.9.output.LayerNorm.weight\n","\u001b[32m2024-04-24T17:32:31 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.output.LayerNorm.bias from model.bert.encoder.layer.9.output.LayerNorm.bias\n","\u001b[32m2024-04-24T17:32:31 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.self.query.weight from model.bert.encoder.layer.10.attention.self.query.weight\n","\u001b[32m2024-04-24T17:32:31 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.self.query.bias from model.bert.encoder.layer.10.attention.self.query.bias\n","\u001b[32m2024-04-24T17:32:31 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.self.key.weight from model.bert.encoder.layer.10.attention.self.key.weight\n","\u001b[32m2024-04-24T17:32:31 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.self.key.bias from model.bert.encoder.layer.10.attention.self.key.bias\n","\u001b[32m2024-04-24T17:32:31 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.self.value.weight from model.bert.encoder.layer.10.attention.self.value.weight\n","\u001b[32m2024-04-24T17:32:31 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.self.value.bias from model.bert.encoder.layer.10.attention.self.value.bias\n","\u001b[32m2024-04-24T17:32:31 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.output.dense.weight from model.bert.encoder.layer.10.attention.output.dense.weight\n","\u001b[32m2024-04-24T17:32:31 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.output.dense.bias from model.bert.encoder.layer.10.attention.output.dense.bias\n","\u001b[32m2024-04-24T17:32:31 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.output.LayerNorm.weight from model.bert.encoder.layer.10.attention.output.LayerNorm.weight\n","\u001b[32m2024-04-24T17:32:31 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.output.LayerNorm.bias from model.bert.encoder.layer.10.attention.output.LayerNorm.bias\n","\u001b[32m2024-04-24T17:32:31 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.intermediate.dense.weight from model.bert.encoder.layer.10.intermediate.dense.weight\n","\u001b[32m2024-04-24T17:32:31 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.intermediate.dense.bias from model.bert.encoder.layer.10.intermediate.dense.bias\n","\u001b[32m2024-04-24T17:32:31 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.output.dense.weight from model.bert.encoder.layer.10.output.dense.weight\n","\u001b[32m2024-04-24T17:32:31 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.output.dense.bias from model.bert.encoder.layer.10.output.dense.bias\n","\u001b[32m2024-04-24T17:32:31 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.output.LayerNorm.weight from model.bert.encoder.layer.10.output.LayerNorm.weight\n","\u001b[32m2024-04-24T17:32:31 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.output.LayerNorm.bias from model.bert.encoder.layer.10.output.LayerNorm.bias\n","\u001b[32m2024-04-24T17:32:31 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.self.query.weight from model.bert.encoder.layer.11.attention.self.query.weight\n","\u001b[32m2024-04-24T17:32:31 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.self.query.bias from model.bert.encoder.layer.11.attention.self.query.bias\n","\u001b[32m2024-04-24T17:32:31 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.self.key.weight from model.bert.encoder.layer.11.attention.self.key.weight\n","\u001b[32m2024-04-24T17:32:31 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.self.key.bias from model.bert.encoder.layer.11.attention.self.key.bias\n","\u001b[32m2024-04-24T17:32:31 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.self.value.weight from model.bert.encoder.layer.11.attention.self.value.weight\n","\u001b[32m2024-04-24T17:32:31 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.self.value.bias from model.bert.encoder.layer.11.attention.self.value.bias\n","\u001b[32m2024-04-24T17:32:31 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.output.dense.weight from model.bert.encoder.layer.11.attention.output.dense.weight\n","\u001b[32m2024-04-24T17:32:31 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.output.dense.bias from model.bert.encoder.layer.11.attention.output.dense.bias\n","\u001b[32m2024-04-24T17:32:31 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.output.LayerNorm.weight from model.bert.encoder.layer.11.attention.output.LayerNorm.weight\n","\u001b[32m2024-04-24T17:32:31 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.output.LayerNorm.bias from model.bert.encoder.layer.11.attention.output.LayerNorm.bias\n","\u001b[32m2024-04-24T17:32:31 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.intermediate.dense.weight from model.bert.encoder.layer.11.intermediate.dense.weight\n","\u001b[32m2024-04-24T17:32:31 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.intermediate.dense.bias from model.bert.encoder.layer.11.intermediate.dense.bias\n","\u001b[32m2024-04-24T17:32:31 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.output.dense.weight from model.bert.encoder.layer.11.output.dense.weight\n","\u001b[32m2024-04-24T17:32:31 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.output.dense.bias from model.bert.encoder.layer.11.output.dense.bias\n","\u001b[32m2024-04-24T17:32:31 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.output.LayerNorm.weight from model.bert.encoder.layer.11.output.LayerNorm.weight\n","\u001b[32m2024-04-24T17:32:31 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.output.LayerNorm.bias from model.bert.encoder.layer.11.output.LayerNorm.bias\n","\u001b[32m2024-04-24T17:32:31 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.pooler.dense.weight from model.bert.pooler.dense.weight\n","\u001b[32m2024-04-24T17:32:31 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.pooler.dense.bias from model.bert.pooler.dense.bias\n","\u001b[32m2024-04-24T17:32:31 | mmf.utils.checkpoint: \u001b[0mPretrained model loaded\n","\u001b[32m2024-04-24T17:32:31 | mmf.utils.checkpoint: \u001b[0mCheckpoint loaded.\n","\u001b[32m2024-04-24T17:32:31 | mmf.utils.checkpoint: \u001b[0mCurrent num updates: 0\n","\u001b[32m2024-04-24T17:32:31 | mmf.utils.checkpoint: \u001b[0mCurrent iteration: 0\n","\u001b[32m2024-04-24T17:32:31 | mmf.utils.checkpoint: \u001b[0mCurrent epoch: 0\n","\u001b[32m2024-04-24T17:32:31 | mmf.trainers.mmf_trainer: \u001b[0m===== Model =====\n","\u001b[32m2024-04-24T17:32:31 | mmf.trainers.mmf_trainer: \u001b[0mVisualBERT(\n","  (model): VisualBERTForClassification(\n","    (bert): VisualBERTBase(\n","      (embeddings): BertVisioLinguisticEmbeddings(\n","        (word_embeddings): Embedding(30522, 768, padding_idx=0)\n","        (position_embeddings): Embedding(512, 768)\n","        (token_type_embeddings): Embedding(2, 768)\n","        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","        (token_type_embeddings_visual): Embedding(2, 768)\n","        (position_embeddings_visual): Embedding(512, 768)\n","        (projection): Linear(in_features=2048, out_features=768, bias=True)\n","      )\n","      (encoder): BertEncoderJit(\n","        (layer): ModuleList(\n","          (0-11): 12 x BertLayerJit(\n","            (attention): BertAttentionJit(\n","              (self): BertSelfAttentionJit(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","              (intermediate_act_fn): GELUActivation()\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","        )\n","      )\n","      (pooler): BertPooler(\n","        (dense): Linear(in_features=768, out_features=768, bias=True)\n","        (activation): Tanh()\n","      )\n","    )\n","    (dropout): Dropout(p=0.1, inplace=False)\n","    (classifier): Sequential(\n","      (0): BertPredictionHeadTransform(\n","        (dense): Linear(in_features=768, out_features=768, bias=True)\n","        (transform_act_fn): GELUActivation()\n","        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      )\n","      (1): Linear(in_features=768, out_features=2, bias=True)\n","    )\n","  )\n","  (losses): Losses(\n","    (losses): ModuleList(\n","      (0): MMFLoss(\n","        (loss_criterion): CrossEntropyLoss(\n","          (loss_fn): CrossEntropyLoss()\n","        )\n","      )\n","    )\n","  )\n",")\n","\u001b[32m2024-04-24T17:32:31 | mmf.utils.general: \u001b[0mTotal Parameters: 112044290. Trained Parameters: 112044290\n","\u001b[32m2024-04-24T17:32:31 | mmf.trainers.core.training_loop: \u001b[0mStarting training...\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2024-04-24T17:32:31 | py.warnings: \u001b[0m/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n","  self.pid = os.fork()\n","\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2024-04-24T17:32:31 | py.warnings: \u001b[0m/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n","  self.pid = os.fork()\n","\n","\u001b[32m2024-04-24T17:36:35 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n","\u001b[32m2024-04-24T17:36:35 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2024-04-24T17:36:37 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2024-04-24T17:36:40 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2024-04-24T17:36:40 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 200/1500, train/hateful_memes/cross_entropy: 0.6012, train/hateful_memes/cross_entropy/avg: 0.6012, train/total_loss: 0.6012, train/total_loss/avg: 0.6012, max mem: 14322.0, experiment: run, epoch: 1, num_updates: 200, iterations: 200, max_updates: 1500, lr: 0.00001, ups: 0.81, time: 04m 08s 736ms, time_since_start: 04m 17s 327ms, eta: 28m 09s 540ms\n","\u001b[32m2024-04-24T17:36:40 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n","\u001b[32m2024-04-24T17:36:40 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n","\u001b[32m2024-04-24T17:36:44 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished evaluation inference. Loaded 9\n","\u001b[32m2024-04-24T17:36:44 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n","\u001b[32m2024-04-24T17:36:44 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2024-04-24T17:36:47 | mmf.utils.checkpoint: \u001b[0mSaving best checkpoint\n","\u001b[32m2024-04-24T17:36:50 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2024-04-24T17:36:54 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2024-04-24T17:36:54 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 200/1500, val/hateful_memes/cross_entropy: 0.6832, val/total_loss: 0.6832, val/hateful_memes/accuracy: 0.6352, val/hateful_memes/binary_f1: 0.0922, val/hateful_memes/roc_auc: 0.5454, num_updates: 200, epoch: 1, iterations: 200, max_updates: 1500, val_time: 14s 432ms, best_update: 200, best_iteration: 200, best_val/hateful_memes/roc_auc: 0.545397\n","\u001b[32m2024-04-24T17:40:56 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n","\u001b[32m2024-04-24T17:40:56 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2024-04-24T17:40:58 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2024-04-24T17:41:04 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2024-04-24T17:41:04 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 400/1500, train/hateful_memes/cross_entropy: 0.4968, train/hateful_memes/cross_entropy/avg: 0.5490, train/total_loss: 0.4968, train/total_loss/avg: 0.5490, max mem: 14371.0, experiment: run, epoch: 1, num_updates: 400, iterations: 400, max_updates: 1500, lr: 0.00001, ups: 0.80, time: 04m 09s 671ms, time_since_start: 08m 41s 439ms, eta: 23m 54s 989ms\n","\u001b[32m2024-04-24T17:41:04 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n","\u001b[32m2024-04-24T17:41:04 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2024-04-24T17:41:04 | py.warnings: \u001b[0m/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n","  self.pid = os.fork()\n","\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2024-04-24T17:41:04 | py.warnings: \u001b[0m/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n","  self.pid = os.fork()\n","\n","\u001b[32m2024-04-24T17:41:08 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished evaluation inference. Loaded 9\n","\u001b[32m2024-04-24T17:41:08 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n","\u001b[32m2024-04-24T17:41:09 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2024-04-24T17:41:13 | mmf.utils.checkpoint: \u001b[0mSaving best checkpoint\n","\u001b[32m2024-04-24T17:41:18 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2024-04-24T17:41:20 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2024-04-24T17:41:20 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 400/1500, val/hateful_memes/cross_entropy: 0.7181, val/total_loss: 0.7181, val/hateful_memes/accuracy: 0.6463, val/hateful_memes/binary_f1: 0.2568, val/hateful_memes/roc_auc: 0.6382, num_updates: 400, epoch: 1, iterations: 400, max_updates: 1500, val_time: 16s 143ms, best_update: 400, best_iteration: 400, best_val/hateful_memes/roc_auc: 0.638176\n","\u001b[32m2024-04-24T17:45:21 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n","\u001b[32m2024-04-24T17:45:21 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2024-04-24T17:45:23 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2024-04-24T17:45:29 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2024-04-24T17:45:29 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 600/1500, train/hateful_memes/cross_entropy: 0.4968, train/hateful_memes/cross_entropy/avg: 0.5197, train/total_loss: 0.4968, train/total_loss/avg: 0.5197, max mem: 14371.0, experiment: run, epoch: 2, num_updates: 600, iterations: 600, max_updates: 1500, lr: 0.00002, ups: 0.80, time: 04m 09s 263ms, time_since_start: 13m 06s 847ms, eta: 19m 32s 163ms\n","\u001b[32m2024-04-24T17:45:29 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n","\u001b[32m2024-04-24T17:45:29 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2024-04-24T17:45:29 | py.warnings: \u001b[0m/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n","  self.pid = os.fork()\n","\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2024-04-24T17:45:29 | py.warnings: \u001b[0m/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n","  self.pid = os.fork()\n","\n","\u001b[32m2024-04-24T17:45:34 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished evaluation inference. Loaded 9\n","\u001b[32m2024-04-24T17:45:34 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n","\u001b[32m2024-04-24T17:45:34 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2024-04-24T17:45:36 | mmf.utils.checkpoint: \u001b[0mSaving best checkpoint\n","\u001b[32m2024-04-24T17:45:42 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2024-04-24T17:45:44 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2024-04-24T17:45:44 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 600/1500, val/hateful_memes/cross_entropy: 0.8488, val/total_loss: 0.8488, val/hateful_memes/accuracy: 0.6556, val/hateful_memes/binary_f1: 0.3162, val/hateful_memes/roc_auc: 0.6787, num_updates: 600, epoch: 2, iterations: 600, max_updates: 1500, val_time: 14s 776ms, best_update: 600, best_iteration: 600, best_val/hateful_memes/roc_auc: 0.678662\n","\u001b[32m2024-04-24T17:49:45 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n","\u001b[32m2024-04-24T17:49:45 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2024-04-24T17:49:47 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2024-04-24T17:49:53 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2024-04-24T17:49:53 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 800/1500, train/hateful_memes/cross_entropy: 0.4611, train/hateful_memes/cross_entropy/avg: 0.4582, train/total_loss: 0.4611, train/total_loss/avg: 0.4582, max mem: 14371.0, experiment: run, epoch: 2, num_updates: 800, iterations: 800, max_updates: 1500, lr: 0.00002, ups: 0.80, time: 04m 09s 044ms, time_since_start: 17m 30s 671ms, eta: 15m 10s 882ms\n","\u001b[32m2024-04-24T17:49:53 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n","\u001b[32m2024-04-24T17:49:53 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2024-04-24T17:49:53 | py.warnings: \u001b[0m/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n","  self.pid = os.fork()\n","\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2024-04-24T17:49:53 | py.warnings: \u001b[0m/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n","  self.pid = os.fork()\n","\n","\u001b[32m2024-04-24T17:49:57 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished evaluation inference. Loaded 9\n","\u001b[32m2024-04-24T17:49:57 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n","\u001b[32m2024-04-24T17:49:58 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2024-04-24T17:50:00 | mmf.utils.checkpoint: \u001b[0mSaving best checkpoint\n","\u001b[32m2024-04-24T17:50:04 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2024-04-24T17:50:09 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2024-04-24T17:50:09 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 800/1500, val/hateful_memes/cross_entropy: 0.8405, val/total_loss: 0.8405, val/hateful_memes/accuracy: 0.6870, val/hateful_memes/binary_f1: 0.5015, val/hateful_memes/roc_auc: 0.7034, num_updates: 800, epoch: 2, iterations: 800, max_updates: 1500, val_time: 16s 218ms, best_update: 800, best_iteration: 800, best_val/hateful_memes/roc_auc: 0.703353\n","\u001b[32m2024-04-24T17:54:13 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n","\u001b[32m2024-04-24T17:54:13 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2024-04-24T17:54:15 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2024-04-24T17:54:18 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2024-04-24T17:54:18 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1000/1500, train/hateful_memes/cross_entropy: 0.4611, train/hateful_memes/cross_entropy/avg: 0.3889, train/total_loss: 0.4611, train/total_loss/avg: 0.3889, max mem: 14372.0, experiment: run, epoch: 3, num_updates: 1000, iterations: 1000, max_updates: 1500, lr: 0.00003, ups: 0.80, time: 04m 09s 191ms, time_since_start: 21m 56s 084ms, eta: 10m 51s 012ms\n","\u001b[32m2024-04-24T17:54:18 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n","\u001b[32m2024-04-24T17:54:18 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2024-04-24T17:54:18 | py.warnings: \u001b[0m/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n","  self.pid = os.fork()\n","\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2024-04-24T17:54:18 | py.warnings: \u001b[0m/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n","  self.pid = os.fork()\n","\n","\u001b[32m2024-04-24T17:54:23 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished evaluation inference. Loaded 9\n","\u001b[32m2024-04-24T17:54:23 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n","\u001b[32m2024-04-24T17:54:23 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2024-04-24T17:54:25 | mmf.utils.checkpoint: \u001b[0mSaving best checkpoint\n","\u001b[32m2024-04-24T17:54:31 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2024-04-24T17:54:36 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2024-04-24T17:54:36 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1000/1500, val/hateful_memes/cross_entropy: 1.2546, val/total_loss: 1.2546, val/hateful_memes/accuracy: 0.6907, val/hateful_memes/binary_f1: 0.4339, val/hateful_memes/roc_auc: 0.7143, num_updates: 1000, epoch: 3, iterations: 1000, max_updates: 1500, val_time: 17s 165ms, best_update: 1000, best_iteration: 1000, best_val/hateful_memes/roc_auc: 0.714250\n","\u001b[32m2024-04-24T17:58:39 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n","\u001b[32m2024-04-24T17:58:39 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2024-04-24T17:58:41 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2024-04-24T17:58:49 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2024-04-24T17:58:49 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1200/1500, train/hateful_memes/cross_entropy: 0.2738, train/hateful_memes/cross_entropy/avg: 0.3484, train/total_loss: 0.2738, train/total_loss/avg: 0.3484, max mem: 14372.0, experiment: run, epoch: 3, num_updates: 1200, iterations: 1200, max_updates: 1500, lr: 0.00003, ups: 0.79, time: 04m 12s 967ms, time_since_start: 26m 26s 219ms, eta: 06m 36s 526ms\n","\u001b[32m2024-04-24T17:58:49 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n","\u001b[32m2024-04-24T17:58:49 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2024-04-24T17:58:49 | py.warnings: \u001b[0m/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n","  self.pid = os.fork()\n","\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2024-04-24T17:58:49 | py.warnings: \u001b[0m/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n","  self.pid = os.fork()\n","\n","\u001b[32m2024-04-24T17:58:53 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished evaluation inference. Loaded 9\n","\u001b[32m2024-04-24T17:58:53 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n","\u001b[32m2024-04-24T17:58:53 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2024-04-24T17:58:58 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2024-04-24T17:59:00 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2024-04-24T17:59:00 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1200/1500, val/hateful_memes/cross_entropy: 1.3826, val/total_loss: 1.3826, val/hateful_memes/accuracy: 0.6907, val/hateful_memes/binary_f1: 0.3792, val/hateful_memes/roc_auc: 0.7077, num_updates: 1200, epoch: 3, iterations: 1200, max_updates: 1500, val_time: 11s 404ms, best_update: 1000, best_iteration: 1000, best_val/hateful_memes/roc_auc: 0.714250\n","\u001b[32m2024-04-24T18:03:02 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n","\u001b[32m2024-04-24T18:03:02 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2024-04-24T18:03:04 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2024-04-24T18:03:07 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2024-04-24T18:03:07 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1400/1500, train/hateful_memes/cross_entropy: 0.2738, train/hateful_memes/cross_entropy/avg: 0.3152, train/total_loss: 0.2738, train/total_loss/avg: 0.3152, max mem: 14372.0, experiment: run, epoch: 4, num_updates: 1400, iterations: 1400, max_updates: 1500, lr: 0.00003, ups: 0.81, time: 04m 07s 229ms, time_since_start: 30m 44s 854ms, eta: 02m 09s 177ms\n","\u001b[32m2024-04-24T18:03:07 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n","\u001b[32m2024-04-24T18:03:07 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2024-04-24T18:03:07 | py.warnings: \u001b[0m/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n","  self.pid = os.fork()\n","\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2024-04-24T18:03:07 | py.warnings: \u001b[0m/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n","  self.pid = os.fork()\n","\n","\u001b[32m2024-04-24T18:03:12 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished evaluation inference. Loaded 9\n","\u001b[32m2024-04-24T18:03:12 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n","\u001b[32m2024-04-24T18:03:12 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2024-04-24T18:03:14 | mmf.utils.checkpoint: \u001b[0mSaving best checkpoint\n","\u001b[32m2024-04-24T18:03:18 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2024-04-24T18:03:21 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2024-04-24T18:03:21 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1400/1500, val/hateful_memes/cross_entropy: 1.2732, val/total_loss: 1.2732, val/hateful_memes/accuracy: 0.6852, val/hateful_memes/binary_f1: 0.4333, val/hateful_memes/roc_auc: 0.7263, num_updates: 1400, epoch: 4, iterations: 1400, max_updates: 1500, val_time: 13s 835ms, best_update: 1400, best_iteration: 1400, best_val/hateful_memes/roc_auc: 0.726265\n","\u001b[32m2024-04-24T18:05:23 | mmf.trainers.core.training_loop: \u001b[0mStepping into final validation check\n","\u001b[32m2024-04-24T18:05:23 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2024-04-24T18:05:23 | py.warnings: \u001b[0m/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n","  self.pid = os.fork()\n","\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2024-04-24T18:05:23 | py.warnings: \u001b[0m/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n","  self.pid = os.fork()\n","\n","\u001b[32m2024-04-24T18:05:27 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished evaluation inference. Loaded 9\n","\u001b[32m2024-04-24T18:05:27 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n","\u001b[32m2024-04-24T18:05:27 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1500/1500, val/hateful_memes/cross_entropy: 1.4140, val/total_loss: 1.4140, val/hateful_memes/accuracy: 0.6889, val/hateful_memes/binary_f1: 0.4650, val/hateful_memes/roc_auc: 0.6989, num_updates: 1500, epoch: 4, iterations: 1500, max_updates: 1500, val_time: 02m 19s 978ms, best_update: 1400, best_iteration: 1400, best_val/hateful_memes/roc_auc: 0.726265\n","\u001b[32m2024-04-24T18:05:27 | mmf.utils.checkpoint: \u001b[0mRestoring checkpoint\n","\u001b[32m2024-04-24T18:05:27 | mmf.utils.checkpoint: \u001b[0mLoading checkpoint\n","\u001b[32m2024-04-24T18:05:28 | mmf.utils.checkpoint: \u001b[0mCheckpoint loaded.\n","\u001b[32m2024-04-24T18:05:28 | mmf.utils.checkpoint: \u001b[0mCurrent num updates: 1400\n","\u001b[32m2024-04-24T18:05:28 | mmf.utils.checkpoint: \u001b[0mCurrent iteration: 1400\n","\u001b[32m2024-04-24T18:05:28 | mmf.utils.checkpoint: \u001b[0mCurrent epoch: 4\n","\u001b[32m2024-04-24T18:05:28 | mmf.trainers.mmf_trainer: \u001b[0mStarting inference on val set\n","\u001b[32m2024-04-24T18:05:28 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n","  0% 0/9 [00:00<?, ?it/s]\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2024-04-24T18:05:28 | py.warnings: \u001b[0m/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n","  self.pid = os.fork()\n","\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2024-04-24T18:05:28 | py.warnings: \u001b[0m/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n","  self.pid = os.fork()\n","\n","100% 9/9 [00:04<00:00,  2.10it/s]\n","\u001b[32m2024-04-24T18:05:33 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished evaluation inference. Loaded 9\n","\u001b[32m2024-04-24T18:05:33 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n","\u001b[32m2024-04-24T18:05:33 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1400/1500, val/hateful_memes/cross_entropy: 1.2732, val/total_loss: 1.2732, val/hateful_memes/accuracy: 0.6852, val/hateful_memes/binary_f1: 0.4333, val/hateful_memes/roc_auc: 0.7263\n","\u001b[32m2024-04-24T18:05:33 | mmf.trainers.callbacks.logistics: \u001b[0mFinished run in 33m 10s 663ms\n"]}]},{"cell_type":"markdown","source":["### Dataset: Augmented with combined transformations"],"metadata":{"id":"76HjJsngYLVs"}},{"cell_type":"code","source":["!mmf_run config=\"./configs/defaults.yaml\" \\\n","  model=visual_bert \\\n","  dataset=hateful_memes \\\n","  run_type=train_val \\\n","  training.log_interval=200 \\\n","  training.batch_size=64 \\\n","  training.evaluation_interval=200 \\\n","  training.tensorboard=True \\\n","  training.checkpoint_interval=200 \\\n","  checkpoint.resume_pretrained=True \\\n","  checkpoint.resume_zoo=visual_bert.pretrained.coco \\\n","  dataset_config.hateful_memes.annotations.train[0]=\"hateful_memes/defaults/annotations/train_dev_transform_all_combined.jsonl\" \\\n","  dataset_config.hateful_memes.annotations.val[0]=\"hateful_memes/defaults/annotations/dev_unseen.jsonl\" \\\n","  dataset_config.hateful_memes.annotations.test[0]=\"hateful_memes/defaults/annotations/test_unseen.jsonl\""],"metadata":{"id":"E5I5j8qnVu1D","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1713798280849,"user_tz":420,"elapsed":63468,"user":{"displayName":"Aneri Shah","userId":"14009776940163552827"}},"outputId":"d98907ac-39bc-4664-c7c3-d9be18389896"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["2024-04-22 14:31:35.365915: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","2024-04-22 14:31:35.365966: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","2024-04-22 14:31:35.367158: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","2024-04-22 14:31:36.432665: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","2024-04-22 14:31:40.776437: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:47] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n","/usr/local/lib/python3.10/dist-packages/transformers/deepspeed.py:23: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/omegaconf/grammar_visitor.py:253: UserWarning: In the sequence `MMF_USER_DIR,` some elements are missing: please replace them with empty quoted strings. See https://github.com/omry/omegaconf/issues/572 for details.\n","  warnings.warn(\n","\u001b[32m2024-04-22T14:31:45 | mmf.utils.configuration: \u001b[0mOverriding option config to ./configs/defaults.yaml\n","\u001b[32m2024-04-22T14:31:45 | mmf.utils.configuration: \u001b[0mOverriding option model to visual_bert\n","\u001b[32m2024-04-22T14:31:45 | mmf.utils.configuration: \u001b[0mOverriding option datasets to hateful_memes\n","\u001b[32m2024-04-22T14:31:45 | mmf.utils.configuration: \u001b[0mOverriding option run_type to train_val\n","\u001b[32m2024-04-22T14:31:45 | mmf.utils.configuration: \u001b[0mOverriding option training.log_interval to 200\n","\u001b[32m2024-04-22T14:31:45 | mmf.utils.configuration: \u001b[0mOverriding option training.batch_size to 64\n","\u001b[32m2024-04-22T14:31:45 | mmf.utils.configuration: \u001b[0mOverriding option training.evaluation_interval to 200\n","\u001b[32m2024-04-22T14:31:45 | mmf.utils.configuration: \u001b[0mOverriding option training.tensorboard to True\n","\u001b[32m2024-04-22T14:31:45 | mmf.utils.configuration: \u001b[0mOverriding option training.checkpoint_interval to 200\n","\u001b[32m2024-04-22T14:31:45 | mmf.utils.configuration: \u001b[0mOverriding option checkpoint.resume_pretrained to True\n","\u001b[32m2024-04-22T14:31:45 | mmf.utils.configuration: \u001b[0mOverriding option checkpoint.resume_zoo to visual_bert.pretrained.coco\n","\u001b[32m2024-04-22T14:31:45 | mmf.utils.configuration: \u001b[0mOverriding option dataset_config.hateful_memes.annotations.train[0] to hateful_memes/defaults/annotations/train_dev_transform_all_combined.jsonl\n","\u001b[32m2024-04-22T14:31:45 | mmf.utils.configuration: \u001b[0mOverriding option dataset_config.hateful_memes.annotations.val[0] to hateful_memes/defaults/annotations/dev_unseen.jsonl\n","\u001b[32m2024-04-22T14:31:45 | mmf.utils.configuration: \u001b[0mOverriding option dataset_config.hateful_memes.annotations.test[0] to hateful_memes/defaults/annotations/test_unseen.jsonl\n","/usr/local/lib/python3.10/dist-packages/omegaconf/grammar_visitor.py:253: UserWarning: In the sequence `MMF_LOG_DIR,` some elements are missing: please replace them with empty quoted strings. See https://github.com/omry/omegaconf/issues/572 for details.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/omegaconf/grammar_visitor.py:253: UserWarning: In the sequence `MMF_REPORT_DIR,` some elements are missing: please replace them with empty quoted strings. See https://github.com/omry/omegaconf/issues/572 for details.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/omegaconf/grammar_visitor.py:253: UserWarning: In the sequence `MMF_TENSORBOARD_LOGDIR,` some elements are missing: please replace them with empty quoted strings. See https://github.com/omry/omegaconf/issues/572 for details.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/omegaconf/grammar_visitor.py:253: UserWarning: In the sequence `MMF_WANDB_LOGDIR,` some elements are missing: please replace them with empty quoted strings. See https://github.com/omry/omegaconf/issues/572 for details.\n","  warnings.warn(\n","\u001b[32m2024-04-22T14:31:45 | mmf: \u001b[0mLogging to: ./save/train.log\n","\u001b[32m2024-04-22T14:31:45 | mmf_cli.run: \u001b[0mNamespace(config_override=None, local_rank=None, opts=['config=./configs/defaults.yaml', 'model=visual_bert', 'dataset=hateful_memes', 'run_type=train_val', 'training.log_interval=200', 'training.batch_size=64', 'training.evaluation_interval=200', 'training.tensorboard=True', 'training.checkpoint_interval=200', 'checkpoint.resume_pretrained=True', 'checkpoint.resume_zoo=visual_bert.pretrained.coco', 'dataset_config.hateful_memes.annotations.train[0]=hateful_memes/defaults/annotations/train_dev_transform_all_combined.jsonl', 'dataset_config.hateful_memes.annotations.val[0]=hateful_memes/defaults/annotations/dev_unseen.jsonl', 'dataset_config.hateful_memes.annotations.test[0]=hateful_memes/defaults/annotations/test_unseen.jsonl'])\n","\u001b[32m2024-04-22T14:31:45 | mmf_cli.run: \u001b[0mTorch version: 2.2.0+cu121\n","\u001b[32m2024-04-22T14:31:45 | mmf.utils.general: \u001b[0mCUDA Device 0 is: NVIDIA L4\n","\u001b[32m2024-04-22T14:31:45 | mmf_cli.run: \u001b[0mUsing seed 45890347\n","\u001b[32m2024-04-22T14:31:45 | mmf.trainers.mmf_trainer: \u001b[0mLoading datasets\n","loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--bert-base-uncased/snapshots/86b5e0934494bd15c9632b12f734a8a67f723594/config.json\n","Model config BertConfig {\n","  \"_name_or_path\": \"bert-base-uncased\",\n","  \"architectures\": [\n","    \"BertForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.38.2\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n","loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--bert-base-uncased/snapshots/86b5e0934494bd15c9632b12f734a8a67f723594/vocab.txt\n","loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--bert-base-uncased/snapshots/86b5e0934494bd15c9632b12f734a8a67f723594/tokenizer.json\n","loading file added_tokens.json from cache at None\n","loading file special_tokens_map.json from cache at None\n","loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--bert-base-uncased/snapshots/86b5e0934494bd15c9632b12f734a8a67f723594/tokenizer_config.json\n","loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--bert-base-uncased/snapshots/86b5e0934494bd15c9632b12f734a8a67f723594/config.json\n","Model config BertConfig {\n","  \"_name_or_path\": \"bert-base-uncased\",\n","  \"architectures\": [\n","    \"BertForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.38.2\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n","\u001b[32m2024-04-22T14:31:46 | mmf.datasets.multi_datamodule: \u001b[0mMultitasking disabled by default for single dataset training\n","\u001b[32m2024-04-22T14:31:46 | mmf.datasets.multi_datamodule: \u001b[0mMultitasking disabled by default for single dataset training\n","\u001b[32m2024-04-22T14:31:46 | mmf.datasets.multi_datamodule: \u001b[0mMultitasking disabled by default for single dataset training\n","\u001b[32m2024-04-22T14:31:46 | mmf.trainers.mmf_trainer: \u001b[0mLoading model\n","Model config BertConfig {\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bert_model_name\": \"bert-base-uncased\",\n","  \"bypass_transformer\": false,\n","  \"classifier_dropout\": null,\n","  \"embedding_strategy\": \"plain\",\n","  \"finetune_lr_multiplier\": 1,\n","  \"freeze_base\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"losses\": [\n","    \"cross_entropy\"\n","  ],\n","  \"max_position_embeddings\": 512,\n","  \"model\": \"visual_bert\",\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"pooler_strategy\": \"default\",\n","  \"position_embedding_type\": \"absolute\",\n","  \"random_initialize\": false,\n","  \"special_visual_initialize\": true,\n","  \"training_head_type\": \"classification\",\n","  \"transformers_version\": \"4.38.2\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"visual_embedding_dim\": 2048,\n","  \"vocab_size\": 30522,\n","  \"zerobias\": false\n","}\n","\n","loading weights file model.safetensors from cache at /root/.cache/torch/mmf/distributed_-1/models--bert-base-uncased/snapshots/86b5e0934494bd15c9632b12f734a8a67f723594/model.safetensors\n","Some weights of the model checkpoint at bert-base-uncased were not used when initializing VisualBERTBase: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n","- This IS expected if you are initializing VisualBERTBase from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing VisualBERTBase from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of VisualBERTBase were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['bert.embeddings.position_embeddings_visual.weight', 'bert.embeddings.projection.bias', 'bert.embeddings.projection.weight', 'bert.embeddings.token_type_embeddings_visual.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","\u001b[32m2024-04-22T14:31:46 | mmf.trainers.mmf_trainer: \u001b[0mLoading optimizer\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2024-04-22T14:31:46 | py.warnings: \u001b[0m/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n","\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2024-04-22T14:31:46 | py.warnings: \u001b[0m/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n","\n","\u001b[32m2024-04-22T14:31:46 | mmf.trainers.mmf_trainer: \u001b[0mLoading metrics\n","\u001b[32m2024-04-22T14:31:46 | mmf.utils.checkpoint: \u001b[0mLoading checkpoint\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2024-04-22T14:31:47 | mmf: \u001b[0mKey data_parallel is not present in registry, returning default value of None\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2024-04-22T14:31:47 | mmf: \u001b[0mKey distributed is not present in registry, returning default value of None\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2024-04-22T14:31:47 | mmf: \u001b[0mKey data_parallel is not present in registry, returning default value of None\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2024-04-22T14:31:47 | mmf: \u001b[0mKey distributed is not present in registry, returning default value of None\n","\u001b[32m2024-04-22T14:31:47 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.word_embeddings.weight from model.bert.embeddings.word_embeddings.weight\n","\u001b[32m2024-04-22T14:31:47 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.position_embeddings.weight from model.bert.embeddings.position_embeddings.weight\n","\u001b[32m2024-04-22T14:31:47 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.token_type_embeddings.weight from model.bert.embeddings.token_type_embeddings.weight\n","\u001b[32m2024-04-22T14:31:47 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.LayerNorm.weight from model.bert.embeddings.LayerNorm.weight\n","\u001b[32m2024-04-22T14:31:47 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.LayerNorm.bias from model.bert.embeddings.LayerNorm.bias\n","\u001b[32m2024-04-22T14:31:47 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.token_type_embeddings_visual.weight from model.bert.embeddings.token_type_embeddings_visual.weight\n","\u001b[32m2024-04-22T14:31:47 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.position_embeddings_visual.weight from model.bert.embeddings.position_embeddings_visual.weight\n","\u001b[32m2024-04-22T14:31:47 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.projection.weight from model.bert.embeddings.projection.weight\n","\u001b[32m2024-04-22T14:31:47 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.projection.bias from model.bert.embeddings.projection.bias\n","\u001b[32m2024-04-22T14:31:47 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.self.query.weight from model.bert.encoder.layer.0.attention.self.query.weight\n","\u001b[32m2024-04-22T14:31:47 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.self.query.bias from model.bert.encoder.layer.0.attention.self.query.bias\n","\u001b[32m2024-04-22T14:31:47 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.self.key.weight from model.bert.encoder.layer.0.attention.self.key.weight\n","\u001b[32m2024-04-22T14:31:47 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.self.key.bias from model.bert.encoder.layer.0.attention.self.key.bias\n","\u001b[32m2024-04-22T14:31:47 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.self.value.weight from model.bert.encoder.layer.0.attention.self.value.weight\n","\u001b[32m2024-04-22T14:31:47 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.self.value.bias from model.bert.encoder.layer.0.attention.self.value.bias\n","\u001b[32m2024-04-22T14:31:47 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.output.dense.weight from model.bert.encoder.layer.0.attention.output.dense.weight\n","\u001b[32m2024-04-22T14:31:47 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.output.dense.bias from model.bert.encoder.layer.0.attention.output.dense.bias\n","\u001b[32m2024-04-22T14:31:47 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.output.LayerNorm.weight from model.bert.encoder.layer.0.attention.output.LayerNorm.weight\n","\u001b[32m2024-04-22T14:31:47 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.output.LayerNorm.bias from model.bert.encoder.layer.0.attention.output.LayerNorm.bias\n","\u001b[32m2024-04-22T14:31:47 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.intermediate.dense.weight from model.bert.encoder.layer.0.intermediate.dense.weight\n","\u001b[32m2024-04-22T14:31:47 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.intermediate.dense.bias from model.bert.encoder.layer.0.intermediate.dense.bias\n","\u001b[32m2024-04-22T14:31:47 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.output.dense.weight from model.bert.encoder.layer.0.output.dense.weight\n","\u001b[32m2024-04-22T14:31:47 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.output.dense.bias from model.bert.encoder.layer.0.output.dense.bias\n","\u001b[32m2024-04-22T14:31:47 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.output.LayerNorm.weight from model.bert.encoder.layer.0.output.LayerNorm.weight\n","\u001b[32m2024-04-22T14:31:47 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.output.LayerNorm.bias from model.bert.encoder.layer.0.output.LayerNorm.bias\n","\u001b[32m2024-04-22T14:31:47 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.self.query.weight from model.bert.encoder.layer.1.attention.self.query.weight\n","\u001b[32m2024-04-22T14:31:47 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.self.query.bias from model.bert.encoder.layer.1.attention.self.query.bias\n","\u001b[32m2024-04-22T14:31:47 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.self.key.weight from model.bert.encoder.layer.1.attention.self.key.weight\n","\u001b[32m2024-04-22T14:31:47 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.self.key.bias from model.bert.encoder.layer.1.attention.self.key.bias\n","\u001b[32m2024-04-22T14:31:47 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.self.value.weight from model.bert.encoder.layer.1.attention.self.value.weight\n","\u001b[32m2024-04-22T14:31:47 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.self.value.bias from model.bert.encoder.layer.1.attention.self.value.bias\n","\u001b[32m2024-04-22T14:31:47 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.output.dense.weight from model.bert.encoder.layer.1.attention.output.dense.weight\n","\u001b[32m2024-04-22T14:31:47 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.output.dense.bias from model.bert.encoder.layer.1.attention.output.dense.bias\n","\u001b[32m2024-04-22T14:31:47 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.output.LayerNorm.weight from model.bert.encoder.layer.1.attention.output.LayerNorm.weight\n","\u001b[32m2024-04-22T14:31:47 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.output.LayerNorm.bias from model.bert.encoder.layer.1.attention.output.LayerNorm.bias\n","\u001b[32m2024-04-22T14:31:47 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.intermediate.dense.weight from model.bert.encoder.layer.1.intermediate.dense.weight\n","\u001b[32m2024-04-22T14:31:47 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.intermediate.dense.bias from model.bert.encoder.layer.1.intermediate.dense.bias\n","\u001b[32m2024-04-22T14:31:47 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.output.dense.weight from model.bert.encoder.layer.1.output.dense.weight\n","\u001b[32m2024-04-22T14:31:47 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.output.dense.bias from model.bert.encoder.layer.1.output.dense.bias\n","\u001b[32m2024-04-22T14:31:47 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.output.LayerNorm.weight from model.bert.encoder.layer.1.output.LayerNorm.weight\n","\u001b[32m2024-04-22T14:31:47 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.output.LayerNorm.bias from model.bert.encoder.layer.1.output.LayerNorm.bias\n","\u001b[32m2024-04-22T14:31:47 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.self.query.weight from model.bert.encoder.layer.2.attention.self.query.weight\n","\u001b[32m2024-04-22T14:31:47 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.self.query.bias from model.bert.encoder.layer.2.attention.self.query.bias\n","\u001b[32m2024-04-22T14:31:47 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.self.key.weight from model.bert.encoder.layer.2.attention.self.key.weight\n","\u001b[32m2024-04-22T14:31:47 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.self.key.bias from model.bert.encoder.layer.2.attention.self.key.bias\n","\u001b[32m2024-04-22T14:31:47 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.self.value.weight from model.bert.encoder.layer.2.attention.self.value.weight\n","\u001b[32m2024-04-22T14:31:47 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.self.value.bias from model.bert.encoder.layer.2.attention.self.value.bias\n","\u001b[32m2024-04-22T14:31:47 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.output.dense.weight from model.bert.encoder.layer.2.attention.output.dense.weight\n","\u001b[32m2024-04-22T14:31:47 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.output.dense.bias from model.bert.encoder.layer.2.attention.output.dense.bias\n","\u001b[32m2024-04-22T14:31:47 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.output.LayerNorm.weight from model.bert.encoder.layer.2.attention.output.LayerNorm.weight\n","\u001b[32m2024-04-22T14:31:47 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.output.LayerNorm.bias from model.bert.encoder.layer.2.attention.output.LayerNorm.bias\n","\u001b[32m2024-04-22T14:31:47 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.intermediate.dense.weight from model.bert.encoder.layer.2.intermediate.dense.weight\n","\u001b[32m2024-04-22T14:31:47 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.intermediate.dense.bias from model.bert.encoder.layer.2.intermediate.dense.bias\n","\u001b[32m2024-04-22T14:31:47 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.output.dense.weight from model.bert.encoder.layer.2.output.dense.weight\n","\u001b[32m2024-04-22T14:31:47 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.output.dense.bias from model.bert.encoder.layer.2.output.dense.bias\n","\u001b[32m2024-04-22T14:31:47 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.output.LayerNorm.weight from model.bert.encoder.layer.2.output.LayerNorm.weight\n","\u001b[32m2024-04-22T14:31:47 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.output.LayerNorm.bias from model.bert.encoder.layer.2.output.LayerNorm.bias\n","\u001b[32m2024-04-22T14:31:47 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.self.query.weight from model.bert.encoder.layer.3.attention.self.query.weight\n","\u001b[32m2024-04-22T14:31:47 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.self.query.bias from model.bert.encoder.layer.3.attention.self.query.bias\n","\u001b[32m2024-04-22T14:31:47 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.self.key.weight from model.bert.encoder.layer.3.attention.self.key.weight\n","\u001b[32m2024-04-22T14:31:47 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.self.key.bias from model.bert.encoder.layer.3.attention.self.key.bias\n","\u001b[32m2024-04-22T14:31:47 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.self.value.weight from model.bert.encoder.layer.3.attention.self.value.weight\n","\u001b[32m2024-04-22T14:31:47 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.self.value.bias from model.bert.encoder.layer.3.attention.self.value.bias\n","\u001b[32m2024-04-22T14:31:47 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.output.dense.weight from model.bert.encoder.layer.3.attention.output.dense.weight\n","\u001b[32m2024-04-22T14:31:47 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.output.dense.bias from model.bert.encoder.layer.3.attention.output.dense.bias\n","\u001b[32m2024-04-22T14:31:47 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.output.LayerNorm.weight from model.bert.encoder.layer.3.attention.output.LayerNorm.weight\n","\u001b[32m2024-04-22T14:31:47 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.output.LayerNorm.bias from model.bert.encoder.layer.3.attention.output.LayerNorm.bias\n","\u001b[32m2024-04-22T14:31:47 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.intermediate.dense.weight from model.bert.encoder.layer.3.intermediate.dense.weight\n","\u001b[32m2024-04-22T14:31:47 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.intermediate.dense.bias from model.bert.encoder.layer.3.intermediate.dense.bias\n","\u001b[32m2024-04-22T14:31:47 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.output.dense.weight from model.bert.encoder.layer.3.output.dense.weight\n","\u001b[32m2024-04-22T14:31:47 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.output.dense.bias from model.bert.encoder.layer.3.output.dense.bias\n","\u001b[32m2024-04-22T14:31:47 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.output.LayerNorm.weight from model.bert.encoder.layer.3.output.LayerNorm.weight\n","\u001b[32m2024-04-22T14:31:47 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.output.LayerNorm.bias from model.bert.encoder.layer.3.output.LayerNorm.bias\n","\u001b[32m2024-04-22T14:31:47 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.self.query.weight from model.bert.encoder.layer.4.attention.self.query.weight\n","\u001b[32m2024-04-22T14:31:47 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.self.query.bias from model.bert.encoder.layer.4.attention.self.query.bias\n","\u001b[32m2024-04-22T14:31:47 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.self.key.weight from model.bert.encoder.layer.4.attention.self.key.weight\n","\u001b[32m2024-04-22T14:31:47 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.self.key.bias from model.bert.encoder.layer.4.attention.self.key.bias\n","\u001b[32m2024-04-22T14:31:47 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.self.value.weight from model.bert.encoder.layer.4.attention.self.value.weight\n","\u001b[32m2024-04-22T14:31:47 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.self.value.bias from model.bert.encoder.layer.4.attention.self.value.bias\n","\u001b[32m2024-04-22T14:31:47 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.output.dense.weight from model.bert.encoder.layer.4.attention.output.dense.weight\n","\u001b[32m2024-04-22T14:31:47 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.output.dense.bias from model.bert.encoder.layer.4.attention.output.dense.bias\n","\u001b[32m2024-04-22T14:31:47 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.output.LayerNorm.weight from model.bert.encoder.layer.4.attention.output.LayerNorm.weight\n","\u001b[32m2024-04-22T14:31:47 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.output.LayerNorm.bias from model.bert.encoder.layer.4.attention.output.LayerNorm.bias\n","\u001b[32m2024-04-22T14:31:47 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.intermediate.dense.weight from model.bert.encoder.layer.4.intermediate.dense.weight\n","\u001b[32m2024-04-22T14:31:47 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.intermediate.dense.bias from model.bert.encoder.layer.4.intermediate.dense.bias\n","\u001b[32m2024-04-22T14:31:47 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.output.dense.weight from model.bert.encoder.layer.4.output.dense.weight\n","\u001b[32m2024-04-22T14:31:47 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.output.dense.bias from model.bert.encoder.layer.4.output.dense.bias\n","\u001b[32m2024-04-22T14:31:47 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.output.LayerNorm.weight from model.bert.encoder.layer.4.output.LayerNorm.weight\n","\u001b[32m2024-04-22T14:31:47 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.output.LayerNorm.bias from model.bert.encoder.layer.4.output.LayerNorm.bias\n","\u001b[32m2024-04-22T14:31:47 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.self.query.weight from model.bert.encoder.layer.5.attention.self.query.weight\n","\u001b[32m2024-04-22T14:31:47 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.self.query.bias from model.bert.encoder.layer.5.attention.self.query.bias\n","\u001b[32m2024-04-22T14:31:47 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.self.key.weight from model.bert.encoder.layer.5.attention.self.key.weight\n","\u001b[32m2024-04-22T14:31:47 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.self.key.bias from model.bert.encoder.layer.5.attention.self.key.bias\n","\u001b[32m2024-04-22T14:31:47 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.self.value.weight from model.bert.encoder.layer.5.attention.self.value.weight\n","\u001b[32m2024-04-22T14:31:47 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.self.value.bias from model.bert.encoder.layer.5.attention.self.value.bias\n","\u001b[32m2024-04-22T14:31:47 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.output.dense.weight from model.bert.encoder.layer.5.attention.output.dense.weight\n","\u001b[32m2024-04-22T14:31:47 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.output.dense.bias from model.bert.encoder.layer.5.attention.output.dense.bias\n","\u001b[32m2024-04-22T14:31:47 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.output.LayerNorm.weight from model.bert.encoder.layer.5.attention.output.LayerNorm.weight\n","\u001b[32m2024-04-22T14:31:47 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.output.LayerNorm.bias from model.bert.encoder.layer.5.attention.output.LayerNorm.bias\n","\u001b[32m2024-04-22T14:31:47 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.intermediate.dense.weight from model.bert.encoder.layer.5.intermediate.dense.weight\n","\u001b[32m2024-04-22T14:31:47 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.intermediate.dense.bias from model.bert.encoder.layer.5.intermediate.dense.bias\n","\u001b[32m2024-04-22T14:31:47 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.output.dense.weight from model.bert.encoder.layer.5.output.dense.weight\n","\u001b[32m2024-04-22T14:31:47 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.output.dense.bias from model.bert.encoder.layer.5.output.dense.bias\n","\u001b[32m2024-04-22T14:31:47 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.output.LayerNorm.weight from model.bert.encoder.layer.5.output.LayerNorm.weight\n","\u001b[32m2024-04-22T14:31:47 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.output.LayerNorm.bias from model.bert.encoder.layer.5.output.LayerNorm.bias\n","\u001b[32m2024-04-22T14:31:47 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.self.query.weight from model.bert.encoder.layer.6.attention.self.query.weight\n","\u001b[32m2024-04-22T14:31:47 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.self.query.bias from model.bert.encoder.layer.6.attention.self.query.bias\n","\u001b[32m2024-04-22T14:31:47 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.self.key.weight from model.bert.encoder.layer.6.attention.self.key.weight\n","\u001b[32m2024-04-22T14:31:47 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.self.key.bias from model.bert.encoder.layer.6.attention.self.key.bias\n","\u001b[32m2024-04-22T14:31:47 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.self.value.weight from model.bert.encoder.layer.6.attention.self.value.weight\n","\u001b[32m2024-04-22T14:31:47 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.self.value.bias from model.bert.encoder.layer.6.attention.self.value.bias\n","\u001b[32m2024-04-22T14:31:47 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.output.dense.weight from model.bert.encoder.layer.6.attention.output.dense.weight\n","\u001b[32m2024-04-22T14:31:47 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.output.dense.bias from model.bert.encoder.layer.6.attention.output.dense.bias\n","\u001b[32m2024-04-22T14:31:47 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.output.LayerNorm.weight from model.bert.encoder.layer.6.attention.output.LayerNorm.weight\n","\u001b[32m2024-04-22T14:31:47 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.output.LayerNorm.bias from model.bert.encoder.layer.6.attention.output.LayerNorm.bias\n","\u001b[32m2024-04-22T14:31:47 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.intermediate.dense.weight from model.bert.encoder.layer.6.intermediate.dense.weight\n","\u001b[32m2024-04-22T14:31:47 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.intermediate.dense.bias from model.bert.encoder.layer.6.intermediate.dense.bias\n","\u001b[32m2024-04-22T14:31:47 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.output.dense.weight from model.bert.encoder.layer.6.output.dense.weight\n","\u001b[32m2024-04-22T14:31:47 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.output.dense.bias from model.bert.encoder.layer.6.output.dense.bias\n","\u001b[32m2024-04-22T14:31:47 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.output.LayerNorm.weight from model.bert.encoder.layer.6.output.LayerNorm.weight\n","\u001b[32m2024-04-22T14:31:47 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.output.LayerNorm.bias from model.bert.encoder.layer.6.output.LayerNorm.bias\n","\u001b[32m2024-04-22T14:31:47 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.self.query.weight from model.bert.encoder.layer.7.attention.self.query.weight\n","\u001b[32m2024-04-22T14:31:47 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.self.query.bias from model.bert.encoder.layer.7.attention.self.query.bias\n","\u001b[32m2024-04-22T14:31:47 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.self.key.weight from model.bert.encoder.layer.7.attention.self.key.weight\n","\u001b[32m2024-04-22T14:31:47 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.self.key.bias from model.bert.encoder.layer.7.attention.self.key.bias\n","\u001b[32m2024-04-22T14:31:47 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.self.value.weight from model.bert.encoder.layer.7.attention.self.value.weight\n","\u001b[32m2024-04-22T14:31:47 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.self.value.bias from model.bert.encoder.layer.7.attention.self.value.bias\n","\u001b[32m2024-04-22T14:31:47 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.output.dense.weight from model.bert.encoder.layer.7.attention.output.dense.weight\n","\u001b[32m2024-04-22T14:31:47 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.output.dense.bias from model.bert.encoder.layer.7.attention.output.dense.bias\n","\u001b[32m2024-04-22T14:31:47 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.output.LayerNorm.weight from model.bert.encoder.layer.7.attention.output.LayerNorm.weight\n","\u001b[32m2024-04-22T14:31:47 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.output.LayerNorm.bias from model.bert.encoder.layer.7.attention.output.LayerNorm.bias\n","\u001b[32m2024-04-22T14:31:47 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.intermediate.dense.weight from model.bert.encoder.layer.7.intermediate.dense.weight\n","\u001b[32m2024-04-22T14:31:47 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.intermediate.dense.bias from model.bert.encoder.layer.7.intermediate.dense.bias\n","\u001b[32m2024-04-22T14:31:47 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.output.dense.weight from model.bert.encoder.layer.7.output.dense.weight\n","\u001b[32m2024-04-22T14:31:47 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.output.dense.bias from model.bert.encoder.layer.7.output.dense.bias\n","\u001b[32m2024-04-22T14:31:47 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.output.LayerNorm.weight from model.bert.encoder.layer.7.output.LayerNorm.weight\n","\u001b[32m2024-04-22T14:31:47 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.output.LayerNorm.bias from model.bert.encoder.layer.7.output.LayerNorm.bias\n","\u001b[32m2024-04-22T14:31:47 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.self.query.weight from model.bert.encoder.layer.8.attention.self.query.weight\n","\u001b[32m2024-04-22T14:31:47 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.self.query.bias from model.bert.encoder.layer.8.attention.self.query.bias\n","\u001b[32m2024-04-22T14:31:47 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.self.key.weight from model.bert.encoder.layer.8.attention.self.key.weight\n","\u001b[32m2024-04-22T14:31:47 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.self.key.bias from model.bert.encoder.layer.8.attention.self.key.bias\n","\u001b[32m2024-04-22T14:31:47 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.self.value.weight from model.bert.encoder.layer.8.attention.self.value.weight\n","\u001b[32m2024-04-22T14:31:47 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.self.value.bias from model.bert.encoder.layer.8.attention.self.value.bias\n","\u001b[32m2024-04-22T14:31:47 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.output.dense.weight from model.bert.encoder.layer.8.attention.output.dense.weight\n","\u001b[32m2024-04-22T14:31:47 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.output.dense.bias from model.bert.encoder.layer.8.attention.output.dense.bias\n","\u001b[32m2024-04-22T14:31:47 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.output.LayerNorm.weight from model.bert.encoder.layer.8.attention.output.LayerNorm.weight\n","\u001b[32m2024-04-22T14:31:47 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.output.LayerNorm.bias from model.bert.encoder.layer.8.attention.output.LayerNorm.bias\n","\u001b[32m2024-04-22T14:31:47 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.intermediate.dense.weight from model.bert.encoder.layer.8.intermediate.dense.weight\n","\u001b[32m2024-04-22T14:31:47 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.intermediate.dense.bias from model.bert.encoder.layer.8.intermediate.dense.bias\n","\u001b[32m2024-04-22T14:31:47 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.output.dense.weight from model.bert.encoder.layer.8.output.dense.weight\n","\u001b[32m2024-04-22T14:31:47 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.output.dense.bias from model.bert.encoder.layer.8.output.dense.bias\n","\u001b[32m2024-04-22T14:31:47 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.output.LayerNorm.weight from model.bert.encoder.layer.8.output.LayerNorm.weight\n","\u001b[32m2024-04-22T14:31:47 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.output.LayerNorm.bias from model.bert.encoder.layer.8.output.LayerNorm.bias\n","\u001b[32m2024-04-22T14:31:47 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.self.query.weight from model.bert.encoder.layer.9.attention.self.query.weight\n","\u001b[32m2024-04-22T14:31:47 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.self.query.bias from model.bert.encoder.layer.9.attention.self.query.bias\n","\u001b[32m2024-04-22T14:31:47 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.self.key.weight from model.bert.encoder.layer.9.attention.self.key.weight\n","\u001b[32m2024-04-22T14:31:47 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.self.key.bias from model.bert.encoder.layer.9.attention.self.key.bias\n","\u001b[32m2024-04-22T14:31:47 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.self.value.weight from model.bert.encoder.layer.9.attention.self.value.weight\n","\u001b[32m2024-04-22T14:31:47 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.self.value.bias from model.bert.encoder.layer.9.attention.self.value.bias\n","\u001b[32m2024-04-22T14:31:47 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.output.dense.weight from model.bert.encoder.layer.9.attention.output.dense.weight\n","\u001b[32m2024-04-22T14:31:47 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.output.dense.bias from model.bert.encoder.layer.9.attention.output.dense.bias\n","\u001b[32m2024-04-22T14:31:47 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.output.LayerNorm.weight from model.bert.encoder.layer.9.attention.output.LayerNorm.weight\n","\u001b[32m2024-04-22T14:31:47 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.output.LayerNorm.bias from model.bert.encoder.layer.9.attention.output.LayerNorm.bias\n","\u001b[32m2024-04-22T14:31:47 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.intermediate.dense.weight from model.bert.encoder.layer.9.intermediate.dense.weight\n","\u001b[32m2024-04-22T14:31:47 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.intermediate.dense.bias from model.bert.encoder.layer.9.intermediate.dense.bias\n","\u001b[32m2024-04-22T14:31:47 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.output.dense.weight from model.bert.encoder.layer.9.output.dense.weight\n","\u001b[32m2024-04-22T14:31:47 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.output.dense.bias from model.bert.encoder.layer.9.output.dense.bias\n","\u001b[32m2024-04-22T14:31:47 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.output.LayerNorm.weight from model.bert.encoder.layer.9.output.LayerNorm.weight\n","\u001b[32m2024-04-22T14:31:47 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.output.LayerNorm.bias from model.bert.encoder.layer.9.output.LayerNorm.bias\n","\u001b[32m2024-04-22T14:31:47 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.self.query.weight from model.bert.encoder.layer.10.attention.self.query.weight\n","\u001b[32m2024-04-22T14:31:47 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.self.query.bias from model.bert.encoder.layer.10.attention.self.query.bias\n","\u001b[32m2024-04-22T14:31:47 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.self.key.weight from model.bert.encoder.layer.10.attention.self.key.weight\n","\u001b[32m2024-04-22T14:31:47 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.self.key.bias from model.bert.encoder.layer.10.attention.self.key.bias\n","\u001b[32m2024-04-22T14:31:47 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.self.value.weight from model.bert.encoder.layer.10.attention.self.value.weight\n","\u001b[32m2024-04-22T14:31:47 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.self.value.bias from model.bert.encoder.layer.10.attention.self.value.bias\n","\u001b[32m2024-04-22T14:31:47 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.output.dense.weight from model.bert.encoder.layer.10.attention.output.dense.weight\n","\u001b[32m2024-04-22T14:31:47 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.output.dense.bias from model.bert.encoder.layer.10.attention.output.dense.bias\n","\u001b[32m2024-04-22T14:31:47 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.output.LayerNorm.weight from model.bert.encoder.layer.10.attention.output.LayerNorm.weight\n","\u001b[32m2024-04-22T14:31:47 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.output.LayerNorm.bias from model.bert.encoder.layer.10.attention.output.LayerNorm.bias\n","\u001b[32m2024-04-22T14:31:47 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.intermediate.dense.weight from model.bert.encoder.layer.10.intermediate.dense.weight\n","\u001b[32m2024-04-22T14:31:47 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.intermediate.dense.bias from model.bert.encoder.layer.10.intermediate.dense.bias\n","\u001b[32m2024-04-22T14:31:47 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.output.dense.weight from model.bert.encoder.layer.10.output.dense.weight\n","\u001b[32m2024-04-22T14:31:47 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.output.dense.bias from model.bert.encoder.layer.10.output.dense.bias\n","\u001b[32m2024-04-22T14:31:47 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.output.LayerNorm.weight from model.bert.encoder.layer.10.output.LayerNorm.weight\n","\u001b[32m2024-04-22T14:31:47 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.output.LayerNorm.bias from model.bert.encoder.layer.10.output.LayerNorm.bias\n","\u001b[32m2024-04-22T14:31:47 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.self.query.weight from model.bert.encoder.layer.11.attention.self.query.weight\n","\u001b[32m2024-04-22T14:31:47 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.self.query.bias from model.bert.encoder.layer.11.attention.self.query.bias\n","\u001b[32m2024-04-22T14:31:47 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.self.key.weight from model.bert.encoder.layer.11.attention.self.key.weight\n","\u001b[32m2024-04-22T14:31:47 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.self.key.bias from model.bert.encoder.layer.11.attention.self.key.bias\n","\u001b[32m2024-04-22T14:31:47 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.self.value.weight from model.bert.encoder.layer.11.attention.self.value.weight\n","\u001b[32m2024-04-22T14:31:47 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.self.value.bias from model.bert.encoder.layer.11.attention.self.value.bias\n","\u001b[32m2024-04-22T14:31:47 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.output.dense.weight from model.bert.encoder.layer.11.attention.output.dense.weight\n","\u001b[32m2024-04-22T14:31:47 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.output.dense.bias from model.bert.encoder.layer.11.attention.output.dense.bias\n","\u001b[32m2024-04-22T14:31:47 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.output.LayerNorm.weight from model.bert.encoder.layer.11.attention.output.LayerNorm.weight\n","\u001b[32m2024-04-22T14:31:47 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.output.LayerNorm.bias from model.bert.encoder.layer.11.attention.output.LayerNorm.bias\n","\u001b[32m2024-04-22T14:31:47 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.intermediate.dense.weight from model.bert.encoder.layer.11.intermediate.dense.weight\n","\u001b[32m2024-04-22T14:31:47 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.intermediate.dense.bias from model.bert.encoder.layer.11.intermediate.dense.bias\n","\u001b[32m2024-04-22T14:31:47 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.output.dense.weight from model.bert.encoder.layer.11.output.dense.weight\n","\u001b[32m2024-04-22T14:31:47 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.output.dense.bias from model.bert.encoder.layer.11.output.dense.bias\n","\u001b[32m2024-04-22T14:31:47 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.output.LayerNorm.weight from model.bert.encoder.layer.11.output.LayerNorm.weight\n","\u001b[32m2024-04-22T14:31:47 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.output.LayerNorm.bias from model.bert.encoder.layer.11.output.LayerNorm.bias\n","\u001b[32m2024-04-22T14:31:47 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.pooler.dense.weight from model.bert.pooler.dense.weight\n","\u001b[32m2024-04-22T14:31:47 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.pooler.dense.bias from model.bert.pooler.dense.bias\n","\u001b[32m2024-04-22T14:31:47 | mmf.utils.checkpoint: \u001b[0mPretrained model loaded\n","\u001b[32m2024-04-22T14:31:47 | mmf.utils.checkpoint: \u001b[0mCheckpoint loaded.\n","\u001b[32m2024-04-22T14:31:47 | mmf.utils.checkpoint: \u001b[0mCurrent num updates: 0\n","\u001b[32m2024-04-22T14:31:47 | mmf.utils.checkpoint: \u001b[0mCurrent iteration: 0\n","\u001b[32m2024-04-22T14:31:47 | mmf.utils.checkpoint: \u001b[0mCurrent epoch: 0\n","\u001b[32m2024-04-22T14:31:47 | mmf.trainers.mmf_trainer: \u001b[0m===== Model =====\n","\u001b[32m2024-04-22T14:31:47 | mmf.trainers.mmf_trainer: \u001b[0mVisualBERT(\n","  (model): VisualBERTForClassification(\n","    (bert): VisualBERTBase(\n","      (embeddings): BertVisioLinguisticEmbeddings(\n","        (word_embeddings): Embedding(30522, 768, padding_idx=0)\n","        (position_embeddings): Embedding(512, 768)\n","        (token_type_embeddings): Embedding(2, 768)\n","        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","        (token_type_embeddings_visual): Embedding(2, 768)\n","        (position_embeddings_visual): Embedding(512, 768)\n","        (projection): Linear(in_features=2048, out_features=768, bias=True)\n","      )\n","      (encoder): BertEncoderJit(\n","        (layer): ModuleList(\n","          (0-11): 12 x BertLayerJit(\n","            (attention): BertAttentionJit(\n","              (self): BertSelfAttentionJit(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","              (intermediate_act_fn): GELUActivation()\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","        )\n","      )\n","      (pooler): BertPooler(\n","        (dense): Linear(in_features=768, out_features=768, bias=True)\n","        (activation): Tanh()\n","      )\n","    )\n","    (dropout): Dropout(p=0.1, inplace=False)\n","    (classifier): Sequential(\n","      (0): BertPredictionHeadTransform(\n","        (dense): Linear(in_features=768, out_features=768, bias=True)\n","        (transform_act_fn): GELUActivation()\n","        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      )\n","      (1): Linear(in_features=768, out_features=2, bias=True)\n","    )\n","  )\n","  (losses): Losses(\n","    (losses): ModuleList(\n","      (0): MMFLoss(\n","        (loss_criterion): CrossEntropyLoss(\n","          (loss_fn): CrossEntropyLoss()\n","        )\n","      )\n","    )\n","  )\n",")\n","\u001b[32m2024-04-22T14:31:47 | mmf.utils.general: \u001b[0mTotal Parameters: 112044290. Trained Parameters: 112044290\n","\u001b[32m2024-04-22T14:31:47 | mmf.trainers.core.training_loop: \u001b[0mStarting training...\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2024-04-22T14:31:47 | py.warnings: \u001b[0m/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n","  self.pid = os.fork()\n","\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2024-04-22T14:31:47 | py.warnings: \u001b[0m/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n","  self.pid = os.fork()\n","\n","\u001b[32m2024-04-22T14:35:47 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n","\u001b[32m2024-04-22T14:35:47 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2024-04-22T14:35:51 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2024-04-22T14:35:53 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2024-04-22T14:35:53 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 200/1500, train/hateful_memes/cross_entropy: 0.6604, train/hateful_memes/cross_entropy/avg: 0.6604, train/total_loss: 0.6604, train/total_loss/avg: 0.6604, max mem: 14322.0, experiment: run, epoch: 1, num_updates: 200, iterations: 200, max_updates: 1500, lr: 0.00001, ups: 0.81, time: 04m 06s 660ms, time_since_start: 04m 07s 329ms, eta: 27m 55s 443ms\n","\u001b[32m2024-04-22T14:35:53 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n","\u001b[32m2024-04-22T14:35:53 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n","\u001b[32m2024-04-22T14:35:58 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished evaluation inference. Loaded 9\n","\u001b[32m2024-04-22T14:35:58 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n","\u001b[32m2024-04-22T14:35:58 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2024-04-22T14:36:00 | mmf.utils.checkpoint: \u001b[0mSaving best checkpoint\n","\u001b[32m2024-04-22T14:36:04 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2024-04-22T14:36:12 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2024-04-22T14:36:12 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 200/1500, val/hateful_memes/cross_entropy: 0.6763, val/total_loss: 0.6763, val/hateful_memes/accuracy: 0.6259, val/hateful_memes/binary_f1: 0.0194, val/hateful_memes/roc_auc: 0.5624, num_updates: 200, epoch: 1, iterations: 200, max_updates: 1500, val_time: 18s 638ms, best_update: 200, best_iteration: 200, best_val/hateful_memes/roc_auc: 0.562412\n","\u001b[32m2024-04-22T14:40:11 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n","\u001b[32m2024-04-22T14:40:11 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2024-04-22T14:40:13 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2024-04-22T14:40:18 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2024-04-22T14:40:18 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 400/1500, train/hateful_memes/cross_entropy: 0.4549, train/hateful_memes/cross_entropy/avg: 0.5576, train/total_loss: 0.4549, train/total_loss/avg: 0.5576, max mem: 14371.0, experiment: run, epoch: 2, num_updates: 400, iterations: 400, max_updates: 1500, lr: 0.00001, ups: 0.82, time: 04m 05s 832ms, time_since_start: 08m 31s 805ms, eta: 23m 32s 925ms\n","\u001b[32m2024-04-22T14:40:18 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n","\u001b[32m2024-04-22T14:40:18 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2024-04-22T14:40:18 | py.warnings: \u001b[0m/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n","  self.pid = os.fork()\n","\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2024-04-22T14:40:18 | py.warnings: \u001b[0m/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n","  self.pid = os.fork()\n","\n","\u001b[32m2024-04-22T14:40:22 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished evaluation inference. Loaded 9\n","\u001b[32m2024-04-22T14:40:22 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n","\u001b[32m2024-04-22T14:40:23 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2024-04-22T14:40:25 | mmf.utils.checkpoint: \u001b[0mSaving best checkpoint\n","\u001b[32m2024-04-22T14:40:31 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2024-04-22T14:40:35 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2024-04-22T14:40:35 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 400/1500, val/hateful_memes/cross_entropy: 0.6878, val/total_loss: 0.6878, val/hateful_memes/accuracy: 0.6611, val/hateful_memes/binary_f1: 0.3711, val/hateful_memes/roc_auc: 0.6684, num_updates: 400, epoch: 2, iterations: 400, max_updates: 1500, val_time: 17s 046ms, best_update: 400, best_iteration: 400, best_val/hateful_memes/roc_auc: 0.668353\n","\u001b[32m2024-04-22T14:44:34 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n","\u001b[32m2024-04-22T14:44:34 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2024-04-22T14:44:36 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2024-04-22T14:44:43 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2024-04-22T14:44:43 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 600/1500, train/hateful_memes/cross_entropy: 0.4549, train/hateful_memes/cross_entropy/avg: 0.5014, train/total_loss: 0.4549, train/total_loss/avg: 0.5014, max mem: 14371.0, experiment: run, epoch: 3, num_updates: 600, iterations: 600, max_updates: 1500, lr: 0.00002, ups: 0.81, time: 04m 07s 584ms, time_since_start: 12m 56s 438ms, eta: 19m 24s 266ms\n","\u001b[32m2024-04-22T14:44:43 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n","\u001b[32m2024-04-22T14:44:43 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2024-04-22T14:44:43 | py.warnings: \u001b[0m/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n","  self.pid = os.fork()\n","\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2024-04-22T14:44:43 | py.warnings: \u001b[0m/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n","  self.pid = os.fork()\n","\n","\u001b[32m2024-04-22T14:44:47 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished evaluation inference. Loaded 9\n","\u001b[32m2024-04-22T14:44:47 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n","\u001b[32m2024-04-22T14:44:47 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2024-04-22T14:44:50 | mmf.utils.checkpoint: \u001b[0mSaving best checkpoint\n","\u001b[32m2024-04-22T14:44:54 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2024-04-22T14:45:01 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2024-04-22T14:45:01 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 600/1500, val/hateful_memes/cross_entropy: 0.8084, val/total_loss: 0.8084, val/hateful_memes/accuracy: 0.6741, val/hateful_memes/binary_f1: 0.5028, val/hateful_memes/roc_auc: 0.7063, num_updates: 600, epoch: 3, iterations: 600, max_updates: 1500, val_time: 18s 064ms, best_update: 600, best_iteration: 600, best_val/hateful_memes/roc_auc: 0.706279\n","\u001b[32m2024-04-22T14:49:00 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n","\u001b[32m2024-04-22T14:49:00 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2024-04-22T14:49:03 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2024-04-22T14:49:09 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2024-04-22T14:49:09 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 800/1500, train/hateful_memes/cross_entropy: 0.3889, train/hateful_memes/cross_entropy/avg: 0.4098, train/total_loss: 0.3889, train/total_loss/avg: 0.4098, max mem: 14371.0, experiment: run, epoch: 3, num_updates: 800, iterations: 800, max_updates: 1500, lr: 0.00002, ups: 0.81, time: 04m 07s 903ms, time_since_start: 17m 22s 409ms, eta: 15m 06s 705ms\n","\u001b[32m2024-04-22T14:49:09 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n","\u001b[32m2024-04-22T14:49:09 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2024-04-22T14:49:09 | py.warnings: \u001b[0m/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n","  self.pid = os.fork()\n","\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2024-04-22T14:49:09 | py.warnings: \u001b[0m/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n","  self.pid = os.fork()\n","\n","\u001b[32m2024-04-22T14:49:13 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished evaluation inference. Loaded 9\n","\u001b[32m2024-04-22T14:49:13 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n","\u001b[32m2024-04-22T14:49:13 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2024-04-22T14:49:18 | mmf.utils.checkpoint: \u001b[0mSaving best checkpoint\n","\u001b[32m2024-04-22T14:49:20 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2024-04-22T14:49:23 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2024-04-22T14:49:23 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 800/1500, val/hateful_memes/cross_entropy: 0.9212, val/total_loss: 0.9212, val/hateful_memes/accuracy: 0.6648, val/hateful_memes/binary_f1: 0.4026, val/hateful_memes/roc_auc: 0.7256, num_updates: 800, epoch: 3, iterations: 800, max_updates: 1500, val_time: 14s 881ms, best_update: 800, best_iteration: 800, best_val/hateful_memes/roc_auc: 0.725632\n","\u001b[32m2024-04-22T14:53:21 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n","\u001b[32m2024-04-22T14:53:21 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2024-04-22T14:53:26 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2024-04-22T14:53:28 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2024-04-22T14:53:28 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1000/1500, train/hateful_memes/cross_entropy: 0.3889, train/hateful_memes/cross_entropy/avg: 0.3588, train/total_loss: 0.3889, train/total_loss/avg: 0.3588, max mem: 14371.0, experiment: run, epoch: 4, num_updates: 1000, iterations: 1000, max_updates: 1500, lr: 0.00003, ups: 0.82, time: 04m 05s 047ms, time_since_start: 21m 42s 340ms, eta: 10m 40s 185ms\n","\u001b[32m2024-04-22T14:53:28 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n","\u001b[32m2024-04-22T14:53:29 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2024-04-22T14:53:29 | py.warnings: \u001b[0m/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n","  self.pid = os.fork()\n","\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2024-04-22T14:53:29 | py.warnings: \u001b[0m/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n","  self.pid = os.fork()\n","\n","\u001b[32m2024-04-22T14:53:33 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished evaluation inference. Loaded 9\n","\u001b[32m2024-04-22T14:53:33 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n","\u001b[32m2024-04-22T14:53:33 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2024-04-22T14:53:35 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2024-04-22T14:53:39 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2024-04-22T14:53:39 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1000/1500, val/hateful_memes/cross_entropy: 1.0193, val/total_loss: 1.0193, val/hateful_memes/accuracy: 0.6704, val/hateful_memes/binary_f1: 0.5110, val/hateful_memes/roc_auc: 0.7151, num_updates: 1000, epoch: 4, iterations: 1000, max_updates: 1500, val_time: 10s 223ms, best_update: 800, best_iteration: 800, best_val/hateful_memes/roc_auc: 0.725632\n","\u001b[32m2024-04-22T14:57:37 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n","\u001b[32m2024-04-22T14:57:37 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2024-04-22T14:57:39 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2024-04-22T14:57:45 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2024-04-22T14:57:45 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1200/1500, train/hateful_memes/cross_entropy: 0.1545, train/hateful_memes/cross_entropy/avg: 0.3045, train/total_loss: 0.1545, train/total_loss/avg: 0.3045, max mem: 14372.0, experiment: run, epoch: 5, num_updates: 1200, iterations: 1200, max_updates: 1500, lr: 0.00003, ups: 0.81, time: 04m 06s 386ms, time_since_start: 25m 58s 952ms, eta: 06m 26s 211ms\n","\u001b[32m2024-04-22T14:57:45 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n","\u001b[32m2024-04-22T14:57:45 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2024-04-22T14:57:45 | py.warnings: \u001b[0m/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n","  self.pid = os.fork()\n","\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2024-04-22T14:57:45 | py.warnings: \u001b[0m/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n","  self.pid = os.fork()\n","\n","\u001b[32m2024-04-22T14:57:49 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished evaluation inference. Loaded 9\n","\u001b[32m2024-04-22T14:57:49 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n","\u001b[32m2024-04-22T14:57:50 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2024-04-22T14:57:52 | mmf.utils.checkpoint: \u001b[0mSaving best checkpoint\n","\u001b[32m2024-04-22T14:57:57 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2024-04-22T14:58:00 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2024-04-22T14:58:00 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1200/1500, val/hateful_memes/cross_entropy: 1.1296, val/total_loss: 1.1296, val/hateful_memes/accuracy: 0.6889, val/hateful_memes/binary_f1: 0.5172, val/hateful_memes/roc_auc: 0.7311, num_updates: 1200, epoch: 5, iterations: 1200, max_updates: 1500, val_time: 14s 973ms, best_update: 1200, best_iteration: 1200, best_val/hateful_memes/roc_auc: 0.731118\n","\u001b[32m2024-04-22T15:01:59 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n","\u001b[32m2024-04-22T15:01:59 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2024-04-22T15:02:01 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2024-04-22T15:02:07 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2024-04-22T15:02:07 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1400/1500, train/hateful_memes/cross_entropy: 0.1855, train/hateful_memes/cross_entropy/avg: 0.2875, train/total_loss: 0.1855, train/total_loss/avg: 0.2875, max mem: 14372.0, experiment: run, epoch: 6, num_updates: 1400, iterations: 1400, max_updates: 1500, lr: 0.00003, ups: 0.81, time: 04m 06s 576ms, time_since_start: 30m 20s 504ms, eta: 02m 08s 835ms\n","\u001b[32m2024-04-22T15:02:07 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n","\u001b[32m2024-04-22T15:02:07 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2024-04-22T15:02:07 | py.warnings: \u001b[0m/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n","  self.pid = os.fork()\n","\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2024-04-22T15:02:07 | py.warnings: \u001b[0m/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n","  self.pid = os.fork()\n","\n","\u001b[32m2024-04-22T15:02:11 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished evaluation inference. Loaded 9\n","\u001b[32m2024-04-22T15:02:11 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n","\u001b[32m2024-04-22T15:02:11 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2024-04-22T15:02:14 | mmf.utils.checkpoint: \u001b[0mSaving best checkpoint\n","\u001b[32m2024-04-22T15:02:18 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2024-04-22T15:02:25 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2024-04-22T15:02:25 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1400/1500, val/hateful_memes/cross_entropy: 1.2900, val/total_loss: 1.2900, val/hateful_memes/accuracy: 0.7037, val/hateful_memes/binary_f1: 0.5000, val/hateful_memes/roc_auc: 0.7337, num_updates: 1400, epoch: 6, iterations: 1400, max_updates: 1500, val_time: 18s 392ms, best_update: 1400, best_iteration: 1400, best_val/hateful_memes/roc_auc: 0.733706\n","\u001b[32m2024-04-22T15:04:26 | mmf.trainers.core.training_loop: \u001b[0mStepping into final validation check\n","\u001b[32m2024-04-22T15:04:26 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2024-04-22T15:04:26 | py.warnings: \u001b[0m/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n","  self.pid = os.fork()\n","\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2024-04-22T15:04:26 | py.warnings: \u001b[0m/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n","  self.pid = os.fork()\n","\n","\u001b[32m2024-04-22T15:04:30 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished evaluation inference. Loaded 9\n","\u001b[32m2024-04-22T15:04:30 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n","\u001b[32m2024-04-22T15:04:31 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1500/1500, val/hateful_memes/cross_entropy: 1.5742, val/total_loss: 1.5742, val/hateful_memes/accuracy: 0.6870, val/hateful_memes/binary_f1: 0.4531, val/hateful_memes/roc_auc: 0.7261, num_updates: 1500, epoch: 6, iterations: 1500, max_updates: 1500, val_time: 02m 24s 073ms, best_update: 1400, best_iteration: 1400, best_val/hateful_memes/roc_auc: 0.733706\n","\u001b[32m2024-04-22T15:04:31 | mmf.utils.checkpoint: \u001b[0mRestoring checkpoint\n","\u001b[32m2024-04-22T15:04:31 | mmf.utils.checkpoint: \u001b[0mLoading checkpoint\n","\u001b[32m2024-04-22T15:04:32 | mmf.utils.checkpoint: \u001b[0mCheckpoint loaded.\n","\u001b[32m2024-04-22T15:04:32 | mmf.utils.checkpoint: \u001b[0mCurrent num updates: 1400\n","\u001b[32m2024-04-22T15:04:32 | mmf.utils.checkpoint: \u001b[0mCurrent iteration: 1400\n","\u001b[32m2024-04-22T15:04:32 | mmf.utils.checkpoint: \u001b[0mCurrent epoch: 6\n","\u001b[32m2024-04-22T15:04:33 | mmf.trainers.mmf_trainer: \u001b[0mStarting inference on val set\n","\u001b[32m2024-04-22T15:04:33 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n","  0% 0/9 [00:00<?, ?it/s]\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2024-04-22T15:04:33 | py.warnings: \u001b[0m/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n","  self.pid = os.fork()\n","\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2024-04-22T15:04:33 | py.warnings: \u001b[0m/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n","  self.pid = os.fork()\n","\n","100% 9/9 [00:04<00:00,  2.01it/s]\n","\u001b[32m2024-04-22T15:04:37 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished evaluation inference. Loaded 9\n","\u001b[32m2024-04-22T15:04:37 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n","\u001b[32m2024-04-22T15:04:37 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1400/1500, val/hateful_memes/cross_entropy: 1.2900, val/total_loss: 1.2900, val/hateful_memes/accuracy: 0.7037, val/hateful_memes/binary_f1: 0.5000, val/hateful_memes/roc_auc: 0.7337\n","\u001b[32m2024-04-22T15:04:37 | mmf.trainers.callbacks.logistics: \u001b[0mFinished run in 32m 51s 192ms\n"]}]},{"cell_type":"markdown","source":["## Pretrained with VizWiz"],"metadata":{"id":"wGKPgBv4Bh72"}},{"cell_type":"markdown","source":["### Dataset: Default"],"metadata":{"id":"J3fUue7SNwen"}},{"cell_type":"code","source":["!mmf_run config=\"./configs/defaults.yaml\" \\\n","  model=visual_bert \\\n","  dataset=hateful_memes \\\n","  run_type=train_val \\\n","  training.log_interval=200 \\\n","  training.max_updates=1500 \\\n","  training.batch_size=64 \\\n","  training.evaluation_interval=200 \\\n","  training.tensorboard=True \\\n","  training.checkpoint_interval=200 \\\n","  checkpoint.resume_pretrained=True \\\n","  checkpoint.resume_file=\"/content/drive/MyDrive/hateful-memes/Deep Learning Group Project/visualBERT_vizwiz_04212024_pretrained.ckpt\" \\\n","  dataset_config.hateful_memes.annotations.train[0]=\"hateful_memes/defaults/annotations/train.jsonl\" \\\n","  dataset_config.hateful_memes.annotations.val[0]=\"hateful_memes/defaults/annotations/dev_unseen.jsonl\" \\\n","  dataset_config.hateful_memes.annotations.test[0]=\"hateful_memes/defaults/annotations/test_unseen.jsonl\""],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ASWcd-ltNzDN","executionInfo":{"status":"ok","timestamp":1713916439819,"user_tz":420,"elapsed":1977792,"user":{"displayName":"Aneri Shah","userId":"14009776940163552827"}},"outputId":"04f6ba71-f1b3-4dc8-e0e7-b03dd3083e65"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["2024-04-23 23:21:05.146836: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","2024-04-23 23:21:05.146890: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","2024-04-23 23:21:05.148719: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","2024-04-23 23:21:06.248358: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","2024-04-23 23:21:10.895631: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:47] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n","/usr/local/lib/python3.10/dist-packages/transformers/deepspeed.py:23: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/omegaconf/grammar_visitor.py:253: UserWarning: In the sequence `MMF_USER_DIR,` some elements are missing: please replace them with empty quoted strings. See https://github.com/omry/omegaconf/issues/572 for details.\n","  warnings.warn(\n","\u001b[32m2024-04-23T23:21:15 | mmf.utils.configuration: \u001b[0mOverriding option config to ./configs/defaults.yaml\n","\u001b[32m2024-04-23T23:21:15 | mmf.utils.configuration: \u001b[0mOverriding option model to visual_bert\n","\u001b[32m2024-04-23T23:21:15 | mmf.utils.configuration: \u001b[0mOverriding option datasets to hateful_memes\n","\u001b[32m2024-04-23T23:21:15 | mmf.utils.configuration: \u001b[0mOverriding option run_type to train_val\n","\u001b[32m2024-04-23T23:21:15 | mmf.utils.configuration: \u001b[0mOverriding option training.log_interval to 200\n","\u001b[32m2024-04-23T23:21:15 | mmf.utils.configuration: \u001b[0mOverriding option training.max_updates to 1500\n","\u001b[32m2024-04-23T23:21:15 | mmf.utils.configuration: \u001b[0mOverriding option training.batch_size to 64\n","\u001b[32m2024-04-23T23:21:15 | mmf.utils.configuration: \u001b[0mOverriding option training.evaluation_interval to 200\n","\u001b[32m2024-04-23T23:21:15 | mmf.utils.configuration: \u001b[0mOverriding option training.tensorboard to True\n","\u001b[32m2024-04-23T23:21:15 | mmf.utils.configuration: \u001b[0mOverriding option training.checkpoint_interval to 200\n","\u001b[32m2024-04-23T23:21:15 | mmf.utils.configuration: \u001b[0mOverriding option checkpoint.resume_pretrained to True\n","\u001b[32m2024-04-23T23:21:15 | mmf.utils.configuration: \u001b[0mOverriding option checkpoint.resume_file to /content/drive/MyDrive/hateful-memes/Deep Learning Group Project/visualBERT_vizwiz_04212024_pretrained.ckpt\n","\u001b[32m2024-04-23T23:21:15 | mmf.utils.configuration: \u001b[0mOverriding option dataset_config.hateful_memes.annotations.train[0] to hateful_memes/defaults/annotations/train.jsonl\n","\u001b[32m2024-04-23T23:21:15 | mmf.utils.configuration: \u001b[0mOverriding option dataset_config.hateful_memes.annotations.val[0] to hateful_memes/defaults/annotations/dev_unseen.jsonl\n","\u001b[32m2024-04-23T23:21:15 | mmf.utils.configuration: \u001b[0mOverriding option dataset_config.hateful_memes.annotations.test[0] to hateful_memes/defaults/annotations/test_unseen.jsonl\n","/usr/local/lib/python3.10/dist-packages/omegaconf/grammar_visitor.py:253: UserWarning: In the sequence `MMF_LOG_DIR,` some elements are missing: please replace them with empty quoted strings. See https://github.com/omry/omegaconf/issues/572 for details.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/omegaconf/grammar_visitor.py:253: UserWarning: In the sequence `MMF_REPORT_DIR,` some elements are missing: please replace them with empty quoted strings. See https://github.com/omry/omegaconf/issues/572 for details.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/omegaconf/grammar_visitor.py:253: UserWarning: In the sequence `MMF_TENSORBOARD_LOGDIR,` some elements are missing: please replace them with empty quoted strings. See https://github.com/omry/omegaconf/issues/572 for details.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/omegaconf/grammar_visitor.py:253: UserWarning: In the sequence `MMF_WANDB_LOGDIR,` some elements are missing: please replace them with empty quoted strings. See https://github.com/omry/omegaconf/issues/572 for details.\n","  warnings.warn(\n","\u001b[32m2024-04-23T23:21:15 | mmf: \u001b[0mLogging to: ./save/train.log\n","\u001b[32m2024-04-23T23:21:15 | mmf_cli.run: \u001b[0mNamespace(config_override=None, local_rank=None, opts=['config=./configs/defaults.yaml', 'model=visual_bert', 'dataset=hateful_memes', 'run_type=train_val', 'training.log_interval=200', 'training.max_updates=1500', 'training.batch_size=64', 'training.evaluation_interval=200', 'training.tensorboard=True', 'training.checkpoint_interval=200', 'checkpoint.resume_pretrained=True', 'checkpoint.resume_file=/content/drive/MyDrive/hateful-memes/Deep Learning Group Project/visualBERT_vizwiz_04212024_pretrained.ckpt', 'dataset_config.hateful_memes.annotations.train[0]=hateful_memes/defaults/annotations/train.jsonl', 'dataset_config.hateful_memes.annotations.val[0]=hateful_memes/defaults/annotations/dev_unseen.jsonl', 'dataset_config.hateful_memes.annotations.test[0]=hateful_memes/defaults/annotations/test_unseen.jsonl'])\n","\u001b[32m2024-04-23T23:21:15 | mmf_cli.run: \u001b[0mTorch version: 2.2.0+cu121\n","\u001b[32m2024-04-23T23:21:15 | mmf.utils.general: \u001b[0mCUDA Device 0 is: NVIDIA L4\n","\u001b[32m2024-04-23T23:21:15 | mmf_cli.run: \u001b[0mUsing seed 15827399\n","\u001b[32m2024-04-23T23:21:15 | mmf.trainers.mmf_trainer: \u001b[0mLoading datasets\n","loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--bert-base-uncased/snapshots/86b5e0934494bd15c9632b12f734a8a67f723594/config.json\n","Model config BertConfig {\n","  \"_name_or_path\": \"bert-base-uncased\",\n","  \"architectures\": [\n","    \"BertForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.39.2\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n","loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--bert-base-uncased/snapshots/86b5e0934494bd15c9632b12f734a8a67f723594/vocab.txt\n","loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--bert-base-uncased/snapshots/86b5e0934494bd15c9632b12f734a8a67f723594/tokenizer.json\n","loading file added_tokens.json from cache at None\n","loading file special_tokens_map.json from cache at None\n","loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--bert-base-uncased/snapshots/86b5e0934494bd15c9632b12f734a8a67f723594/tokenizer_config.json\n","loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--bert-base-uncased/snapshots/86b5e0934494bd15c9632b12f734a8a67f723594/config.json\n","Model config BertConfig {\n","  \"_name_or_path\": \"bert-base-uncased\",\n","  \"architectures\": [\n","    \"BertForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.39.2\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n","\u001b[32m2024-04-23T23:21:16 | mmf.datasets.multi_datamodule: \u001b[0mMultitasking disabled by default for single dataset training\n","\u001b[32m2024-04-23T23:21:16 | mmf.datasets.multi_datamodule: \u001b[0mMultitasking disabled by default for single dataset training\n","\u001b[32m2024-04-23T23:21:16 | mmf.datasets.multi_datamodule: \u001b[0mMultitasking disabled by default for single dataset training\n","\u001b[32m2024-04-23T23:21:16 | mmf.trainers.mmf_trainer: \u001b[0mLoading model\n","Model config BertConfig {\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bert_model_name\": \"bert-base-uncased\",\n","  \"bypass_transformer\": false,\n","  \"classifier_dropout\": null,\n","  \"embedding_strategy\": \"plain\",\n","  \"finetune_lr_multiplier\": 1,\n","  \"freeze_base\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"losses\": [\n","    \"cross_entropy\"\n","  ],\n","  \"max_position_embeddings\": 512,\n","  \"model\": \"visual_bert\",\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"pooler_strategy\": \"default\",\n","  \"position_embedding_type\": \"absolute\",\n","  \"random_initialize\": false,\n","  \"special_visual_initialize\": true,\n","  \"training_head_type\": \"classification\",\n","  \"transformers_version\": \"4.39.2\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"visual_embedding_dim\": 2048,\n","  \"vocab_size\": 30522,\n","  \"zerobias\": false\n","}\n","\n","loading weights file model.safetensors from cache at /root/.cache/torch/mmf/distributed_-1/models--bert-base-uncased/snapshots/86b5e0934494bd15c9632b12f734a8a67f723594/model.safetensors\n","Some weights of the model checkpoint at bert-base-uncased were not used when initializing VisualBERTBase: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n","- This IS expected if you are initializing VisualBERTBase from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing VisualBERTBase from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of VisualBERTBase were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['bert.embeddings.position_embeddings_visual.weight', 'bert.embeddings.projection.bias', 'bert.embeddings.projection.weight', 'bert.embeddings.token_type_embeddings_visual.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","\u001b[32m2024-04-23T23:21:16 | mmf.trainers.mmf_trainer: \u001b[0mLoading optimizer\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2024-04-23T23:21:16 | py.warnings: \u001b[0m/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:457: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n","\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2024-04-23T23:21:16 | py.warnings: \u001b[0m/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:457: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n","\n","\u001b[32m2024-04-23T23:21:16 | mmf.trainers.mmf_trainer: \u001b[0mLoading metrics\n","\u001b[32m2024-04-23T23:21:16 | mmf.utils.checkpoint: \u001b[0mLoading checkpoint\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2024-04-23T23:21:31 | mmf: \u001b[0mKey data_parallel is not present in registry, returning default value of None\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2024-04-23T23:21:31 | mmf: \u001b[0mKey distributed is not present in registry, returning default value of None\n","\u001b[32m2024-04-23T23:21:31 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.word_embeddings.weight from model.bert.embeddings.word_embeddings.weight\n","\u001b[32m2024-04-23T23:21:31 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.position_embeddings.weight from model.bert.embeddings.position_embeddings.weight\n","\u001b[32m2024-04-23T23:21:31 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.token_type_embeddings.weight from model.bert.embeddings.token_type_embeddings.weight\n","\u001b[32m2024-04-23T23:21:31 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.LayerNorm.weight from model.bert.embeddings.LayerNorm.weight\n","\u001b[32m2024-04-23T23:21:31 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.LayerNorm.bias from model.bert.embeddings.LayerNorm.bias\n","\u001b[32m2024-04-23T23:21:31 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.token_type_embeddings_visual.weight from model.bert.embeddings.token_type_embeddings_visual.weight\n","\u001b[32m2024-04-23T23:21:31 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.position_embeddings_visual.weight from model.bert.embeddings.position_embeddings_visual.weight\n","\u001b[32m2024-04-23T23:21:31 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.projection.weight from model.bert.embeddings.projection.weight\n","\u001b[32m2024-04-23T23:21:31 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.projection.bias from model.bert.embeddings.projection.bias\n","\u001b[32m2024-04-23T23:21:31 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.self.query.weight from model.bert.encoder.layer.0.attention.self.query.weight\n","\u001b[32m2024-04-23T23:21:31 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.self.query.bias from model.bert.encoder.layer.0.attention.self.query.bias\n","\u001b[32m2024-04-23T23:21:31 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.self.key.weight from model.bert.encoder.layer.0.attention.self.key.weight\n","\u001b[32m2024-04-23T23:21:31 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.self.key.bias from model.bert.encoder.layer.0.attention.self.key.bias\n","\u001b[32m2024-04-23T23:21:31 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.self.value.weight from model.bert.encoder.layer.0.attention.self.value.weight\n","\u001b[32m2024-04-23T23:21:31 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.self.value.bias from model.bert.encoder.layer.0.attention.self.value.bias\n","\u001b[32m2024-04-23T23:21:31 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.output.dense.weight from model.bert.encoder.layer.0.attention.output.dense.weight\n","\u001b[32m2024-04-23T23:21:31 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.output.dense.bias from model.bert.encoder.layer.0.attention.output.dense.bias\n","\u001b[32m2024-04-23T23:21:31 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.output.LayerNorm.weight from model.bert.encoder.layer.0.attention.output.LayerNorm.weight\n","\u001b[32m2024-04-23T23:21:31 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.output.LayerNorm.bias from model.bert.encoder.layer.0.attention.output.LayerNorm.bias\n","\u001b[32m2024-04-23T23:21:31 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.intermediate.dense.weight from model.bert.encoder.layer.0.intermediate.dense.weight\n","\u001b[32m2024-04-23T23:21:31 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.intermediate.dense.bias from model.bert.encoder.layer.0.intermediate.dense.bias\n","\u001b[32m2024-04-23T23:21:31 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.output.dense.weight from model.bert.encoder.layer.0.output.dense.weight\n","\u001b[32m2024-04-23T23:21:31 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.output.dense.bias from model.bert.encoder.layer.0.output.dense.bias\n","\u001b[32m2024-04-23T23:21:31 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.output.LayerNorm.weight from model.bert.encoder.layer.0.output.LayerNorm.weight\n","\u001b[32m2024-04-23T23:21:31 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.output.LayerNorm.bias from model.bert.encoder.layer.0.output.LayerNorm.bias\n","\u001b[32m2024-04-23T23:21:31 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.self.query.weight from model.bert.encoder.layer.1.attention.self.query.weight\n","\u001b[32m2024-04-23T23:21:31 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.self.query.bias from model.bert.encoder.layer.1.attention.self.query.bias\n","\u001b[32m2024-04-23T23:21:31 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.self.key.weight from model.bert.encoder.layer.1.attention.self.key.weight\n","\u001b[32m2024-04-23T23:21:31 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.self.key.bias from model.bert.encoder.layer.1.attention.self.key.bias\n","\u001b[32m2024-04-23T23:21:31 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.self.value.weight from model.bert.encoder.layer.1.attention.self.value.weight\n","\u001b[32m2024-04-23T23:21:31 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.self.value.bias from model.bert.encoder.layer.1.attention.self.value.bias\n","\u001b[32m2024-04-23T23:21:31 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.output.dense.weight from model.bert.encoder.layer.1.attention.output.dense.weight\n","\u001b[32m2024-04-23T23:21:31 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.output.dense.bias from model.bert.encoder.layer.1.attention.output.dense.bias\n","\u001b[32m2024-04-23T23:21:31 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.output.LayerNorm.weight from model.bert.encoder.layer.1.attention.output.LayerNorm.weight\n","\u001b[32m2024-04-23T23:21:31 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.output.LayerNorm.bias from model.bert.encoder.layer.1.attention.output.LayerNorm.bias\n","\u001b[32m2024-04-23T23:21:31 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.intermediate.dense.weight from model.bert.encoder.layer.1.intermediate.dense.weight\n","\u001b[32m2024-04-23T23:21:31 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.intermediate.dense.bias from model.bert.encoder.layer.1.intermediate.dense.bias\n","\u001b[32m2024-04-23T23:21:31 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.output.dense.weight from model.bert.encoder.layer.1.output.dense.weight\n","\u001b[32m2024-04-23T23:21:31 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.output.dense.bias from model.bert.encoder.layer.1.output.dense.bias\n","\u001b[32m2024-04-23T23:21:31 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.output.LayerNorm.weight from model.bert.encoder.layer.1.output.LayerNorm.weight\n","\u001b[32m2024-04-23T23:21:31 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.output.LayerNorm.bias from model.bert.encoder.layer.1.output.LayerNorm.bias\n","\u001b[32m2024-04-23T23:21:31 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.self.query.weight from model.bert.encoder.layer.2.attention.self.query.weight\n","\u001b[32m2024-04-23T23:21:31 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.self.query.bias from model.bert.encoder.layer.2.attention.self.query.bias\n","\u001b[32m2024-04-23T23:21:31 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.self.key.weight from model.bert.encoder.layer.2.attention.self.key.weight\n","\u001b[32m2024-04-23T23:21:31 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.self.key.bias from model.bert.encoder.layer.2.attention.self.key.bias\n","\u001b[32m2024-04-23T23:21:31 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.self.value.weight from model.bert.encoder.layer.2.attention.self.value.weight\n","\u001b[32m2024-04-23T23:21:31 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.self.value.bias from model.bert.encoder.layer.2.attention.self.value.bias\n","\u001b[32m2024-04-23T23:21:31 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.output.dense.weight from model.bert.encoder.layer.2.attention.output.dense.weight\n","\u001b[32m2024-04-23T23:21:31 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.output.dense.bias from model.bert.encoder.layer.2.attention.output.dense.bias\n","\u001b[32m2024-04-23T23:21:31 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.output.LayerNorm.weight from model.bert.encoder.layer.2.attention.output.LayerNorm.weight\n","\u001b[32m2024-04-23T23:21:31 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.output.LayerNorm.bias from model.bert.encoder.layer.2.attention.output.LayerNorm.bias\n","\u001b[32m2024-04-23T23:21:31 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.intermediate.dense.weight from model.bert.encoder.layer.2.intermediate.dense.weight\n","\u001b[32m2024-04-23T23:21:31 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.intermediate.dense.bias from model.bert.encoder.layer.2.intermediate.dense.bias\n","\u001b[32m2024-04-23T23:21:31 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.output.dense.weight from model.bert.encoder.layer.2.output.dense.weight\n","\u001b[32m2024-04-23T23:21:31 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.output.dense.bias from model.bert.encoder.layer.2.output.dense.bias\n","\u001b[32m2024-04-23T23:21:31 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.output.LayerNorm.weight from model.bert.encoder.layer.2.output.LayerNorm.weight\n","\u001b[32m2024-04-23T23:21:31 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.output.LayerNorm.bias from model.bert.encoder.layer.2.output.LayerNorm.bias\n","\u001b[32m2024-04-23T23:21:31 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.self.query.weight from model.bert.encoder.layer.3.attention.self.query.weight\n","\u001b[32m2024-04-23T23:21:31 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.self.query.bias from model.bert.encoder.layer.3.attention.self.query.bias\n","\u001b[32m2024-04-23T23:21:31 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.self.key.weight from model.bert.encoder.layer.3.attention.self.key.weight\n","\u001b[32m2024-04-23T23:21:31 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.self.key.bias from model.bert.encoder.layer.3.attention.self.key.bias\n","\u001b[32m2024-04-23T23:21:31 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.self.value.weight from model.bert.encoder.layer.3.attention.self.value.weight\n","\u001b[32m2024-04-23T23:21:31 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.self.value.bias from model.bert.encoder.layer.3.attention.self.value.bias\n","\u001b[32m2024-04-23T23:21:31 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.output.dense.weight from model.bert.encoder.layer.3.attention.output.dense.weight\n","\u001b[32m2024-04-23T23:21:31 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.output.dense.bias from model.bert.encoder.layer.3.attention.output.dense.bias\n","\u001b[32m2024-04-23T23:21:31 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.output.LayerNorm.weight from model.bert.encoder.layer.3.attention.output.LayerNorm.weight\n","\u001b[32m2024-04-23T23:21:31 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.output.LayerNorm.bias from model.bert.encoder.layer.3.attention.output.LayerNorm.bias\n","\u001b[32m2024-04-23T23:21:31 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.intermediate.dense.weight from model.bert.encoder.layer.3.intermediate.dense.weight\n","\u001b[32m2024-04-23T23:21:31 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.intermediate.dense.bias from model.bert.encoder.layer.3.intermediate.dense.bias\n","\u001b[32m2024-04-23T23:21:31 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.output.dense.weight from model.bert.encoder.layer.3.output.dense.weight\n","\u001b[32m2024-04-23T23:21:31 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.output.dense.bias from model.bert.encoder.layer.3.output.dense.bias\n","\u001b[32m2024-04-23T23:21:31 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.output.LayerNorm.weight from model.bert.encoder.layer.3.output.LayerNorm.weight\n","\u001b[32m2024-04-23T23:21:31 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.output.LayerNorm.bias from model.bert.encoder.layer.3.output.LayerNorm.bias\n","\u001b[32m2024-04-23T23:21:31 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.self.query.weight from model.bert.encoder.layer.4.attention.self.query.weight\n","\u001b[32m2024-04-23T23:21:31 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.self.query.bias from model.bert.encoder.layer.4.attention.self.query.bias\n","\u001b[32m2024-04-23T23:21:31 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.self.key.weight from model.bert.encoder.layer.4.attention.self.key.weight\n","\u001b[32m2024-04-23T23:21:31 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.self.key.bias from model.bert.encoder.layer.4.attention.self.key.bias\n","\u001b[32m2024-04-23T23:21:31 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.self.value.weight from model.bert.encoder.layer.4.attention.self.value.weight\n","\u001b[32m2024-04-23T23:21:31 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.self.value.bias from model.bert.encoder.layer.4.attention.self.value.bias\n","\u001b[32m2024-04-23T23:21:31 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.output.dense.weight from model.bert.encoder.layer.4.attention.output.dense.weight\n","\u001b[32m2024-04-23T23:21:31 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.output.dense.bias from model.bert.encoder.layer.4.attention.output.dense.bias\n","\u001b[32m2024-04-23T23:21:31 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.output.LayerNorm.weight from model.bert.encoder.layer.4.attention.output.LayerNorm.weight\n","\u001b[32m2024-04-23T23:21:31 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.output.LayerNorm.bias from model.bert.encoder.layer.4.attention.output.LayerNorm.bias\n","\u001b[32m2024-04-23T23:21:31 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.intermediate.dense.weight from model.bert.encoder.layer.4.intermediate.dense.weight\n","\u001b[32m2024-04-23T23:21:31 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.intermediate.dense.bias from model.bert.encoder.layer.4.intermediate.dense.bias\n","\u001b[32m2024-04-23T23:21:31 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.output.dense.weight from model.bert.encoder.layer.4.output.dense.weight\n","\u001b[32m2024-04-23T23:21:31 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.output.dense.bias from model.bert.encoder.layer.4.output.dense.bias\n","\u001b[32m2024-04-23T23:21:31 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.output.LayerNorm.weight from model.bert.encoder.layer.4.output.LayerNorm.weight\n","\u001b[32m2024-04-23T23:21:31 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.output.LayerNorm.bias from model.bert.encoder.layer.4.output.LayerNorm.bias\n","\u001b[32m2024-04-23T23:21:31 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.self.query.weight from model.bert.encoder.layer.5.attention.self.query.weight\n","\u001b[32m2024-04-23T23:21:31 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.self.query.bias from model.bert.encoder.layer.5.attention.self.query.bias\n","\u001b[32m2024-04-23T23:21:31 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.self.key.weight from model.bert.encoder.layer.5.attention.self.key.weight\n","\u001b[32m2024-04-23T23:21:31 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.self.key.bias from model.bert.encoder.layer.5.attention.self.key.bias\n","\u001b[32m2024-04-23T23:21:31 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.self.value.weight from model.bert.encoder.layer.5.attention.self.value.weight\n","\u001b[32m2024-04-23T23:21:31 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.self.value.bias from model.bert.encoder.layer.5.attention.self.value.bias\n","\u001b[32m2024-04-23T23:21:31 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.output.dense.weight from model.bert.encoder.layer.5.attention.output.dense.weight\n","\u001b[32m2024-04-23T23:21:31 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.output.dense.bias from model.bert.encoder.layer.5.attention.output.dense.bias\n","\u001b[32m2024-04-23T23:21:31 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.output.LayerNorm.weight from model.bert.encoder.layer.5.attention.output.LayerNorm.weight\n","\u001b[32m2024-04-23T23:21:31 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.output.LayerNorm.bias from model.bert.encoder.layer.5.attention.output.LayerNorm.bias\n","\u001b[32m2024-04-23T23:21:31 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.intermediate.dense.weight from model.bert.encoder.layer.5.intermediate.dense.weight\n","\u001b[32m2024-04-23T23:21:31 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.intermediate.dense.bias from model.bert.encoder.layer.5.intermediate.dense.bias\n","\u001b[32m2024-04-23T23:21:31 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.output.dense.weight from model.bert.encoder.layer.5.output.dense.weight\n","\u001b[32m2024-04-23T23:21:31 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.output.dense.bias from model.bert.encoder.layer.5.output.dense.bias\n","\u001b[32m2024-04-23T23:21:31 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.output.LayerNorm.weight from model.bert.encoder.layer.5.output.LayerNorm.weight\n","\u001b[32m2024-04-23T23:21:31 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.output.LayerNorm.bias from model.bert.encoder.layer.5.output.LayerNorm.bias\n","\u001b[32m2024-04-23T23:21:31 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.self.query.weight from model.bert.encoder.layer.6.attention.self.query.weight\n","\u001b[32m2024-04-23T23:21:31 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.self.query.bias from model.bert.encoder.layer.6.attention.self.query.bias\n","\u001b[32m2024-04-23T23:21:31 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.self.key.weight from model.bert.encoder.layer.6.attention.self.key.weight\n","\u001b[32m2024-04-23T23:21:31 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.self.key.bias from model.bert.encoder.layer.6.attention.self.key.bias\n","\u001b[32m2024-04-23T23:21:31 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.self.value.weight from model.bert.encoder.layer.6.attention.self.value.weight\n","\u001b[32m2024-04-23T23:21:31 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.self.value.bias from model.bert.encoder.layer.6.attention.self.value.bias\n","\u001b[32m2024-04-23T23:21:31 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.output.dense.weight from model.bert.encoder.layer.6.attention.output.dense.weight\n","\u001b[32m2024-04-23T23:21:31 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.output.dense.bias from model.bert.encoder.layer.6.attention.output.dense.bias\n","\u001b[32m2024-04-23T23:21:31 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.output.LayerNorm.weight from model.bert.encoder.layer.6.attention.output.LayerNorm.weight\n","\u001b[32m2024-04-23T23:21:31 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.output.LayerNorm.bias from model.bert.encoder.layer.6.attention.output.LayerNorm.bias\n","\u001b[32m2024-04-23T23:21:31 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.intermediate.dense.weight from model.bert.encoder.layer.6.intermediate.dense.weight\n","\u001b[32m2024-04-23T23:21:31 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.intermediate.dense.bias from model.bert.encoder.layer.6.intermediate.dense.bias\n","\u001b[32m2024-04-23T23:21:31 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.output.dense.weight from model.bert.encoder.layer.6.output.dense.weight\n","\u001b[32m2024-04-23T23:21:31 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.output.dense.bias from model.bert.encoder.layer.6.output.dense.bias\n","\u001b[32m2024-04-23T23:21:31 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.output.LayerNorm.weight from model.bert.encoder.layer.6.output.LayerNorm.weight\n","\u001b[32m2024-04-23T23:21:31 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.output.LayerNorm.bias from model.bert.encoder.layer.6.output.LayerNorm.bias\n","\u001b[32m2024-04-23T23:21:31 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.self.query.weight from model.bert.encoder.layer.7.attention.self.query.weight\n","\u001b[32m2024-04-23T23:21:31 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.self.query.bias from model.bert.encoder.layer.7.attention.self.query.bias\n","\u001b[32m2024-04-23T23:21:31 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.self.key.weight from model.bert.encoder.layer.7.attention.self.key.weight\n","\u001b[32m2024-04-23T23:21:31 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.self.key.bias from model.bert.encoder.layer.7.attention.self.key.bias\n","\u001b[32m2024-04-23T23:21:31 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.self.value.weight from model.bert.encoder.layer.7.attention.self.value.weight\n","\u001b[32m2024-04-23T23:21:31 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.self.value.bias from model.bert.encoder.layer.7.attention.self.value.bias\n","\u001b[32m2024-04-23T23:21:31 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.output.dense.weight from model.bert.encoder.layer.7.attention.output.dense.weight\n","\u001b[32m2024-04-23T23:21:31 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.output.dense.bias from model.bert.encoder.layer.7.attention.output.dense.bias\n","\u001b[32m2024-04-23T23:21:31 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.output.LayerNorm.weight from model.bert.encoder.layer.7.attention.output.LayerNorm.weight\n","\u001b[32m2024-04-23T23:21:31 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.output.LayerNorm.bias from model.bert.encoder.layer.7.attention.output.LayerNorm.bias\n","\u001b[32m2024-04-23T23:21:31 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.intermediate.dense.weight from model.bert.encoder.layer.7.intermediate.dense.weight\n","\u001b[32m2024-04-23T23:21:31 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.intermediate.dense.bias from model.bert.encoder.layer.7.intermediate.dense.bias\n","\u001b[32m2024-04-23T23:21:31 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.output.dense.weight from model.bert.encoder.layer.7.output.dense.weight\n","\u001b[32m2024-04-23T23:21:31 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.output.dense.bias from model.bert.encoder.layer.7.output.dense.bias\n","\u001b[32m2024-04-23T23:21:31 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.output.LayerNorm.weight from model.bert.encoder.layer.7.output.LayerNorm.weight\n","\u001b[32m2024-04-23T23:21:31 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.output.LayerNorm.bias from model.bert.encoder.layer.7.output.LayerNorm.bias\n","\u001b[32m2024-04-23T23:21:31 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.self.query.weight from model.bert.encoder.layer.8.attention.self.query.weight\n","\u001b[32m2024-04-23T23:21:31 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.self.query.bias from model.bert.encoder.layer.8.attention.self.query.bias\n","\u001b[32m2024-04-23T23:21:31 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.self.key.weight from model.bert.encoder.layer.8.attention.self.key.weight\n","\u001b[32m2024-04-23T23:21:31 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.self.key.bias from model.bert.encoder.layer.8.attention.self.key.bias\n","\u001b[32m2024-04-23T23:21:31 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.self.value.weight from model.bert.encoder.layer.8.attention.self.value.weight\n","\u001b[32m2024-04-23T23:21:31 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.self.value.bias from model.bert.encoder.layer.8.attention.self.value.bias\n","\u001b[32m2024-04-23T23:21:31 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.output.dense.weight from model.bert.encoder.layer.8.attention.output.dense.weight\n","\u001b[32m2024-04-23T23:21:31 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.output.dense.bias from model.bert.encoder.layer.8.attention.output.dense.bias\n","\u001b[32m2024-04-23T23:21:31 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.output.LayerNorm.weight from model.bert.encoder.layer.8.attention.output.LayerNorm.weight\n","\u001b[32m2024-04-23T23:21:31 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.output.LayerNorm.bias from model.bert.encoder.layer.8.attention.output.LayerNorm.bias\n","\u001b[32m2024-04-23T23:21:31 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.intermediate.dense.weight from model.bert.encoder.layer.8.intermediate.dense.weight\n","\u001b[32m2024-04-23T23:21:31 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.intermediate.dense.bias from model.bert.encoder.layer.8.intermediate.dense.bias\n","\u001b[32m2024-04-23T23:21:31 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.output.dense.weight from model.bert.encoder.layer.8.output.dense.weight\n","\u001b[32m2024-04-23T23:21:31 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.output.dense.bias from model.bert.encoder.layer.8.output.dense.bias\n","\u001b[32m2024-04-23T23:21:31 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.output.LayerNorm.weight from model.bert.encoder.layer.8.output.LayerNorm.weight\n","\u001b[32m2024-04-23T23:21:31 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.output.LayerNorm.bias from model.bert.encoder.layer.8.output.LayerNorm.bias\n","\u001b[32m2024-04-23T23:21:31 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.self.query.weight from model.bert.encoder.layer.9.attention.self.query.weight\n","\u001b[32m2024-04-23T23:21:31 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.self.query.bias from model.bert.encoder.layer.9.attention.self.query.bias\n","\u001b[32m2024-04-23T23:21:31 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.self.key.weight from model.bert.encoder.layer.9.attention.self.key.weight\n","\u001b[32m2024-04-23T23:21:31 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.self.key.bias from model.bert.encoder.layer.9.attention.self.key.bias\n","\u001b[32m2024-04-23T23:21:31 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.self.value.weight from model.bert.encoder.layer.9.attention.self.value.weight\n","\u001b[32m2024-04-23T23:21:31 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.self.value.bias from model.bert.encoder.layer.9.attention.self.value.bias\n","\u001b[32m2024-04-23T23:21:31 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.output.dense.weight from model.bert.encoder.layer.9.attention.output.dense.weight\n","\u001b[32m2024-04-23T23:21:31 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.output.dense.bias from model.bert.encoder.layer.9.attention.output.dense.bias\n","\u001b[32m2024-04-23T23:21:31 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.output.LayerNorm.weight from model.bert.encoder.layer.9.attention.output.LayerNorm.weight\n","\u001b[32m2024-04-23T23:21:31 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.output.LayerNorm.bias from model.bert.encoder.layer.9.attention.output.LayerNorm.bias\n","\u001b[32m2024-04-23T23:21:31 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.intermediate.dense.weight from model.bert.encoder.layer.9.intermediate.dense.weight\n","\u001b[32m2024-04-23T23:21:31 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.intermediate.dense.bias from model.bert.encoder.layer.9.intermediate.dense.bias\n","\u001b[32m2024-04-23T23:21:31 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.output.dense.weight from model.bert.encoder.layer.9.output.dense.weight\n","\u001b[32m2024-04-23T23:21:31 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.output.dense.bias from model.bert.encoder.layer.9.output.dense.bias\n","\u001b[32m2024-04-23T23:21:31 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.output.LayerNorm.weight from model.bert.encoder.layer.9.output.LayerNorm.weight\n","\u001b[32m2024-04-23T23:21:31 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.output.LayerNorm.bias from model.bert.encoder.layer.9.output.LayerNorm.bias\n","\u001b[32m2024-04-23T23:21:31 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.self.query.weight from model.bert.encoder.layer.10.attention.self.query.weight\n","\u001b[32m2024-04-23T23:21:31 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.self.query.bias from model.bert.encoder.layer.10.attention.self.query.bias\n","\u001b[32m2024-04-23T23:21:31 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.self.key.weight from model.bert.encoder.layer.10.attention.self.key.weight\n","\u001b[32m2024-04-23T23:21:31 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.self.key.bias from model.bert.encoder.layer.10.attention.self.key.bias\n","\u001b[32m2024-04-23T23:21:31 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.self.value.weight from model.bert.encoder.layer.10.attention.self.value.weight\n","\u001b[32m2024-04-23T23:21:31 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.self.value.bias from model.bert.encoder.layer.10.attention.self.value.bias\n","\u001b[32m2024-04-23T23:21:31 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.output.dense.weight from model.bert.encoder.layer.10.attention.output.dense.weight\n","\u001b[32m2024-04-23T23:21:31 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.output.dense.bias from model.bert.encoder.layer.10.attention.output.dense.bias\n","\u001b[32m2024-04-23T23:21:31 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.output.LayerNorm.weight from model.bert.encoder.layer.10.attention.output.LayerNorm.weight\n","\u001b[32m2024-04-23T23:21:31 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.output.LayerNorm.bias from model.bert.encoder.layer.10.attention.output.LayerNorm.bias\n","\u001b[32m2024-04-23T23:21:31 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.intermediate.dense.weight from model.bert.encoder.layer.10.intermediate.dense.weight\n","\u001b[32m2024-04-23T23:21:31 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.intermediate.dense.bias from model.bert.encoder.layer.10.intermediate.dense.bias\n","\u001b[32m2024-04-23T23:21:31 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.output.dense.weight from model.bert.encoder.layer.10.output.dense.weight\n","\u001b[32m2024-04-23T23:21:31 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.output.dense.bias from model.bert.encoder.layer.10.output.dense.bias\n","\u001b[32m2024-04-23T23:21:31 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.output.LayerNorm.weight from model.bert.encoder.layer.10.output.LayerNorm.weight\n","\u001b[32m2024-04-23T23:21:31 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.output.LayerNorm.bias from model.bert.encoder.layer.10.output.LayerNorm.bias\n","\u001b[32m2024-04-23T23:21:31 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.self.query.weight from model.bert.encoder.layer.11.attention.self.query.weight\n","\u001b[32m2024-04-23T23:21:31 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.self.query.bias from model.bert.encoder.layer.11.attention.self.query.bias\n","\u001b[32m2024-04-23T23:21:31 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.self.key.weight from model.bert.encoder.layer.11.attention.self.key.weight\n","\u001b[32m2024-04-23T23:21:31 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.self.key.bias from model.bert.encoder.layer.11.attention.self.key.bias\n","\u001b[32m2024-04-23T23:21:31 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.self.value.weight from model.bert.encoder.layer.11.attention.self.value.weight\n","\u001b[32m2024-04-23T23:21:31 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.self.value.bias from model.bert.encoder.layer.11.attention.self.value.bias\n","\u001b[32m2024-04-23T23:21:31 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.output.dense.weight from model.bert.encoder.layer.11.attention.output.dense.weight\n","\u001b[32m2024-04-23T23:21:31 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.output.dense.bias from model.bert.encoder.layer.11.attention.output.dense.bias\n","\u001b[32m2024-04-23T23:21:31 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.output.LayerNorm.weight from model.bert.encoder.layer.11.attention.output.LayerNorm.weight\n","\u001b[32m2024-04-23T23:21:31 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.output.LayerNorm.bias from model.bert.encoder.layer.11.attention.output.LayerNorm.bias\n","\u001b[32m2024-04-23T23:21:31 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.intermediate.dense.weight from model.bert.encoder.layer.11.intermediate.dense.weight\n","\u001b[32m2024-04-23T23:21:31 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.intermediate.dense.bias from model.bert.encoder.layer.11.intermediate.dense.bias\n","\u001b[32m2024-04-23T23:21:31 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.output.dense.weight from model.bert.encoder.layer.11.output.dense.weight\n","\u001b[32m2024-04-23T23:21:31 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.output.dense.bias from model.bert.encoder.layer.11.output.dense.bias\n","\u001b[32m2024-04-23T23:21:31 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.output.LayerNorm.weight from model.bert.encoder.layer.11.output.LayerNorm.weight\n","\u001b[32m2024-04-23T23:21:31 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.output.LayerNorm.bias from model.bert.encoder.layer.11.output.LayerNorm.bias\n","\u001b[32m2024-04-23T23:21:31 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.pooler.dense.weight from model.bert.pooler.dense.weight\n","\u001b[32m2024-04-23T23:21:31 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.pooler.dense.bias from model.bert.pooler.dense.bias\n","\u001b[32m2024-04-23T23:21:31 | mmf.utils.checkpoint: \u001b[0mPretrained model loaded\n","\u001b[32m2024-04-23T23:21:31 | mmf.utils.checkpoint: \u001b[0mCheckpoint loaded.\n","\u001b[32m2024-04-23T23:21:31 | mmf.utils.checkpoint: \u001b[0mCurrent num updates: 0\n","\u001b[32m2024-04-23T23:21:31 | mmf.utils.checkpoint: \u001b[0mCurrent iteration: 0\n","\u001b[32m2024-04-23T23:21:31 | mmf.utils.checkpoint: \u001b[0mCurrent epoch: 0\n","\u001b[32m2024-04-23T23:21:31 | mmf.trainers.mmf_trainer: \u001b[0m===== Model =====\n","\u001b[32m2024-04-23T23:21:31 | mmf.trainers.mmf_trainer: \u001b[0mVisualBERT(\n","  (model): VisualBERTForClassification(\n","    (bert): VisualBERTBase(\n","      (embeddings): BertVisioLinguisticEmbeddings(\n","        (word_embeddings): Embedding(30522, 768, padding_idx=0)\n","        (position_embeddings): Embedding(512, 768)\n","        (token_type_embeddings): Embedding(2, 768)\n","        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","        (token_type_embeddings_visual): Embedding(2, 768)\n","        (position_embeddings_visual): Embedding(512, 768)\n","        (projection): Linear(in_features=2048, out_features=768, bias=True)\n","      )\n","      (encoder): BertEncoderJit(\n","        (layer): ModuleList(\n","          (0-11): 12 x BertLayerJit(\n","            (attention): BertAttentionJit(\n","              (self): BertSelfAttentionJit(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","              (intermediate_act_fn): GELUActivation()\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","        )\n","      )\n","      (pooler): BertPooler(\n","        (dense): Linear(in_features=768, out_features=768, bias=True)\n","        (activation): Tanh()\n","      )\n","    )\n","    (dropout): Dropout(p=0.1, inplace=False)\n","    (classifier): Sequential(\n","      (0): BertPredictionHeadTransform(\n","        (dense): Linear(in_features=768, out_features=768, bias=True)\n","        (transform_act_fn): GELUActivation()\n","        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      )\n","      (1): Linear(in_features=768, out_features=2, bias=True)\n","    )\n","  )\n","  (losses): Losses(\n","    (losses): ModuleList(\n","      (0): MMFLoss(\n","        (loss_criterion): CrossEntropyLoss(\n","          (loss_fn): CrossEntropyLoss()\n","        )\n","      )\n","    )\n","  )\n",")\n","\u001b[32m2024-04-23T23:21:31 | mmf.utils.general: \u001b[0mTotal Parameters: 112044290. Trained Parameters: 112044290\n","\u001b[32m2024-04-23T23:21:31 | mmf.trainers.core.training_loop: \u001b[0mStarting training...\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2024-04-23T23:21:31 | py.warnings: \u001b[0m/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n","  self.pid = os.fork()\n","\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2024-04-23T23:21:31 | py.warnings: \u001b[0m/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n","  self.pid = os.fork()\n","\n","\u001b[32m2024-04-23T23:25:24 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n","\u001b[32m2024-04-23T23:25:24 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2024-04-23T23:25:26 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2024-04-23T23:25:29 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2024-04-23T23:25:29 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 200/1500, train/hateful_memes/cross_entropy: 0.5516, train/hateful_memes/cross_entropy/avg: 0.5516, train/total_loss: 0.5516, train/total_loss/avg: 0.5516, max mem: 14320.0, experiment: run, epoch: 2, num_updates: 200, iterations: 200, max_updates: 1500, lr: 0.00001, ups: 0.84, time: 03m 57s 827ms, time_since_start: 04m 13s 095ms, eta: 26m 55s 440ms\n","\u001b[32m2024-04-23T23:25:29 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n","\u001b[32m2024-04-23T23:25:29 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n","\u001b[32m2024-04-23T23:25:34 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished evaluation inference. Loaded 9\n","\u001b[32m2024-04-23T23:25:34 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n","\u001b[32m2024-04-23T23:25:34 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2024-04-23T23:25:36 | mmf.utils.checkpoint: \u001b[0mSaving best checkpoint\n","\u001b[32m2024-04-23T23:25:40 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2024-04-23T23:25:46 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2024-04-23T23:25:46 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 200/1500, val/hateful_memes/cross_entropy: 0.6724, val/total_loss: 0.6724, val/hateful_memes/accuracy: 0.6167, val/hateful_memes/binary_f1: 0.1818, val/hateful_memes/roc_auc: 0.5598, num_updates: 200, epoch: 2, iterations: 200, max_updates: 1500, val_time: 16s 663ms, best_update: 200, best_iteration: 200, best_val/hateful_memes/roc_auc: 0.559809\n","\u001b[32m2024-04-23T23:29:43 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n","\u001b[32m2024-04-23T23:29:43 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2024-04-23T23:29:45 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2024-04-23T23:29:50 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2024-04-23T23:29:50 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 400/1500, train/hateful_memes/cross_entropy: 0.5158, train/hateful_memes/cross_entropy/avg: 0.5337, train/total_loss: 0.5158, train/total_loss/avg: 0.5337, max mem: 14369.0, experiment: run, epoch: 4, num_updates: 400, iterations: 400, max_updates: 1500, lr: 0.00001, ups: 0.82, time: 04m 04s 378ms, time_since_start: 08m 34s 145ms, eta: 23m 24s 564ms\n","\u001b[32m2024-04-23T23:29:50 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n","\u001b[32m2024-04-23T23:29:50 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2024-04-23T23:29:50 | py.warnings: \u001b[0m/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n","  self.pid = os.fork()\n","\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2024-04-23T23:29:50 | py.warnings: \u001b[0m/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n","  self.pid = os.fork()\n","\n","\u001b[32m2024-04-23T23:29:55 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished evaluation inference. Loaded 9\n","\u001b[32m2024-04-23T23:29:55 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n","\u001b[32m2024-04-23T23:29:55 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2024-04-23T23:29:58 | mmf.utils.checkpoint: \u001b[0mSaving best checkpoint\n","\u001b[32m2024-04-23T23:30:02 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2024-04-23T23:30:09 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2024-04-23T23:30:09 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 400/1500, val/hateful_memes/cross_entropy: 0.6955, val/total_loss: 0.6955, val/hateful_memes/accuracy: 0.6222, val/hateful_memes/binary_f1: 0.3964, val/hateful_memes/roc_auc: 0.5888, num_updates: 400, epoch: 4, iterations: 400, max_updates: 1500, val_time: 18s 368ms, best_update: 400, best_iteration: 400, best_val/hateful_memes/roc_auc: 0.588765\n","\u001b[32m2024-04-23T23:34:06 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n","\u001b[32m2024-04-23T23:34:06 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2024-04-23T23:34:08 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2024-04-23T23:34:14 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2024-04-23T23:34:14 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 600/1500, train/hateful_memes/cross_entropy: 0.5158, train/hateful_memes/cross_entropy/avg: 0.4803, train/total_loss: 0.5158, train/total_loss/avg: 0.4803, max mem: 14369.0, experiment: run, epoch: 5, num_updates: 600, iterations: 600, max_updates: 1500, lr: 0.00002, ups: 0.82, time: 04m 05s 449ms, time_since_start: 12m 57s 966ms, eta: 19m 14s 225ms\n","\u001b[32m2024-04-23T23:34:14 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n","\u001b[32m2024-04-23T23:34:14 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2024-04-23T23:34:14 | py.warnings: \u001b[0m/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n","  self.pid = os.fork()\n","\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2024-04-23T23:34:14 | py.warnings: \u001b[0m/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n","  self.pid = os.fork()\n","\n","\u001b[32m2024-04-23T23:34:18 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished evaluation inference. Loaded 9\n","\u001b[32m2024-04-23T23:34:18 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n","\u001b[32m2024-04-23T23:34:19 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2024-04-23T23:34:21 | mmf.utils.checkpoint: \u001b[0mSaving best checkpoint\n","\u001b[32m2024-04-23T23:34:26 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2024-04-23T23:34:30 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2024-04-23T23:34:30 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 600/1500, val/hateful_memes/cross_entropy: 0.8997, val/total_loss: 0.8997, val/hateful_memes/accuracy: 0.6407, val/hateful_memes/binary_f1: 0.4260, val/hateful_memes/roc_auc: 0.6222, num_updates: 600, epoch: 5, iterations: 600, max_updates: 1500, val_time: 16s 180ms, best_update: 600, best_iteration: 600, best_val/hateful_memes/roc_auc: 0.622206\n","\u001b[32m2024-04-23T23:38:28 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n","\u001b[32m2024-04-23T23:38:28 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2024-04-23T23:38:30 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2024-04-23T23:38:33 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2024-04-23T23:38:33 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 800/1500, train/hateful_memes/cross_entropy: 0.3736, train/hateful_memes/cross_entropy/avg: 0.4017, train/total_loss: 0.3736, train/total_loss/avg: 0.4017, max mem: 14369.0, experiment: run, epoch: 7, num_updates: 800, iterations: 800, max_updates: 1500, lr: 0.00002, ups: 0.82, time: 04m 03s 137ms, time_since_start: 17m 17s 285ms, eta: 14m 49s 275ms\n","\u001b[32m2024-04-23T23:38:33 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n","\u001b[32m2024-04-23T23:38:34 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2024-04-23T23:38:34 | py.warnings: \u001b[0m/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n","  self.pid = os.fork()\n","\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2024-04-23T23:38:34 | py.warnings: \u001b[0m/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n","  self.pid = os.fork()\n","\n","\u001b[32m2024-04-23T23:38:38 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished evaluation inference. Loaded 9\n","\u001b[32m2024-04-23T23:38:38 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n","\u001b[32m2024-04-23T23:38:38 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2024-04-23T23:38:40 | mmf.utils.checkpoint: \u001b[0mSaving best checkpoint\n","\u001b[32m2024-04-23T23:38:46 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2024-04-23T23:38:50 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2024-04-23T23:38:50 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 800/1500, val/hateful_memes/cross_entropy: 0.9979, val/total_loss: 0.9979, val/hateful_memes/accuracy: 0.6370, val/hateful_memes/binary_f1: 0.4674, val/hateful_memes/roc_auc: 0.6306, num_updates: 800, epoch: 7, iterations: 800, max_updates: 1500, val_time: 16s 242ms, best_update: 800, best_iteration: 800, best_val/hateful_memes/roc_auc: 0.630574\n","\u001b[32m2024-04-23T23:42:48 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n","\u001b[32m2024-04-23T23:42:48 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2024-04-23T23:42:50 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2024-04-23T23:42:55 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2024-04-23T23:42:55 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1000/1500, train/hateful_memes/cross_entropy: 0.3736, train/hateful_memes/cross_entropy/avg: 0.3327, train/total_loss: 0.3736, train/total_loss/avg: 0.3327, max mem: 14369.0, experiment: run, epoch: 8, num_updates: 1000, iterations: 1000, max_updates: 1500, lr: 0.00003, ups: 0.82, time: 04m 05s 317ms, time_since_start: 21m 38s 847ms, eta: 10m 40s 891ms\n","\u001b[32m2024-04-23T23:42:55 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n","\u001b[32m2024-04-23T23:42:55 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2024-04-23T23:42:55 | py.warnings: \u001b[0m/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n","  self.pid = os.fork()\n","\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2024-04-23T23:42:55 | py.warnings: \u001b[0m/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n","  self.pid = os.fork()\n","\n","\u001b[32m2024-04-23T23:42:59 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished evaluation inference. Loaded 9\n","\u001b[32m2024-04-23T23:42:59 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n","\u001b[32m2024-04-23T23:42:59 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2024-04-23T23:43:02 | mmf.utils.checkpoint: \u001b[0mSaving best checkpoint\n","\u001b[32m2024-04-23T23:43:07 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2024-04-23T23:43:12 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2024-04-23T23:43:12 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1000/1500, val/hateful_memes/cross_entropy: 1.3110, val/total_loss: 1.3110, val/hateful_memes/accuracy: 0.6611, val/hateful_memes/binary_f1: 0.4116, val/hateful_memes/roc_auc: 0.6585, num_updates: 1000, epoch: 8, iterations: 1000, max_updates: 1500, val_time: 16s 848ms, best_update: 1000, best_iteration: 1000, best_val/hateful_memes/roc_auc: 0.658500\n","\u001b[32m2024-04-23T23:47:10 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n","\u001b[32m2024-04-23T23:47:10 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2024-04-23T23:47:12 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2024-04-23T23:47:18 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2024-04-23T23:47:18 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1200/1500, train/hateful_memes/cross_entropy: 0.1656, train/hateful_memes/cross_entropy/avg: 0.2843, train/total_loss: 0.1656, train/total_loss/avg: 0.2843, max mem: 14369.0, experiment: run, epoch: 10, num_updates: 1200, iterations: 1200, max_updates: 1500, lr: 0.00003, ups: 0.82, time: 04m 05s 892ms, time_since_start: 26m 01s 590ms, eta: 06m 25s 436ms\n","\u001b[32m2024-04-23T23:47:18 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n","\u001b[32m2024-04-23T23:47:18 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2024-04-23T23:47:18 | py.warnings: \u001b[0m/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n","  self.pid = os.fork()\n","\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2024-04-23T23:47:18 | py.warnings: \u001b[0m/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n","  self.pid = os.fork()\n","\n","\u001b[32m2024-04-23T23:47:22 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished evaluation inference. Loaded 9\n","\u001b[32m2024-04-23T23:47:22 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n","\u001b[32m2024-04-23T23:47:22 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2024-04-23T23:47:25 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2024-04-23T23:47:29 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2024-04-23T23:47:29 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1200/1500, val/hateful_memes/cross_entropy: 1.3509, val/total_loss: 1.3509, val/hateful_memes/accuracy: 0.6241, val/hateful_memes/binary_f1: 0.4988, val/hateful_memes/roc_auc: 0.6424, num_updates: 1200, epoch: 10, iterations: 1200, max_updates: 1500, val_time: 10s 921ms, best_update: 1000, best_iteration: 1000, best_val/hateful_memes/roc_auc: 0.658500\n","\u001b[32m2024-04-23T23:51:26 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n","\u001b[32m2024-04-23T23:51:26 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2024-04-23T23:51:28 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2024-04-23T23:51:35 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2024-04-23T23:51:35 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1400/1500, train/hateful_memes/cross_entropy: 0.1676, train/hateful_memes/cross_entropy/avg: 0.2676, train/total_loss: 0.1676, train/total_loss/avg: 0.2676, max mem: 14369.0, experiment: run, epoch: 11, num_updates: 1400, iterations: 1400, max_updates: 1500, lr: 0.00003, ups: 0.81, time: 04m 06s 027ms, time_since_start: 30m 18s 541ms, eta: 02m 08s 549ms\n","\u001b[32m2024-04-23T23:51:35 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n","\u001b[32m2024-04-23T23:51:35 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2024-04-23T23:51:35 | py.warnings: \u001b[0m/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n","  self.pid = os.fork()\n","\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2024-04-23T23:51:35 | py.warnings: \u001b[0m/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n","  self.pid = os.fork()\n","\n","\u001b[32m2024-04-23T23:51:39 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished evaluation inference. Loaded 9\n","\u001b[32m2024-04-23T23:51:39 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n","\u001b[32m2024-04-23T23:51:39 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2024-04-23T23:51:41 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2024-04-23T23:51:47 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2024-04-23T23:51:47 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1400/1500, val/hateful_memes/cross_entropy: 1.5168, val/total_loss: 1.5168, val/hateful_memes/accuracy: 0.6574, val/hateful_memes/binary_f1: 0.3643, val/hateful_memes/roc_auc: 0.6528, num_updates: 1400, epoch: 11, iterations: 1400, max_updates: 1500, val_time: 12s 242ms, best_update: 1000, best_iteration: 1000, best_val/hateful_memes/roc_auc: 0.658500\n","\u001b[32m2024-04-23T23:53:46 | mmf.trainers.core.training_loop: \u001b[0mStepping into final validation check\n","\u001b[32m2024-04-23T23:53:46 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2024-04-23T23:53:46 | py.warnings: \u001b[0m/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n","  self.pid = os.fork()\n","\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2024-04-23T23:53:46 | py.warnings: \u001b[0m/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n","  self.pid = os.fork()\n","\n","\u001b[32m2024-04-23T23:53:50 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished evaluation inference. Loaded 9\n","\u001b[32m2024-04-23T23:53:50 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n","\u001b[32m2024-04-23T23:53:50 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1500/1500, val/hateful_memes/cross_entropy: 1.3859, val/total_loss: 1.3859, val/hateful_memes/accuracy: 0.6481, val/hateful_memes/binary_f1: 0.4477, val/hateful_memes/roc_auc: 0.6672, num_updates: 1500, epoch: 12, iterations: 1500, max_updates: 1500, val_time: 02m 15s 481ms, best_update: 1000, best_iteration: 1000, best_val/hateful_memes/roc_auc: 0.658500\n","\u001b[32m2024-04-23T23:53:50 | mmf.utils.checkpoint: \u001b[0mRestoring checkpoint\n","\u001b[32m2024-04-23T23:53:50 | mmf.utils.checkpoint: \u001b[0mLoading checkpoint\n","\u001b[32m2024-04-23T23:53:51 | mmf.utils.checkpoint: \u001b[0mCheckpoint loaded.\n","\u001b[32m2024-04-23T23:53:51 | mmf.utils.checkpoint: \u001b[0mCurrent num updates: 1000\n","\u001b[32m2024-04-23T23:53:51 | mmf.utils.checkpoint: \u001b[0mCurrent iteration: 1000\n","\u001b[32m2024-04-23T23:53:51 | mmf.utils.checkpoint: \u001b[0mCurrent epoch: 8\n","\u001b[32m2024-04-23T23:53:52 | mmf.trainers.mmf_trainer: \u001b[0mStarting inference on val set\n","\u001b[32m2024-04-23T23:53:52 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n","  0% 0/9 [00:00<?, ?it/s]\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2024-04-23T23:53:52 | py.warnings: \u001b[0m/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n","  self.pid = os.fork()\n","\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2024-04-23T23:53:52 | py.warnings: \u001b[0m/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n","  self.pid = os.fork()\n","\n","100% 9/9 [00:04<00:00,  2.10it/s]\n","\u001b[32m2024-04-23T23:53:56 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished evaluation inference. Loaded 9\n","\u001b[32m2024-04-23T23:53:56 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n","\u001b[32m2024-04-23T23:53:56 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1000/1500, val/hateful_memes/cross_entropy: 1.3110, val/total_loss: 1.3110, val/hateful_memes/accuracy: 0.6611, val/hateful_memes/binary_f1: 0.4116, val/hateful_memes/roc_auc: 0.6585\n","\u001b[32m2024-04-23T23:53:56 | mmf.trainers.callbacks.logistics: \u001b[0mFinished run in 32m 39s 962ms\n"]}]},{"cell_type":"markdown","source":["### Dataset: Appended with images from dev"],"metadata":{"id":"bfoYCO8Pqge3"}},{"cell_type":"code","source":["!mmf_run config=\"./configs/defaults.yaml\" \\\n","  model=visual_bert \\\n","  dataset=hateful_memes \\\n","  run_type=train_val \\\n","  training.log_interval=200 \\\n","  training.batch_size=64 \\\n","  training.evaluation_interval=200 \\\n","  training.tensorboard=True \\\n","  training.checkpoint_interval=200 \\\n","  checkpoint.resume_pretrained=True \\\n","  checkpoint.resume_file=\"/content/drive/MyDrive/hateful-memes/Deep Learning Group Project/visualBERT_vizwiz_04212024_pretrained.ckpt\" \\\n","  dataset_config.hateful_memes.annotations.train[0]=\"hateful_memes/defaults/annotations/train_dev.jsonl\" \\\n","  dataset_config.hateful_memes.annotations.val[0]=\"hateful_memes/defaults/annotations/dev_unseen.jsonl\" \\\n","  dataset_config.hateful_memes.annotations.test[0]=\"hateful_memes/defaults/annotations/test_unseen.jsonl\""],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0cWFuTZTQCyQ","executionInfo":{"status":"ok","timestamp":1713925463661,"user_tz":420,"elapsed":1694353,"user":{"displayName":"Aneri Shah","userId":"14009776940163552827"}},"outputId":"659fb80a-99cd-411c-d7cc-f769e19c8c62"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["2024-04-24 01:47:01.546861: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","2024-04-24 01:47:01.546925: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","2024-04-24 01:47:01.548643: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","2024-04-24 01:47:02.655882: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","2024-04-24 01:47:07.215852: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:47] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n","/usr/local/lib/python3.10/dist-packages/transformers/deepspeed.py:23: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/omegaconf/grammar_visitor.py:253: UserWarning: In the sequence `MMF_USER_DIR,` some elements are missing: please replace them with empty quoted strings. See https://github.com/omry/omegaconf/issues/572 for details.\n","  warnings.warn(\n","\u001b[32m2024-04-24T01:47:13 | mmf.utils.configuration: \u001b[0mOverriding option config to ./configs/defaults.yaml\n","\u001b[32m2024-04-24T01:47:13 | mmf.utils.configuration: \u001b[0mOverriding option model to visual_bert\n","\u001b[32m2024-04-24T01:47:13 | mmf.utils.configuration: \u001b[0mOverriding option datasets to hateful_memes\n","\u001b[32m2024-04-24T01:47:13 | mmf.utils.configuration: \u001b[0mOverriding option run_type to train_val\n","\u001b[32m2024-04-24T01:47:13 | mmf.utils.configuration: \u001b[0mOverriding option training.log_interval to 200\n","\u001b[32m2024-04-24T01:47:13 | mmf.utils.configuration: \u001b[0mOverriding option training.batch_size to 64\n","\u001b[32m2024-04-24T01:47:13 | mmf.utils.configuration: \u001b[0mOverriding option training.evaluation_interval to 200\n","\u001b[32m2024-04-24T01:47:13 | mmf.utils.configuration: \u001b[0mOverriding option training.tensorboard to True\n","\u001b[32m2024-04-24T01:47:13 | mmf.utils.configuration: \u001b[0mOverriding option training.checkpoint_interval to 200\n","\u001b[32m2024-04-24T01:47:13 | mmf.utils.configuration: \u001b[0mOverriding option checkpoint.resume_pretrained to True\n","\u001b[32m2024-04-24T01:47:13 | mmf.utils.configuration: \u001b[0mOverriding option checkpoint.resume_file to /content/drive/MyDrive/hateful-memes/Deep Learning Group Project/visualBERT_vizwiz_04212024_pretrained.ckpt\n","\u001b[32m2024-04-24T01:47:13 | mmf.utils.configuration: \u001b[0mOverriding option dataset_config.hateful_memes.annotations.train[0] to hateful_memes/defaults/annotations/train_dev.jsonl\n","\u001b[32m2024-04-24T01:47:13 | mmf.utils.configuration: \u001b[0mOverriding option dataset_config.hateful_memes.annotations.val[0] to hateful_memes/defaults/annotations/dev_unseen.jsonl\n","\u001b[32m2024-04-24T01:47:13 | mmf.utils.configuration: \u001b[0mOverriding option dataset_config.hateful_memes.annotations.test[0] to hateful_memes/defaults/annotations/test_unseen.jsonl\n","/usr/local/lib/python3.10/dist-packages/omegaconf/grammar_visitor.py:253: UserWarning: In the sequence `MMF_LOG_DIR,` some elements are missing: please replace them with empty quoted strings. See https://github.com/omry/omegaconf/issues/572 for details.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/omegaconf/grammar_visitor.py:253: UserWarning: In the sequence `MMF_REPORT_DIR,` some elements are missing: please replace them with empty quoted strings. See https://github.com/omry/omegaconf/issues/572 for details.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/omegaconf/grammar_visitor.py:253: UserWarning: In the sequence `MMF_TENSORBOARD_LOGDIR,` some elements are missing: please replace them with empty quoted strings. See https://github.com/omry/omegaconf/issues/572 for details.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/omegaconf/grammar_visitor.py:253: UserWarning: In the sequence `MMF_WANDB_LOGDIR,` some elements are missing: please replace them with empty quoted strings. See https://github.com/omry/omegaconf/issues/572 for details.\n","  warnings.warn(\n","\u001b[32m2024-04-24T01:47:13 | mmf: \u001b[0mLogging to: ./save/train.log\n","\u001b[32m2024-04-24T01:47:13 | mmf_cli.run: \u001b[0mNamespace(config_override=None, local_rank=None, opts=['config=./configs/defaults.yaml', 'model=visual_bert', 'dataset=hateful_memes', 'run_type=train_val', 'training.log_interval=200', 'training.batch_size=64', 'training.evaluation_interval=200', 'training.tensorboard=True', 'training.checkpoint_interval=200', 'checkpoint.resume_pretrained=True', 'checkpoint.resume_file=/content/drive/MyDrive/hateful-memes/Deep Learning Group Project/visualBERT_vizwiz_04212024_pretrained.ckpt', 'dataset_config.hateful_memes.annotations.train[0]=hateful_memes/defaults/annotations/train_dev.jsonl', 'dataset_config.hateful_memes.annotations.val[0]=hateful_memes/defaults/annotations/dev_unseen.jsonl', 'dataset_config.hateful_memes.annotations.test[0]=hateful_memes/defaults/annotations/test_unseen.jsonl'])\n","\u001b[32m2024-04-24T01:47:13 | mmf_cli.run: \u001b[0mTorch version: 2.2.0+cu121\n","\u001b[32m2024-04-24T01:47:13 | mmf.utils.general: \u001b[0mCUDA Device 0 is: NVIDIA L4\n","\u001b[32m2024-04-24T01:47:13 | mmf_cli.run: \u001b[0mUsing seed 13917643\n","\u001b[32m2024-04-24T01:47:13 | mmf.trainers.mmf_trainer: \u001b[0mLoading datasets\n","[ Downloading: https://dl.fbaipublicfiles.com/mmf/data/datasets/hateful_memes/defaults/features/features_2020_10_01.tar.gz to /root/.cache/torch/mmf/data/datasets/hateful_memes/defaults/features/features.tar.gz ]\n","Downloading features.tar.gz: 100% 10.3G/10.3G [00:55<00:00, 184MB/s]\n","[ Starting checksum for features.tar.gz]\n","[ Checksum successful for features.tar.gz]\n","Unpacking features.tar.gz\n","tokenizer_config.json: 100% 48.0/48.0 [00:00<00:00, 341kB/s]\n","config.json: 100% 570/570 [00:00<00:00, 4.05MB/s]\n","loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--bert-base-uncased/snapshots/86b5e0934494bd15c9632b12f734a8a67f723594/config.json\n","Model config BertConfig {\n","  \"_name_or_path\": \"bert-base-uncased\",\n","  \"architectures\": [\n","    \"BertForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.39.2\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n","vocab.txt: 100% 232k/232k [00:00<00:00, 1.72MB/s]\n","tokenizer.json: 100% 466k/466k [00:00<00:00, 3.49MB/s]\n","loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--bert-base-uncased/snapshots/86b5e0934494bd15c9632b12f734a8a67f723594/vocab.txt\n","loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--bert-base-uncased/snapshots/86b5e0934494bd15c9632b12f734a8a67f723594/tokenizer.json\n","loading file added_tokens.json from cache at None\n","loading file special_tokens_map.json from cache at None\n","loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--bert-base-uncased/snapshots/86b5e0934494bd15c9632b12f734a8a67f723594/tokenizer_config.json\n","loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--bert-base-uncased/snapshots/86b5e0934494bd15c9632b12f734a8a67f723594/config.json\n","Model config BertConfig {\n","  \"_name_or_path\": \"bert-base-uncased\",\n","  \"architectures\": [\n","    \"BertForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.39.2\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n","\u001b[32m2024-04-24T01:50:47 | mmf.datasets.multi_datamodule: \u001b[0mMultitasking disabled by default for single dataset training\n","\u001b[32m2024-04-24T01:50:47 | mmf.datasets.multi_datamodule: \u001b[0mMultitasking disabled by default for single dataset training\n","\u001b[32m2024-04-24T01:50:47 | mmf.datasets.multi_datamodule: \u001b[0mMultitasking disabled by default for single dataset training\n","\u001b[32m2024-04-24T01:50:47 | mmf.trainers.mmf_trainer: \u001b[0mLoading model\n","Model config BertConfig {\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bert_model_name\": \"bert-base-uncased\",\n","  \"bypass_transformer\": false,\n","  \"classifier_dropout\": null,\n","  \"embedding_strategy\": \"plain\",\n","  \"finetune_lr_multiplier\": 1,\n","  \"freeze_base\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"losses\": [\n","    \"cross_entropy\"\n","  ],\n","  \"max_position_embeddings\": 512,\n","  \"model\": \"visual_bert\",\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"pooler_strategy\": \"default\",\n","  \"position_embedding_type\": \"absolute\",\n","  \"random_initialize\": false,\n","  \"special_visual_initialize\": true,\n","  \"training_head_type\": \"classification\",\n","  \"transformers_version\": \"4.39.2\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"visual_embedding_dim\": 2048,\n","  \"vocab_size\": 30522,\n","  \"zerobias\": false\n","}\n","\n","model.safetensors: 100% 440M/440M [00:01<00:00, 246MB/s]\n","loading weights file model.safetensors from cache at /root/.cache/torch/mmf/distributed_-1/models--bert-base-uncased/snapshots/86b5e0934494bd15c9632b12f734a8a67f723594/model.safetensors\n","Some weights of the model checkpoint at bert-base-uncased were not used when initializing VisualBERTBase: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n","- This IS expected if you are initializing VisualBERTBase from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing VisualBERTBase from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of VisualBERTBase were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['bert.embeddings.position_embeddings_visual.weight', 'bert.embeddings.projection.bias', 'bert.embeddings.projection.weight', 'bert.embeddings.token_type_embeddings_visual.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","\u001b[32m2024-04-24T01:50:49 | mmf.trainers.mmf_trainer: \u001b[0mLoading optimizer\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2024-04-24T01:50:49 | py.warnings: \u001b[0m/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:457: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n","\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2024-04-24T01:50:49 | py.warnings: \u001b[0m/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:457: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n","\n","\u001b[32m2024-04-24T01:50:49 | mmf.trainers.mmf_trainer: \u001b[0mLoading metrics\n","\u001b[32m2024-04-24T01:50:50 | mmf.utils.checkpoint: \u001b[0mLoading checkpoint\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2024-04-24T01:51:05 | mmf: \u001b[0mKey data_parallel is not present in registry, returning default value of None\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2024-04-24T01:51:05 | mmf: \u001b[0mKey distributed is not present in registry, returning default value of None\n","\u001b[32m2024-04-24T01:51:05 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.word_embeddings.weight from model.bert.embeddings.word_embeddings.weight\n","\u001b[32m2024-04-24T01:51:05 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.position_embeddings.weight from model.bert.embeddings.position_embeddings.weight\n","\u001b[32m2024-04-24T01:51:05 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.token_type_embeddings.weight from model.bert.embeddings.token_type_embeddings.weight\n","\u001b[32m2024-04-24T01:51:05 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.LayerNorm.weight from model.bert.embeddings.LayerNorm.weight\n","\u001b[32m2024-04-24T01:51:05 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.LayerNorm.bias from model.bert.embeddings.LayerNorm.bias\n","\u001b[32m2024-04-24T01:51:05 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.token_type_embeddings_visual.weight from model.bert.embeddings.token_type_embeddings_visual.weight\n","\u001b[32m2024-04-24T01:51:05 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.position_embeddings_visual.weight from model.bert.embeddings.position_embeddings_visual.weight\n","\u001b[32m2024-04-24T01:51:05 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.projection.weight from model.bert.embeddings.projection.weight\n","\u001b[32m2024-04-24T01:51:05 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.projection.bias from model.bert.embeddings.projection.bias\n","\u001b[32m2024-04-24T01:51:05 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.self.query.weight from model.bert.encoder.layer.0.attention.self.query.weight\n","\u001b[32m2024-04-24T01:51:05 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.self.query.bias from model.bert.encoder.layer.0.attention.self.query.bias\n","\u001b[32m2024-04-24T01:51:05 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.self.key.weight from model.bert.encoder.layer.0.attention.self.key.weight\n","\u001b[32m2024-04-24T01:51:05 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.self.key.bias from model.bert.encoder.layer.0.attention.self.key.bias\n","\u001b[32m2024-04-24T01:51:05 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.self.value.weight from model.bert.encoder.layer.0.attention.self.value.weight\n","\u001b[32m2024-04-24T01:51:05 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.self.value.bias from model.bert.encoder.layer.0.attention.self.value.bias\n","\u001b[32m2024-04-24T01:51:05 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.output.dense.weight from model.bert.encoder.layer.0.attention.output.dense.weight\n","\u001b[32m2024-04-24T01:51:05 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.output.dense.bias from model.bert.encoder.layer.0.attention.output.dense.bias\n","\u001b[32m2024-04-24T01:51:05 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.output.LayerNorm.weight from model.bert.encoder.layer.0.attention.output.LayerNorm.weight\n","\u001b[32m2024-04-24T01:51:05 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.output.LayerNorm.bias from model.bert.encoder.layer.0.attention.output.LayerNorm.bias\n","\u001b[32m2024-04-24T01:51:05 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.intermediate.dense.weight from model.bert.encoder.layer.0.intermediate.dense.weight\n","\u001b[32m2024-04-24T01:51:05 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.intermediate.dense.bias from model.bert.encoder.layer.0.intermediate.dense.bias\n","\u001b[32m2024-04-24T01:51:05 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.output.dense.weight from model.bert.encoder.layer.0.output.dense.weight\n","\u001b[32m2024-04-24T01:51:05 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.output.dense.bias from model.bert.encoder.layer.0.output.dense.bias\n","\u001b[32m2024-04-24T01:51:05 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.output.LayerNorm.weight from model.bert.encoder.layer.0.output.LayerNorm.weight\n","\u001b[32m2024-04-24T01:51:05 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.output.LayerNorm.bias from model.bert.encoder.layer.0.output.LayerNorm.bias\n","\u001b[32m2024-04-24T01:51:05 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.self.query.weight from model.bert.encoder.layer.1.attention.self.query.weight\n","\u001b[32m2024-04-24T01:51:05 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.self.query.bias from model.bert.encoder.layer.1.attention.self.query.bias\n","\u001b[32m2024-04-24T01:51:05 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.self.key.weight from model.bert.encoder.layer.1.attention.self.key.weight\n","\u001b[32m2024-04-24T01:51:05 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.self.key.bias from model.bert.encoder.layer.1.attention.self.key.bias\n","\u001b[32m2024-04-24T01:51:05 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.self.value.weight from model.bert.encoder.layer.1.attention.self.value.weight\n","\u001b[32m2024-04-24T01:51:05 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.self.value.bias from model.bert.encoder.layer.1.attention.self.value.bias\n","\u001b[32m2024-04-24T01:51:05 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.output.dense.weight from model.bert.encoder.layer.1.attention.output.dense.weight\n","\u001b[32m2024-04-24T01:51:05 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.output.dense.bias from model.bert.encoder.layer.1.attention.output.dense.bias\n","\u001b[32m2024-04-24T01:51:05 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.output.LayerNorm.weight from model.bert.encoder.layer.1.attention.output.LayerNorm.weight\n","\u001b[32m2024-04-24T01:51:05 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.output.LayerNorm.bias from model.bert.encoder.layer.1.attention.output.LayerNorm.bias\n","\u001b[32m2024-04-24T01:51:05 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.intermediate.dense.weight from model.bert.encoder.layer.1.intermediate.dense.weight\n","\u001b[32m2024-04-24T01:51:05 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.intermediate.dense.bias from model.bert.encoder.layer.1.intermediate.dense.bias\n","\u001b[32m2024-04-24T01:51:05 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.output.dense.weight from model.bert.encoder.layer.1.output.dense.weight\n","\u001b[32m2024-04-24T01:51:05 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.output.dense.bias from model.bert.encoder.layer.1.output.dense.bias\n","\u001b[32m2024-04-24T01:51:05 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.output.LayerNorm.weight from model.bert.encoder.layer.1.output.LayerNorm.weight\n","\u001b[32m2024-04-24T01:51:05 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.output.LayerNorm.bias from model.bert.encoder.layer.1.output.LayerNorm.bias\n","\u001b[32m2024-04-24T01:51:05 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.self.query.weight from model.bert.encoder.layer.2.attention.self.query.weight\n","\u001b[32m2024-04-24T01:51:05 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.self.query.bias from model.bert.encoder.layer.2.attention.self.query.bias\n","\u001b[32m2024-04-24T01:51:05 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.self.key.weight from model.bert.encoder.layer.2.attention.self.key.weight\n","\u001b[32m2024-04-24T01:51:05 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.self.key.bias from model.bert.encoder.layer.2.attention.self.key.bias\n","\u001b[32m2024-04-24T01:51:05 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.self.value.weight from model.bert.encoder.layer.2.attention.self.value.weight\n","\u001b[32m2024-04-24T01:51:05 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.self.value.bias from model.bert.encoder.layer.2.attention.self.value.bias\n","\u001b[32m2024-04-24T01:51:05 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.output.dense.weight from model.bert.encoder.layer.2.attention.output.dense.weight\n","\u001b[32m2024-04-24T01:51:05 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.output.dense.bias from model.bert.encoder.layer.2.attention.output.dense.bias\n","\u001b[32m2024-04-24T01:51:05 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.output.LayerNorm.weight from model.bert.encoder.layer.2.attention.output.LayerNorm.weight\n","\u001b[32m2024-04-24T01:51:05 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.output.LayerNorm.bias from model.bert.encoder.layer.2.attention.output.LayerNorm.bias\n","\u001b[32m2024-04-24T01:51:05 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.intermediate.dense.weight from model.bert.encoder.layer.2.intermediate.dense.weight\n","\u001b[32m2024-04-24T01:51:05 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.intermediate.dense.bias from model.bert.encoder.layer.2.intermediate.dense.bias\n","\u001b[32m2024-04-24T01:51:05 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.output.dense.weight from model.bert.encoder.layer.2.output.dense.weight\n","\u001b[32m2024-04-24T01:51:05 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.output.dense.bias from model.bert.encoder.layer.2.output.dense.bias\n","\u001b[32m2024-04-24T01:51:05 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.output.LayerNorm.weight from model.bert.encoder.layer.2.output.LayerNorm.weight\n","\u001b[32m2024-04-24T01:51:05 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.output.LayerNorm.bias from model.bert.encoder.layer.2.output.LayerNorm.bias\n","\u001b[32m2024-04-24T01:51:05 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.self.query.weight from model.bert.encoder.layer.3.attention.self.query.weight\n","\u001b[32m2024-04-24T01:51:05 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.self.query.bias from model.bert.encoder.layer.3.attention.self.query.bias\n","\u001b[32m2024-04-24T01:51:05 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.self.key.weight from model.bert.encoder.layer.3.attention.self.key.weight\n","\u001b[32m2024-04-24T01:51:05 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.self.key.bias from model.bert.encoder.layer.3.attention.self.key.bias\n","\u001b[32m2024-04-24T01:51:05 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.self.value.weight from model.bert.encoder.layer.3.attention.self.value.weight\n","\u001b[32m2024-04-24T01:51:05 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.self.value.bias from model.bert.encoder.layer.3.attention.self.value.bias\n","\u001b[32m2024-04-24T01:51:05 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.output.dense.weight from model.bert.encoder.layer.3.attention.output.dense.weight\n","\u001b[32m2024-04-24T01:51:05 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.output.dense.bias from model.bert.encoder.layer.3.attention.output.dense.bias\n","\u001b[32m2024-04-24T01:51:05 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.output.LayerNorm.weight from model.bert.encoder.layer.3.attention.output.LayerNorm.weight\n","\u001b[32m2024-04-24T01:51:05 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.output.LayerNorm.bias from model.bert.encoder.layer.3.attention.output.LayerNorm.bias\n","\u001b[32m2024-04-24T01:51:05 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.intermediate.dense.weight from model.bert.encoder.layer.3.intermediate.dense.weight\n","\u001b[32m2024-04-24T01:51:05 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.intermediate.dense.bias from model.bert.encoder.layer.3.intermediate.dense.bias\n","\u001b[32m2024-04-24T01:51:05 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.output.dense.weight from model.bert.encoder.layer.3.output.dense.weight\n","\u001b[32m2024-04-24T01:51:05 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.output.dense.bias from model.bert.encoder.layer.3.output.dense.bias\n","\u001b[32m2024-04-24T01:51:05 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.output.LayerNorm.weight from model.bert.encoder.layer.3.output.LayerNorm.weight\n","\u001b[32m2024-04-24T01:51:05 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.output.LayerNorm.bias from model.bert.encoder.layer.3.output.LayerNorm.bias\n","\u001b[32m2024-04-24T01:51:05 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.self.query.weight from model.bert.encoder.layer.4.attention.self.query.weight\n","\u001b[32m2024-04-24T01:51:05 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.self.query.bias from model.bert.encoder.layer.4.attention.self.query.bias\n","\u001b[32m2024-04-24T01:51:05 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.self.key.weight from model.bert.encoder.layer.4.attention.self.key.weight\n","\u001b[32m2024-04-24T01:51:05 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.self.key.bias from model.bert.encoder.layer.4.attention.self.key.bias\n","\u001b[32m2024-04-24T01:51:05 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.self.value.weight from model.bert.encoder.layer.4.attention.self.value.weight\n","\u001b[32m2024-04-24T01:51:05 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.self.value.bias from model.bert.encoder.layer.4.attention.self.value.bias\n","\u001b[32m2024-04-24T01:51:05 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.output.dense.weight from model.bert.encoder.layer.4.attention.output.dense.weight\n","\u001b[32m2024-04-24T01:51:05 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.output.dense.bias from model.bert.encoder.layer.4.attention.output.dense.bias\n","\u001b[32m2024-04-24T01:51:05 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.output.LayerNorm.weight from model.bert.encoder.layer.4.attention.output.LayerNorm.weight\n","\u001b[32m2024-04-24T01:51:05 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.output.LayerNorm.bias from model.bert.encoder.layer.4.attention.output.LayerNorm.bias\n","\u001b[32m2024-04-24T01:51:05 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.intermediate.dense.weight from model.bert.encoder.layer.4.intermediate.dense.weight\n","\u001b[32m2024-04-24T01:51:05 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.intermediate.dense.bias from model.bert.encoder.layer.4.intermediate.dense.bias\n","\u001b[32m2024-04-24T01:51:05 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.output.dense.weight from model.bert.encoder.layer.4.output.dense.weight\n","\u001b[32m2024-04-24T01:51:05 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.output.dense.bias from model.bert.encoder.layer.4.output.dense.bias\n","\u001b[32m2024-04-24T01:51:05 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.output.LayerNorm.weight from model.bert.encoder.layer.4.output.LayerNorm.weight\n","\u001b[32m2024-04-24T01:51:05 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.output.LayerNorm.bias from model.bert.encoder.layer.4.output.LayerNorm.bias\n","\u001b[32m2024-04-24T01:51:05 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.self.query.weight from model.bert.encoder.layer.5.attention.self.query.weight\n","\u001b[32m2024-04-24T01:51:05 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.self.query.bias from model.bert.encoder.layer.5.attention.self.query.bias\n","\u001b[32m2024-04-24T01:51:05 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.self.key.weight from model.bert.encoder.layer.5.attention.self.key.weight\n","\u001b[32m2024-04-24T01:51:05 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.self.key.bias from model.bert.encoder.layer.5.attention.self.key.bias\n","\u001b[32m2024-04-24T01:51:05 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.self.value.weight from model.bert.encoder.layer.5.attention.self.value.weight\n","\u001b[32m2024-04-24T01:51:05 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.self.value.bias from model.bert.encoder.layer.5.attention.self.value.bias\n","\u001b[32m2024-04-24T01:51:05 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.output.dense.weight from model.bert.encoder.layer.5.attention.output.dense.weight\n","\u001b[32m2024-04-24T01:51:05 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.output.dense.bias from model.bert.encoder.layer.5.attention.output.dense.bias\n","\u001b[32m2024-04-24T01:51:05 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.output.LayerNorm.weight from model.bert.encoder.layer.5.attention.output.LayerNorm.weight\n","\u001b[32m2024-04-24T01:51:05 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.output.LayerNorm.bias from model.bert.encoder.layer.5.attention.output.LayerNorm.bias\n","\u001b[32m2024-04-24T01:51:05 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.intermediate.dense.weight from model.bert.encoder.layer.5.intermediate.dense.weight\n","\u001b[32m2024-04-24T01:51:05 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.intermediate.dense.bias from model.bert.encoder.layer.5.intermediate.dense.bias\n","\u001b[32m2024-04-24T01:51:05 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.output.dense.weight from model.bert.encoder.layer.5.output.dense.weight\n","\u001b[32m2024-04-24T01:51:05 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.output.dense.bias from model.bert.encoder.layer.5.output.dense.bias\n","\u001b[32m2024-04-24T01:51:05 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.output.LayerNorm.weight from model.bert.encoder.layer.5.output.LayerNorm.weight\n","\u001b[32m2024-04-24T01:51:05 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.output.LayerNorm.bias from model.bert.encoder.layer.5.output.LayerNorm.bias\n","\u001b[32m2024-04-24T01:51:05 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.self.query.weight from model.bert.encoder.layer.6.attention.self.query.weight\n","\u001b[32m2024-04-24T01:51:05 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.self.query.bias from model.bert.encoder.layer.6.attention.self.query.bias\n","\u001b[32m2024-04-24T01:51:05 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.self.key.weight from model.bert.encoder.layer.6.attention.self.key.weight\n","\u001b[32m2024-04-24T01:51:05 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.self.key.bias from model.bert.encoder.layer.6.attention.self.key.bias\n","\u001b[32m2024-04-24T01:51:05 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.self.value.weight from model.bert.encoder.layer.6.attention.self.value.weight\n","\u001b[32m2024-04-24T01:51:05 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.self.value.bias from model.bert.encoder.layer.6.attention.self.value.bias\n","\u001b[32m2024-04-24T01:51:05 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.output.dense.weight from model.bert.encoder.layer.6.attention.output.dense.weight\n","\u001b[32m2024-04-24T01:51:05 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.output.dense.bias from model.bert.encoder.layer.6.attention.output.dense.bias\n","\u001b[32m2024-04-24T01:51:05 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.output.LayerNorm.weight from model.bert.encoder.layer.6.attention.output.LayerNorm.weight\n","\u001b[32m2024-04-24T01:51:05 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.output.LayerNorm.bias from model.bert.encoder.layer.6.attention.output.LayerNorm.bias\n","\u001b[32m2024-04-24T01:51:05 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.intermediate.dense.weight from model.bert.encoder.layer.6.intermediate.dense.weight\n","\u001b[32m2024-04-24T01:51:05 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.intermediate.dense.bias from model.bert.encoder.layer.6.intermediate.dense.bias\n","\u001b[32m2024-04-24T01:51:05 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.output.dense.weight from model.bert.encoder.layer.6.output.dense.weight\n","\u001b[32m2024-04-24T01:51:05 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.output.dense.bias from model.bert.encoder.layer.6.output.dense.bias\n","\u001b[32m2024-04-24T01:51:05 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.output.LayerNorm.weight from model.bert.encoder.layer.6.output.LayerNorm.weight\n","\u001b[32m2024-04-24T01:51:05 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.output.LayerNorm.bias from model.bert.encoder.layer.6.output.LayerNorm.bias\n","\u001b[32m2024-04-24T01:51:05 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.self.query.weight from model.bert.encoder.layer.7.attention.self.query.weight\n","\u001b[32m2024-04-24T01:51:05 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.self.query.bias from model.bert.encoder.layer.7.attention.self.query.bias\n","\u001b[32m2024-04-24T01:51:05 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.self.key.weight from model.bert.encoder.layer.7.attention.self.key.weight\n","\u001b[32m2024-04-24T01:51:05 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.self.key.bias from model.bert.encoder.layer.7.attention.self.key.bias\n","\u001b[32m2024-04-24T01:51:05 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.self.value.weight from model.bert.encoder.layer.7.attention.self.value.weight\n","\u001b[32m2024-04-24T01:51:05 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.self.value.bias from model.bert.encoder.layer.7.attention.self.value.bias\n","\u001b[32m2024-04-24T01:51:05 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.output.dense.weight from model.bert.encoder.layer.7.attention.output.dense.weight\n","\u001b[32m2024-04-24T01:51:05 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.output.dense.bias from model.bert.encoder.layer.7.attention.output.dense.bias\n","\u001b[32m2024-04-24T01:51:05 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.output.LayerNorm.weight from model.bert.encoder.layer.7.attention.output.LayerNorm.weight\n","\u001b[32m2024-04-24T01:51:05 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.output.LayerNorm.bias from model.bert.encoder.layer.7.attention.output.LayerNorm.bias\n","\u001b[32m2024-04-24T01:51:05 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.intermediate.dense.weight from model.bert.encoder.layer.7.intermediate.dense.weight\n","\u001b[32m2024-04-24T01:51:05 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.intermediate.dense.bias from model.bert.encoder.layer.7.intermediate.dense.bias\n","\u001b[32m2024-04-24T01:51:05 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.output.dense.weight from model.bert.encoder.layer.7.output.dense.weight\n","\u001b[32m2024-04-24T01:51:05 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.output.dense.bias from model.bert.encoder.layer.7.output.dense.bias\n","\u001b[32m2024-04-24T01:51:05 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.output.LayerNorm.weight from model.bert.encoder.layer.7.output.LayerNorm.weight\n","\u001b[32m2024-04-24T01:51:05 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.output.LayerNorm.bias from model.bert.encoder.layer.7.output.LayerNorm.bias\n","\u001b[32m2024-04-24T01:51:05 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.self.query.weight from model.bert.encoder.layer.8.attention.self.query.weight\n","\u001b[32m2024-04-24T01:51:05 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.self.query.bias from model.bert.encoder.layer.8.attention.self.query.bias\n","\u001b[32m2024-04-24T01:51:05 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.self.key.weight from model.bert.encoder.layer.8.attention.self.key.weight\n","\u001b[32m2024-04-24T01:51:05 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.self.key.bias from model.bert.encoder.layer.8.attention.self.key.bias\n","\u001b[32m2024-04-24T01:51:05 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.self.value.weight from model.bert.encoder.layer.8.attention.self.value.weight\n","\u001b[32m2024-04-24T01:51:05 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.self.value.bias from model.bert.encoder.layer.8.attention.self.value.bias\n","\u001b[32m2024-04-24T01:51:05 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.output.dense.weight from model.bert.encoder.layer.8.attention.output.dense.weight\n","\u001b[32m2024-04-24T01:51:05 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.output.dense.bias from model.bert.encoder.layer.8.attention.output.dense.bias\n","\u001b[32m2024-04-24T01:51:05 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.output.LayerNorm.weight from model.bert.encoder.layer.8.attention.output.LayerNorm.weight\n","\u001b[32m2024-04-24T01:51:05 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.output.LayerNorm.bias from model.bert.encoder.layer.8.attention.output.LayerNorm.bias\n","\u001b[32m2024-04-24T01:51:05 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.intermediate.dense.weight from model.bert.encoder.layer.8.intermediate.dense.weight\n","\u001b[32m2024-04-24T01:51:05 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.intermediate.dense.bias from model.bert.encoder.layer.8.intermediate.dense.bias\n","\u001b[32m2024-04-24T01:51:05 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.output.dense.weight from model.bert.encoder.layer.8.output.dense.weight\n","\u001b[32m2024-04-24T01:51:05 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.output.dense.bias from model.bert.encoder.layer.8.output.dense.bias\n","\u001b[32m2024-04-24T01:51:05 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.output.LayerNorm.weight from model.bert.encoder.layer.8.output.LayerNorm.weight\n","\u001b[32m2024-04-24T01:51:05 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.output.LayerNorm.bias from model.bert.encoder.layer.8.output.LayerNorm.bias\n","\u001b[32m2024-04-24T01:51:05 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.self.query.weight from model.bert.encoder.layer.9.attention.self.query.weight\n","\u001b[32m2024-04-24T01:51:05 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.self.query.bias from model.bert.encoder.layer.9.attention.self.query.bias\n","\u001b[32m2024-04-24T01:51:05 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.self.key.weight from model.bert.encoder.layer.9.attention.self.key.weight\n","\u001b[32m2024-04-24T01:51:05 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.self.key.bias from model.bert.encoder.layer.9.attention.self.key.bias\n","\u001b[32m2024-04-24T01:51:05 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.self.value.weight from model.bert.encoder.layer.9.attention.self.value.weight\n","\u001b[32m2024-04-24T01:51:05 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.self.value.bias from model.bert.encoder.layer.9.attention.self.value.bias\n","\u001b[32m2024-04-24T01:51:05 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.output.dense.weight from model.bert.encoder.layer.9.attention.output.dense.weight\n","\u001b[32m2024-04-24T01:51:05 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.output.dense.bias from model.bert.encoder.layer.9.attention.output.dense.bias\n","\u001b[32m2024-04-24T01:51:05 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.output.LayerNorm.weight from model.bert.encoder.layer.9.attention.output.LayerNorm.weight\n","\u001b[32m2024-04-24T01:51:05 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.output.LayerNorm.bias from model.bert.encoder.layer.9.attention.output.LayerNorm.bias\n","\u001b[32m2024-04-24T01:51:05 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.intermediate.dense.weight from model.bert.encoder.layer.9.intermediate.dense.weight\n","\u001b[32m2024-04-24T01:51:05 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.intermediate.dense.bias from model.bert.encoder.layer.9.intermediate.dense.bias\n","\u001b[32m2024-04-24T01:51:05 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.output.dense.weight from model.bert.encoder.layer.9.output.dense.weight\n","\u001b[32m2024-04-24T01:51:05 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.output.dense.bias from model.bert.encoder.layer.9.output.dense.bias\n","\u001b[32m2024-04-24T01:51:05 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.output.LayerNorm.weight from model.bert.encoder.layer.9.output.LayerNorm.weight\n","\u001b[32m2024-04-24T01:51:05 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.output.LayerNorm.bias from model.bert.encoder.layer.9.output.LayerNorm.bias\n","\u001b[32m2024-04-24T01:51:05 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.self.query.weight from model.bert.encoder.layer.10.attention.self.query.weight\n","\u001b[32m2024-04-24T01:51:05 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.self.query.bias from model.bert.encoder.layer.10.attention.self.query.bias\n","\u001b[32m2024-04-24T01:51:05 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.self.key.weight from model.bert.encoder.layer.10.attention.self.key.weight\n","\u001b[32m2024-04-24T01:51:05 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.self.key.bias from model.bert.encoder.layer.10.attention.self.key.bias\n","\u001b[32m2024-04-24T01:51:05 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.self.value.weight from model.bert.encoder.layer.10.attention.self.value.weight\n","\u001b[32m2024-04-24T01:51:05 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.self.value.bias from model.bert.encoder.layer.10.attention.self.value.bias\n","\u001b[32m2024-04-24T01:51:05 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.output.dense.weight from model.bert.encoder.layer.10.attention.output.dense.weight\n","\u001b[32m2024-04-24T01:51:05 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.output.dense.bias from model.bert.encoder.layer.10.attention.output.dense.bias\n","\u001b[32m2024-04-24T01:51:05 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.output.LayerNorm.weight from model.bert.encoder.layer.10.attention.output.LayerNorm.weight\n","\u001b[32m2024-04-24T01:51:05 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.output.LayerNorm.bias from model.bert.encoder.layer.10.attention.output.LayerNorm.bias\n","\u001b[32m2024-04-24T01:51:05 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.intermediate.dense.weight from model.bert.encoder.layer.10.intermediate.dense.weight\n","\u001b[32m2024-04-24T01:51:05 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.intermediate.dense.bias from model.bert.encoder.layer.10.intermediate.dense.bias\n","\u001b[32m2024-04-24T01:51:05 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.output.dense.weight from model.bert.encoder.layer.10.output.dense.weight\n","\u001b[32m2024-04-24T01:51:05 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.output.dense.bias from model.bert.encoder.layer.10.output.dense.bias\n","\u001b[32m2024-04-24T01:51:05 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.output.LayerNorm.weight from model.bert.encoder.layer.10.output.LayerNorm.weight\n","\u001b[32m2024-04-24T01:51:05 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.output.LayerNorm.bias from model.bert.encoder.layer.10.output.LayerNorm.bias\n","\u001b[32m2024-04-24T01:51:05 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.self.query.weight from model.bert.encoder.layer.11.attention.self.query.weight\n","\u001b[32m2024-04-24T01:51:05 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.self.query.bias from model.bert.encoder.layer.11.attention.self.query.bias\n","\u001b[32m2024-04-24T01:51:05 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.self.key.weight from model.bert.encoder.layer.11.attention.self.key.weight\n","\u001b[32m2024-04-24T01:51:05 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.self.key.bias from model.bert.encoder.layer.11.attention.self.key.bias\n","\u001b[32m2024-04-24T01:51:05 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.self.value.weight from model.bert.encoder.layer.11.attention.self.value.weight\n","\u001b[32m2024-04-24T01:51:05 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.self.value.bias from model.bert.encoder.layer.11.attention.self.value.bias\n","\u001b[32m2024-04-24T01:51:05 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.output.dense.weight from model.bert.encoder.layer.11.attention.output.dense.weight\n","\u001b[32m2024-04-24T01:51:05 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.output.dense.bias from model.bert.encoder.layer.11.attention.output.dense.bias\n","\u001b[32m2024-04-24T01:51:05 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.output.LayerNorm.weight from model.bert.encoder.layer.11.attention.output.LayerNorm.weight\n","\u001b[32m2024-04-24T01:51:05 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.output.LayerNorm.bias from model.bert.encoder.layer.11.attention.output.LayerNorm.bias\n","\u001b[32m2024-04-24T01:51:05 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.intermediate.dense.weight from model.bert.encoder.layer.11.intermediate.dense.weight\n","\u001b[32m2024-04-24T01:51:05 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.intermediate.dense.bias from model.bert.encoder.layer.11.intermediate.dense.bias\n","\u001b[32m2024-04-24T01:51:05 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.output.dense.weight from model.bert.encoder.layer.11.output.dense.weight\n","\u001b[32m2024-04-24T01:51:05 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.output.dense.bias from model.bert.encoder.layer.11.output.dense.bias\n","\u001b[32m2024-04-24T01:51:05 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.output.LayerNorm.weight from model.bert.encoder.layer.11.output.LayerNorm.weight\n","\u001b[32m2024-04-24T01:51:05 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.output.LayerNorm.bias from model.bert.encoder.layer.11.output.LayerNorm.bias\n","\u001b[32m2024-04-24T01:51:05 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.pooler.dense.weight from model.bert.pooler.dense.weight\n","\u001b[32m2024-04-24T01:51:05 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.pooler.dense.bias from model.bert.pooler.dense.bias\n","\u001b[32m2024-04-24T01:51:05 | mmf.utils.checkpoint: \u001b[0mPretrained model loaded\n","\u001b[32m2024-04-24T01:51:05 | mmf.utils.checkpoint: \u001b[0mCheckpoint loaded.\n","\u001b[32m2024-04-24T01:51:05 | mmf.utils.checkpoint: \u001b[0mCurrent num updates: 0\n","\u001b[32m2024-04-24T01:51:05 | mmf.utils.checkpoint: \u001b[0mCurrent iteration: 0\n","\u001b[32m2024-04-24T01:51:05 | mmf.utils.checkpoint: \u001b[0mCurrent epoch: 0\n","\u001b[32m2024-04-24T01:51:05 | mmf.trainers.mmf_trainer: \u001b[0m===== Model =====\n","\u001b[32m2024-04-24T01:51:05 | mmf.trainers.mmf_trainer: \u001b[0mVisualBERT(\n","  (model): VisualBERTForClassification(\n","    (bert): VisualBERTBase(\n","      (embeddings): BertVisioLinguisticEmbeddings(\n","        (word_embeddings): Embedding(30522, 768, padding_idx=0)\n","        (position_embeddings): Embedding(512, 768)\n","        (token_type_embeddings): Embedding(2, 768)\n","        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","        (token_type_embeddings_visual): Embedding(2, 768)\n","        (position_embeddings_visual): Embedding(512, 768)\n","        (projection): Linear(in_features=2048, out_features=768, bias=True)\n","      )\n","      (encoder): BertEncoderJit(\n","        (layer): ModuleList(\n","          (0-11): 12 x BertLayerJit(\n","            (attention): BertAttentionJit(\n","              (self): BertSelfAttentionJit(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","              (intermediate_act_fn): GELUActivation()\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","        )\n","      )\n","      (pooler): BertPooler(\n","        (dense): Linear(in_features=768, out_features=768, bias=True)\n","        (activation): Tanh()\n","      )\n","    )\n","    (dropout): Dropout(p=0.1, inplace=False)\n","    (classifier): Sequential(\n","      (0): BertPredictionHeadTransform(\n","        (dense): Linear(in_features=768, out_features=768, bias=True)\n","        (transform_act_fn): GELUActivation()\n","        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      )\n","      (1): Linear(in_features=768, out_features=2, bias=True)\n","    )\n","  )\n","  (losses): Losses(\n","    (losses): ModuleList(\n","      (0): MMFLoss(\n","        (loss_criterion): CrossEntropyLoss(\n","          (loss_fn): CrossEntropyLoss()\n","        )\n","      )\n","    )\n","  )\n",")\n","\u001b[32m2024-04-24T01:51:05 | mmf.utils.general: \u001b[0mTotal Parameters: 112044290. Trained Parameters: 112044290\n","\u001b[32m2024-04-24T01:51:05 | mmf.trainers.core.training_loop: \u001b[0mStarting training...\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2024-04-24T01:51:05 | py.warnings: \u001b[0m/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n","  self.pid = os.fork()\n","\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2024-04-24T01:51:05 | py.warnings: \u001b[0m/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n","  self.pid = os.fork()\n","\n","\u001b[32m2024-04-24T01:55:06 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n","\u001b[32m2024-04-24T01:55:06 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2024-04-24T01:55:08 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2024-04-24T01:55:11 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2024-04-24T01:55:11 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 200/1500, train/hateful_memes/cross_entropy: 0.5998, train/hateful_memes/cross_entropy/avg: 0.5998, train/total_loss: 0.5998, train/total_loss/avg: 0.5998, max mem: 14320.0, experiment: run, epoch: 2, num_updates: 200, iterations: 200, max_updates: 1500, lr: 0.00001, ups: 0.81, time: 04m 06s 278ms, time_since_start: 04m 21s 777ms, eta: 27m 52s 849ms\n","\u001b[32m2024-04-24T01:55:11 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n","\u001b[32m2024-04-24T01:55:11 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n","\u001b[32m2024-04-24T01:55:16 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished evaluation inference. Loaded 9\n","\u001b[32m2024-04-24T01:55:16 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n","\u001b[32m2024-04-24T01:55:16 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2024-04-24T01:55:18 | mmf.utils.checkpoint: \u001b[0mSaving best checkpoint\n","\u001b[32m2024-04-24T01:55:22 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2024-04-24T01:55:31 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2024-04-24T01:55:31 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 200/1500, val/hateful_memes/cross_entropy: 0.6781, val/total_loss: 0.6781, val/hateful_memes/accuracy: 0.6278, val/hateful_memes/binary_f1: 0.2239, val/hateful_memes/roc_auc: 0.5391, num_updates: 200, epoch: 2, iterations: 200, max_updates: 1500, val_time: 19s 927ms, best_update: 200, best_iteration: 200, best_val/hateful_memes/roc_auc: 0.539132\n","\u001b[32m2024-04-24T01:59:29 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n","\u001b[32m2024-04-24T01:59:29 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2024-04-24T01:59:31 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2024-04-24T01:59:37 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2024-04-24T01:59:37 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 400/1500, train/hateful_memes/cross_entropy: 0.5998, train/hateful_memes/cross_entropy/avg: 0.6216, train/total_loss: 0.5998, train/total_loss/avg: 0.6216, max mem: 14369.0, experiment: run, epoch: 3, num_updates: 400, iterations: 400, max_updates: 1500, lr: 0.00001, ups: 0.82, time: 04m 05s 397ms, time_since_start: 08m 47s 111ms, eta: 23m 30s 424ms\n","\u001b[32m2024-04-24T01:59:37 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n","\u001b[32m2024-04-24T01:59:37 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2024-04-24T01:59:37 | py.warnings: \u001b[0m/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n","  self.pid = os.fork()\n","\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2024-04-24T01:59:37 | py.warnings: \u001b[0m/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n","  self.pid = os.fork()\n","\n","\u001b[32m2024-04-24T01:59:41 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished evaluation inference. Loaded 9\n","\u001b[32m2024-04-24T01:59:41 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n","\u001b[32m2024-04-24T01:59:41 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2024-04-24T01:59:44 | mmf.utils.checkpoint: \u001b[0mSaving best checkpoint\n","\u001b[32m2024-04-24T01:59:54 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2024-04-24T02:00:04 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2024-04-24T02:00:04 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 400/1500, val/hateful_memes/cross_entropy: 0.7043, val/total_loss: 0.7043, val/hateful_memes/accuracy: 0.5833, val/hateful_memes/binary_f1: 0.4125, val/hateful_memes/roc_auc: 0.5717, num_updates: 400, epoch: 3, iterations: 400, max_updates: 1500, val_time: 27s 165ms, best_update: 400, best_iteration: 400, best_val/hateful_memes/roc_auc: 0.571750\n","\u001b[32m2024-04-24T02:04:01 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n","\u001b[32m2024-04-24T02:04:01 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2024-04-24T02:04:04 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2024-04-24T02:04:08 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2024-04-24T02:04:08 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 600/1500, train/hateful_memes/cross_entropy: 0.5998, train/hateful_memes/cross_entropy/avg: 0.5064, train/total_loss: 0.5998, train/total_loss/avg: 0.5064, max mem: 14369.0, experiment: run, epoch: 5, num_updates: 600, iterations: 600, max_updates: 1500, lr: 0.00002, ups: 0.82, time: 04m 04s 075ms, time_since_start: 13m 18s 354ms, eta: 19m 07s 763ms\n","\u001b[32m2024-04-24T02:04:08 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n","\u001b[32m2024-04-24T02:04:08 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2024-04-24T02:04:08 | py.warnings: \u001b[0m/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n","  self.pid = os.fork()\n","\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2024-04-24T02:04:08 | py.warnings: \u001b[0m/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n","  self.pid = os.fork()\n","\n","\u001b[32m2024-04-24T02:04:12 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished evaluation inference. Loaded 9\n","\u001b[32m2024-04-24T02:04:12 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n","\u001b[32m2024-04-24T02:04:12 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2024-04-24T02:04:15 | mmf.utils.checkpoint: \u001b[0mSaving best checkpoint\n","\u001b[32m2024-04-24T02:04:20 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2024-04-24T02:04:24 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2024-04-24T02:04:24 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 600/1500, val/hateful_memes/cross_entropy: 1.0261, val/total_loss: 1.0261, val/hateful_memes/accuracy: 0.6259, val/hateful_memes/binary_f1: 0.2681, val/hateful_memes/roc_auc: 0.5969, num_updates: 600, epoch: 5, iterations: 600, max_updates: 1500, val_time: 16s 344ms, best_update: 600, best_iteration: 600, best_val/hateful_memes/roc_auc: 0.596926\n","\u001b[32m2024-04-24T02:08:22 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n","\u001b[32m2024-04-24T02:08:22 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2024-04-24T02:08:24 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2024-04-24T02:08:35 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2024-04-24T02:08:35 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 800/1500, train/hateful_memes/cross_entropy: 0.2760, train/hateful_memes/cross_entropy/avg: 0.4264, train/total_loss: 0.2760, train/total_loss/avg: 0.4264, max mem: 14369.0, experiment: run, epoch: 6, num_updates: 800, iterations: 800, max_updates: 1500, lr: 0.00002, ups: 0.80, time: 04m 11s 290ms, time_since_start: 17m 45s 991ms, eta: 15m 19s 096ms\n","\u001b[32m2024-04-24T02:08:35 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n","\u001b[32m2024-04-24T02:08:36 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2024-04-24T02:08:36 | py.warnings: \u001b[0m/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n","  self.pid = os.fork()\n","\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2024-04-24T02:08:36 | py.warnings: \u001b[0m/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n","  self.pid = os.fork()\n","\n","\u001b[32m2024-04-24T02:08:40 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished evaluation inference. Loaded 9\n","\u001b[32m2024-04-24T02:08:40 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n","\u001b[32m2024-04-24T02:08:40 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2024-04-24T02:08:47 | mmf.utils.checkpoint: \u001b[0mSaving best checkpoint\n","\u001b[32m2024-04-24T02:08:50 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2024-04-24T02:09:02 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2024-04-24T02:09:02 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 800/1500, val/hateful_memes/cross_entropy: 1.0863, val/total_loss: 1.0863, val/hateful_memes/accuracy: 0.6241, val/hateful_memes/binary_f1: 0.3514, val/hateful_memes/roc_auc: 0.6200, num_updates: 800, epoch: 6, iterations: 800, max_updates: 1500, val_time: 26s 342ms, best_update: 800, best_iteration: 800, best_val/hateful_memes/roc_auc: 0.620000\n","\u001b[32m2024-04-24T02:13:00 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n","\u001b[32m2024-04-24T02:13:00 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2024-04-24T02:13:02 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2024-04-24T02:13:07 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2024-04-24T02:13:07 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1000/1500, train/hateful_memes/cross_entropy: 0.3228, train/hateful_memes/cross_entropy/avg: 0.4057, train/total_loss: 0.3228, train/total_loss/avg: 0.4057, max mem: 14369.0, experiment: run, epoch: 8, num_updates: 1000, iterations: 1000, max_updates: 1500, lr: 0.00003, ups: 0.82, time: 04m 05s 035ms, time_since_start: 22m 17s 372ms, eta: 10m 40s 156ms\n","\u001b[32m2024-04-24T02:13:07 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n","\u001b[32m2024-04-24T02:13:07 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2024-04-24T02:13:07 | py.warnings: \u001b[0m/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n","  self.pid = os.fork()\n","\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2024-04-24T02:13:07 | py.warnings: \u001b[0m/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n","  self.pid = os.fork()\n","\n","\u001b[32m2024-04-24T02:13:11 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished evaluation inference. Loaded 9\n","\u001b[32m2024-04-24T02:13:11 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n","\u001b[32m2024-04-24T02:13:11 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2024-04-24T02:13:14 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2024-04-24T02:13:19 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2024-04-24T02:13:19 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1000/1500, val/hateful_memes/cross_entropy: 1.4600, val/total_loss: 1.4600, val/hateful_memes/accuracy: 0.6296, val/hateful_memes/binary_f1: 0.3464, val/hateful_memes/roc_auc: 0.6135, num_updates: 1000, epoch: 8, iterations: 1000, max_updates: 1500, val_time: 11s 813ms, best_update: 800, best_iteration: 800, best_val/hateful_memes/roc_auc: 0.620000\n","\u001b[32m2024-04-24T02:17:17 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n","\u001b[32m2024-04-24T02:17:17 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2024-04-24T02:17:19 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2024-04-24T02:17:29 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2024-04-24T02:17:29 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1200/1500, train/hateful_memes/cross_entropy: 0.2760, train/hateful_memes/cross_entropy/avg: 0.3472, train/total_loss: 0.2760, train/total_loss/avg: 0.3472, max mem: 14369.0, experiment: run, epoch: 9, num_updates: 1200, iterations: 1200, max_updates: 1500, lr: 0.00003, ups: 0.80, time: 04m 10s 673ms, time_since_start: 26m 39s 861ms, eta: 06m 32s 929ms\n","\u001b[32m2024-04-24T02:17:29 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n","\u001b[32m2024-04-24T02:17:29 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2024-04-24T02:17:29 | py.warnings: \u001b[0m/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n","  self.pid = os.fork()\n","\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2024-04-24T02:17:29 | py.warnings: \u001b[0m/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n","  self.pid = os.fork()\n","\n","\u001b[32m2024-04-24T02:17:34 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished evaluation inference. Loaded 9\n","\u001b[32m2024-04-24T02:17:34 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n","\u001b[32m2024-04-24T02:17:34 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2024-04-24T02:17:38 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2024-04-24T02:17:42 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2024-04-24T02:17:42 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1200/1500, val/hateful_memes/cross_entropy: 1.4578, val/total_loss: 1.4578, val/hateful_memes/accuracy: 0.6333, val/hateful_memes/binary_f1: 0.3400, val/hateful_memes/roc_auc: 0.6188, num_updates: 1200, epoch: 9, iterations: 1200, max_updates: 1500, val_time: 13s 095ms, best_update: 800, best_iteration: 800, best_val/hateful_memes/roc_auc: 0.620000\n","\u001b[32m2024-04-24T02:21:40 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n","\u001b[32m2024-04-24T02:21:40 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2024-04-24T02:21:42 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2024-04-24T02:21:53 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2024-04-24T02:21:53 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1400/1500, train/hateful_memes/cross_entropy: 0.2760, train/hateful_memes/cross_entropy/avg: 0.3049, train/total_loss: 0.2760, train/total_loss/avg: 0.3049, max mem: 14369.0, experiment: run, epoch: 11, num_updates: 1400, iterations: 1400, max_updates: 1500, lr: 0.00003, ups: 0.80, time: 04m 10s 086ms, time_since_start: 31m 03s 045ms, eta: 02m 10s 670ms\n","\u001b[32m2024-04-24T02:21:53 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n","\u001b[32m2024-04-24T02:21:53 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2024-04-24T02:21:53 | py.warnings: \u001b[0m/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n","  self.pid = os.fork()\n","\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2024-04-24T02:21:53 | py.warnings: \u001b[0m/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n","  self.pid = os.fork()\n","\n","\u001b[32m2024-04-24T02:21:57 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished evaluation inference. Loaded 9\n","\u001b[32m2024-04-24T02:21:57 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n","\u001b[32m2024-04-24T02:21:57 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2024-04-24T02:22:01 | mmf.utils.checkpoint: \u001b[0mSaving best checkpoint\n","\u001b[32m2024-04-24T02:22:07 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2024-04-24T02:22:10 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2024-04-24T02:22:10 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1400/1500, val/hateful_memes/cross_entropy: 1.5724, val/total_loss: 1.5724, val/hateful_memes/accuracy: 0.6241, val/hateful_memes/binary_f1: 0.4150, val/hateful_memes/roc_auc: 0.6276, num_updates: 1400, epoch: 11, iterations: 1400, max_updates: 1500, val_time: 16s 966ms, best_update: 1400, best_iteration: 1400, best_val/hateful_memes/roc_auc: 0.627559\n","\u001b[32m2024-04-24T02:24:08 | mmf.trainers.core.training_loop: \u001b[0mStepping into final validation check\n","\u001b[32m2024-04-24T02:24:08 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2024-04-24T02:24:08 | py.warnings: \u001b[0m/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n","  self.pid = os.fork()\n","\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2024-04-24T02:24:08 | py.warnings: \u001b[0m/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n","  self.pid = os.fork()\n","\n","\u001b[32m2024-04-24T02:24:12 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished evaluation inference. Loaded 9\n","\u001b[32m2024-04-24T02:24:12 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n","\u001b[32m2024-04-24T02:24:13 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1500/1500, val/hateful_memes/cross_entropy: 1.7149, val/total_loss: 1.7149, val/hateful_memes/accuracy: 0.6426, val/hateful_memes/binary_f1: 0.4683, val/hateful_memes/roc_auc: 0.6495, num_updates: 1500, epoch: 12, iterations: 1500, max_updates: 1500, val_time: 02m 20s 114ms, best_update: 1400, best_iteration: 1400, best_val/hateful_memes/roc_auc: 0.627559\n","\u001b[32m2024-04-24T02:24:13 | mmf.utils.checkpoint: \u001b[0mRestoring checkpoint\n","\u001b[32m2024-04-24T02:24:13 | mmf.utils.checkpoint: \u001b[0mLoading checkpoint\n","\u001b[32m2024-04-24T02:24:13 | mmf.utils.checkpoint: \u001b[0mCheckpoint loaded.\n","\u001b[32m2024-04-24T02:24:13 | mmf.utils.checkpoint: \u001b[0mCurrent num updates: 1400\n","\u001b[32m2024-04-24T02:24:13 | mmf.utils.checkpoint: \u001b[0mCurrent iteration: 1400\n","\u001b[32m2024-04-24T02:24:13 | mmf.utils.checkpoint: \u001b[0mCurrent epoch: 11\n","\u001b[32m2024-04-24T02:24:14 | mmf.trainers.mmf_trainer: \u001b[0mStarting inference on val set\n","\u001b[32m2024-04-24T02:24:14 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n","  0% 0/9 [00:00<?, ?it/s]\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2024-04-24T02:24:14 | py.warnings: \u001b[0m/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n","  self.pid = os.fork()\n","\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2024-04-24T02:24:14 | py.warnings: \u001b[0m/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n","  self.pid = os.fork()\n","\n","100% 9/9 [00:04<00:00,  2.13it/s]\n","\u001b[32m2024-04-24T02:24:18 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished evaluation inference. Loaded 9\n","\u001b[32m2024-04-24T02:24:18 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n","\u001b[32m2024-04-24T02:24:19 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1400/1500, val/hateful_memes/cross_entropy: 1.5724, val/total_loss: 1.5724, val/hateful_memes/accuracy: 0.6241, val/hateful_memes/binary_f1: 0.4150, val/hateful_memes/roc_auc: 0.6276\n","\u001b[32m2024-04-24T02:24:19 | mmf.trainers.callbacks.logistics: \u001b[0mFinished run in 33m 29s 048ms\n"]}]},{"cell_type":"markdown","source":["### Dataset: Augmented with partial transformations"],"metadata":{"id":"1go1E_lSvs9A"}},{"cell_type":"code","source":["!mmf_run config=\"./configs/defaults.yaml\" \\\n","  model=visual_bert \\\n","  dataset=hateful_memes \\\n","  run_type=train_val \\\n","  training.log_interval=200 \\\n","  training.batch_size=64 \\\n","  training.evaluation_interval=200 \\\n","  training.tensorboard=True \\\n","  training.checkpoint_interval=200 \\\n","  checkpoint.resume_pretrained=True \\\n","  checkpoint.resume_file=\"/content/drive/MyDrive/hateful-memes/Deep Learning Group Project/visualBERT_vizwiz_04212024_pretrained.ckpt\" \\\n","  dataset_config.hateful_memes.annotations.train[0]=\"hateful_memes/defaults/annotations/train_dev_transform_half.jsonl\" \\\n","  dataset_config.hateful_memes.annotations.val[0]=\"hateful_memes/defaults/annotations/dev_unseen.jsonl\" \\\n","  dataset_config.hateful_memes.annotations.test[0]=\"hateful_memes/defaults/annotations/test_unseen.jsonl\""],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kLDc6ucnvsQ1","executionInfo":{"status":"ok","timestamp":1713927479490,"user_tz":420,"elapsed":922934,"user":{"displayName":"Aneri Shah","userId":"14009776940163552827"}},"outputId":"c6fb679d-8df8-4205-c415-5c385b5bb0a8"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["2024-04-24 02:24:27.641435: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","2024-04-24 02:24:27.641493: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","2024-04-24 02:24:27.642945: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","2024-04-24 02:24:28.788093: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","2024-04-24 02:24:34.490252: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:47] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n","/usr/local/lib/python3.10/dist-packages/transformers/deepspeed.py:23: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/omegaconf/grammar_visitor.py:253: UserWarning: In the sequence `MMF_USER_DIR,` some elements are missing: please replace them with empty quoted strings. See https://github.com/omry/omegaconf/issues/572 for details.\n","  warnings.warn(\n","\u001b[32m2024-04-24T02:24:39 | mmf.utils.configuration: \u001b[0mOverriding option config to ./configs/defaults.yaml\n","\u001b[32m2024-04-24T02:24:39 | mmf.utils.configuration: \u001b[0mOverriding option model to visual_bert\n","\u001b[32m2024-04-24T02:24:39 | mmf.utils.configuration: \u001b[0mOverriding option datasets to hateful_memes\n","\u001b[32m2024-04-24T02:24:39 | mmf.utils.configuration: \u001b[0mOverriding option run_type to train_val\n","\u001b[32m2024-04-24T02:24:39 | mmf.utils.configuration: \u001b[0mOverriding option training.log_interval to 200\n","\u001b[32m2024-04-24T02:24:39 | mmf.utils.configuration: \u001b[0mOverriding option training.batch_size to 64\n","\u001b[32m2024-04-24T02:24:39 | mmf.utils.configuration: \u001b[0mOverriding option training.evaluation_interval to 200\n","\u001b[32m2024-04-24T02:24:39 | mmf.utils.configuration: \u001b[0mOverriding option training.tensorboard to True\n","\u001b[32m2024-04-24T02:24:39 | mmf.utils.configuration: \u001b[0mOverriding option training.checkpoint_interval to 200\n","\u001b[32m2024-04-24T02:24:39 | mmf.utils.configuration: \u001b[0mOverriding option checkpoint.resume_pretrained to True\n","\u001b[32m2024-04-24T02:24:39 | mmf.utils.configuration: \u001b[0mOverriding option checkpoint.resume_file to /content/drive/MyDrive/hateful-memes/Deep Learning Group Project/visualBERT_vizwiz_04212024_pretrained.ckpt\n","\u001b[32m2024-04-24T02:24:39 | mmf.utils.configuration: \u001b[0mOverriding option dataset_config.hateful_memes.annotations.train[0] to hateful_memes/defaults/annotations/train_dev_transform_half.jsonl\n","\u001b[32m2024-04-24T02:24:39 | mmf.utils.configuration: \u001b[0mOverriding option dataset_config.hateful_memes.annotations.val[0] to hateful_memes/defaults/annotations/dev_unseen.jsonl\n","\u001b[32m2024-04-24T02:24:39 | mmf.utils.configuration: \u001b[0mOverriding option dataset_config.hateful_memes.annotations.test[0] to hateful_memes/defaults/annotations/test_unseen.jsonl\n","/usr/local/lib/python3.10/dist-packages/omegaconf/grammar_visitor.py:253: UserWarning: In the sequence `MMF_LOG_DIR,` some elements are missing: please replace them with empty quoted strings. See https://github.com/omry/omegaconf/issues/572 for details.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/omegaconf/grammar_visitor.py:253: UserWarning: In the sequence `MMF_REPORT_DIR,` some elements are missing: please replace them with empty quoted strings. See https://github.com/omry/omegaconf/issues/572 for details.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/omegaconf/grammar_visitor.py:253: UserWarning: In the sequence `MMF_TENSORBOARD_LOGDIR,` some elements are missing: please replace them with empty quoted strings. See https://github.com/omry/omegaconf/issues/572 for details.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/omegaconf/grammar_visitor.py:253: UserWarning: In the sequence `MMF_WANDB_LOGDIR,` some elements are missing: please replace them with empty quoted strings. See https://github.com/omry/omegaconf/issues/572 for details.\n","  warnings.warn(\n","\u001b[32m2024-04-24T02:24:39 | mmf: \u001b[0mLogging to: ./save/train.log\n","\u001b[32m2024-04-24T02:24:39 | mmf_cli.run: \u001b[0mNamespace(config_override=None, local_rank=None, opts=['config=./configs/defaults.yaml', 'model=visual_bert', 'dataset=hateful_memes', 'run_type=train_val', 'training.log_interval=200', 'training.batch_size=64', 'training.evaluation_interval=200', 'training.tensorboard=True', 'training.checkpoint_interval=200', 'checkpoint.resume_pretrained=True', 'checkpoint.resume_file=/content/drive/MyDrive/hateful-memes/Deep Learning Group Project/visualBERT_vizwiz_04212024_pretrained.ckpt', 'dataset_config.hateful_memes.annotations.train[0]=hateful_memes/defaults/annotations/train_dev_transform_half.jsonl', 'dataset_config.hateful_memes.annotations.val[0]=hateful_memes/defaults/annotations/dev_unseen.jsonl', 'dataset_config.hateful_memes.annotations.test[0]=hateful_memes/defaults/annotations/test_unseen.jsonl'])\n","\u001b[32m2024-04-24T02:24:39 | mmf_cli.run: \u001b[0mTorch version: 2.2.0+cu121\n","\u001b[32m2024-04-24T02:24:39 | mmf.utils.general: \u001b[0mCUDA Device 0 is: NVIDIA L4\n","\u001b[32m2024-04-24T02:24:39 | mmf_cli.run: \u001b[0mUsing seed 39723712\n","\u001b[32m2024-04-24T02:24:39 | mmf.trainers.mmf_trainer: \u001b[0mLoading datasets\n","loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--bert-base-uncased/snapshots/86b5e0934494bd15c9632b12f734a8a67f723594/config.json\n","Model config BertConfig {\n","  \"_name_or_path\": \"bert-base-uncased\",\n","  \"architectures\": [\n","    \"BertForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.39.2\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n","loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--bert-base-uncased/snapshots/86b5e0934494bd15c9632b12f734a8a67f723594/vocab.txt\n","loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--bert-base-uncased/snapshots/86b5e0934494bd15c9632b12f734a8a67f723594/tokenizer.json\n","loading file added_tokens.json from cache at None\n","loading file special_tokens_map.json from cache at None\n","loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--bert-base-uncased/snapshots/86b5e0934494bd15c9632b12f734a8a67f723594/tokenizer_config.json\n","loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--bert-base-uncased/snapshots/86b5e0934494bd15c9632b12f734a8a67f723594/config.json\n","Model config BertConfig {\n","  \"_name_or_path\": \"bert-base-uncased\",\n","  \"architectures\": [\n","    \"BertForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.39.2\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n","\u001b[32m2024-04-24T02:24:40 | mmf.datasets.multi_datamodule: \u001b[0mMultitasking disabled by default for single dataset training\n","\u001b[32m2024-04-24T02:24:40 | mmf.datasets.multi_datamodule: \u001b[0mMultitasking disabled by default for single dataset training\n","\u001b[32m2024-04-24T02:24:40 | mmf.datasets.multi_datamodule: \u001b[0mMultitasking disabled by default for single dataset training\n","\u001b[32m2024-04-24T02:24:40 | mmf.trainers.mmf_trainer: \u001b[0mLoading model\n","Model config BertConfig {\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bert_model_name\": \"bert-base-uncased\",\n","  \"bypass_transformer\": false,\n","  \"classifier_dropout\": null,\n","  \"embedding_strategy\": \"plain\",\n","  \"finetune_lr_multiplier\": 1,\n","  \"freeze_base\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"losses\": [\n","    \"cross_entropy\"\n","  ],\n","  \"max_position_embeddings\": 512,\n","  \"model\": \"visual_bert\",\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"pooler_strategy\": \"default\",\n","  \"position_embedding_type\": \"absolute\",\n","  \"random_initialize\": false,\n","  \"special_visual_initialize\": true,\n","  \"training_head_type\": \"classification\",\n","  \"transformers_version\": \"4.39.2\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"visual_embedding_dim\": 2048,\n","  \"vocab_size\": 30522,\n","  \"zerobias\": false\n","}\n","\n","loading weights file model.safetensors from cache at /root/.cache/torch/mmf/distributed_-1/models--bert-base-uncased/snapshots/86b5e0934494bd15c9632b12f734a8a67f723594/model.safetensors\n","Some weights of the model checkpoint at bert-base-uncased were not used when initializing VisualBERTBase: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n","- This IS expected if you are initializing VisualBERTBase from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing VisualBERTBase from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of VisualBERTBase were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['bert.embeddings.position_embeddings_visual.weight', 'bert.embeddings.projection.bias', 'bert.embeddings.projection.weight', 'bert.embeddings.token_type_embeddings_visual.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","\u001b[32m2024-04-24T02:24:43 | mmf.trainers.mmf_trainer: \u001b[0mLoading optimizer\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2024-04-24T02:24:43 | py.warnings: \u001b[0m/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:457: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n","\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2024-04-24T02:24:43 | py.warnings: \u001b[0m/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:457: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n","\n","\u001b[32m2024-04-24T02:24:43 | mmf.trainers.mmf_trainer: \u001b[0mLoading metrics\n","\u001b[32m2024-04-24T02:24:43 | mmf.utils.checkpoint: \u001b[0mLoading checkpoint\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2024-04-24T02:24:48 | mmf: \u001b[0mKey data_parallel is not present in registry, returning default value of None\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2024-04-24T02:24:48 | mmf: \u001b[0mKey distributed is not present in registry, returning default value of None\n","\u001b[32m2024-04-24T02:24:48 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.word_embeddings.weight from model.bert.embeddings.word_embeddings.weight\n","\u001b[32m2024-04-24T02:24:48 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.position_embeddings.weight from model.bert.embeddings.position_embeddings.weight\n","\u001b[32m2024-04-24T02:24:48 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.token_type_embeddings.weight from model.bert.embeddings.token_type_embeddings.weight\n","\u001b[32m2024-04-24T02:24:48 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.LayerNorm.weight from model.bert.embeddings.LayerNorm.weight\n","\u001b[32m2024-04-24T02:24:48 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.LayerNorm.bias from model.bert.embeddings.LayerNorm.bias\n","\u001b[32m2024-04-24T02:24:48 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.token_type_embeddings_visual.weight from model.bert.embeddings.token_type_embeddings_visual.weight\n","\u001b[32m2024-04-24T02:24:48 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.position_embeddings_visual.weight from model.bert.embeddings.position_embeddings_visual.weight\n","\u001b[32m2024-04-24T02:24:48 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.projection.weight from model.bert.embeddings.projection.weight\n","\u001b[32m2024-04-24T02:24:48 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.projection.bias from model.bert.embeddings.projection.bias\n","\u001b[32m2024-04-24T02:24:48 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.self.query.weight from model.bert.encoder.layer.0.attention.self.query.weight\n","\u001b[32m2024-04-24T02:24:48 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.self.query.bias from model.bert.encoder.layer.0.attention.self.query.bias\n","\u001b[32m2024-04-24T02:24:48 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.self.key.weight from model.bert.encoder.layer.0.attention.self.key.weight\n","\u001b[32m2024-04-24T02:24:48 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.self.key.bias from model.bert.encoder.layer.0.attention.self.key.bias\n","\u001b[32m2024-04-24T02:24:48 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.self.value.weight from model.bert.encoder.layer.0.attention.self.value.weight\n","\u001b[32m2024-04-24T02:24:48 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.self.value.bias from model.bert.encoder.layer.0.attention.self.value.bias\n","\u001b[32m2024-04-24T02:24:48 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.output.dense.weight from model.bert.encoder.layer.0.attention.output.dense.weight\n","\u001b[32m2024-04-24T02:24:48 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.output.dense.bias from model.bert.encoder.layer.0.attention.output.dense.bias\n","\u001b[32m2024-04-24T02:24:48 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.output.LayerNorm.weight from model.bert.encoder.layer.0.attention.output.LayerNorm.weight\n","\u001b[32m2024-04-24T02:24:48 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.output.LayerNorm.bias from model.bert.encoder.layer.0.attention.output.LayerNorm.bias\n","\u001b[32m2024-04-24T02:24:48 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.intermediate.dense.weight from model.bert.encoder.layer.0.intermediate.dense.weight\n","\u001b[32m2024-04-24T02:24:48 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.intermediate.dense.bias from model.bert.encoder.layer.0.intermediate.dense.bias\n","\u001b[32m2024-04-24T02:24:48 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.output.dense.weight from model.bert.encoder.layer.0.output.dense.weight\n","\u001b[32m2024-04-24T02:24:48 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.output.dense.bias from model.bert.encoder.layer.0.output.dense.bias\n","\u001b[32m2024-04-24T02:24:48 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.output.LayerNorm.weight from model.bert.encoder.layer.0.output.LayerNorm.weight\n","\u001b[32m2024-04-24T02:24:48 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.output.LayerNorm.bias from model.bert.encoder.layer.0.output.LayerNorm.bias\n","\u001b[32m2024-04-24T02:24:48 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.self.query.weight from model.bert.encoder.layer.1.attention.self.query.weight\n","\u001b[32m2024-04-24T02:24:48 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.self.query.bias from model.bert.encoder.layer.1.attention.self.query.bias\n","\u001b[32m2024-04-24T02:24:48 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.self.key.weight from model.bert.encoder.layer.1.attention.self.key.weight\n","\u001b[32m2024-04-24T02:24:48 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.self.key.bias from model.bert.encoder.layer.1.attention.self.key.bias\n","\u001b[32m2024-04-24T02:24:48 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.self.value.weight from model.bert.encoder.layer.1.attention.self.value.weight\n","\u001b[32m2024-04-24T02:24:48 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.self.value.bias from model.bert.encoder.layer.1.attention.self.value.bias\n","\u001b[32m2024-04-24T02:24:48 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.output.dense.weight from model.bert.encoder.layer.1.attention.output.dense.weight\n","\u001b[32m2024-04-24T02:24:48 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.output.dense.bias from model.bert.encoder.layer.1.attention.output.dense.bias\n","\u001b[32m2024-04-24T02:24:48 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.output.LayerNorm.weight from model.bert.encoder.layer.1.attention.output.LayerNorm.weight\n","\u001b[32m2024-04-24T02:24:48 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.output.LayerNorm.bias from model.bert.encoder.layer.1.attention.output.LayerNorm.bias\n","\u001b[32m2024-04-24T02:24:48 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.intermediate.dense.weight from model.bert.encoder.layer.1.intermediate.dense.weight\n","\u001b[32m2024-04-24T02:24:48 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.intermediate.dense.bias from model.bert.encoder.layer.1.intermediate.dense.bias\n","\u001b[32m2024-04-24T02:24:48 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.output.dense.weight from model.bert.encoder.layer.1.output.dense.weight\n","\u001b[32m2024-04-24T02:24:48 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.output.dense.bias from model.bert.encoder.layer.1.output.dense.bias\n","\u001b[32m2024-04-24T02:24:48 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.output.LayerNorm.weight from model.bert.encoder.layer.1.output.LayerNorm.weight\n","\u001b[32m2024-04-24T02:24:48 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.output.LayerNorm.bias from model.bert.encoder.layer.1.output.LayerNorm.bias\n","\u001b[32m2024-04-24T02:24:48 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.self.query.weight from model.bert.encoder.layer.2.attention.self.query.weight\n","\u001b[32m2024-04-24T02:24:48 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.self.query.bias from model.bert.encoder.layer.2.attention.self.query.bias\n","\u001b[32m2024-04-24T02:24:48 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.self.key.weight from model.bert.encoder.layer.2.attention.self.key.weight\n","\u001b[32m2024-04-24T02:24:48 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.self.key.bias from model.bert.encoder.layer.2.attention.self.key.bias\n","\u001b[32m2024-04-24T02:24:48 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.self.value.weight from model.bert.encoder.layer.2.attention.self.value.weight\n","\u001b[32m2024-04-24T02:24:48 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.self.value.bias from model.bert.encoder.layer.2.attention.self.value.bias\n","\u001b[32m2024-04-24T02:24:48 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.output.dense.weight from model.bert.encoder.layer.2.attention.output.dense.weight\n","\u001b[32m2024-04-24T02:24:48 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.output.dense.bias from model.bert.encoder.layer.2.attention.output.dense.bias\n","\u001b[32m2024-04-24T02:24:48 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.output.LayerNorm.weight from model.bert.encoder.layer.2.attention.output.LayerNorm.weight\n","\u001b[32m2024-04-24T02:24:48 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.output.LayerNorm.bias from model.bert.encoder.layer.2.attention.output.LayerNorm.bias\n","\u001b[32m2024-04-24T02:24:48 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.intermediate.dense.weight from model.bert.encoder.layer.2.intermediate.dense.weight\n","\u001b[32m2024-04-24T02:24:48 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.intermediate.dense.bias from model.bert.encoder.layer.2.intermediate.dense.bias\n","\u001b[32m2024-04-24T02:24:48 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.output.dense.weight from model.bert.encoder.layer.2.output.dense.weight\n","\u001b[32m2024-04-24T02:24:48 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.output.dense.bias from model.bert.encoder.layer.2.output.dense.bias\n","\u001b[32m2024-04-24T02:24:48 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.output.LayerNorm.weight from model.bert.encoder.layer.2.output.LayerNorm.weight\n","\u001b[32m2024-04-24T02:24:48 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.output.LayerNorm.bias from model.bert.encoder.layer.2.output.LayerNorm.bias\n","\u001b[32m2024-04-24T02:24:48 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.self.query.weight from model.bert.encoder.layer.3.attention.self.query.weight\n","\u001b[32m2024-04-24T02:24:48 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.self.query.bias from model.bert.encoder.layer.3.attention.self.query.bias\n","\u001b[32m2024-04-24T02:24:48 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.self.key.weight from model.bert.encoder.layer.3.attention.self.key.weight\n","\u001b[32m2024-04-24T02:24:48 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.self.key.bias from model.bert.encoder.layer.3.attention.self.key.bias\n","\u001b[32m2024-04-24T02:24:48 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.self.value.weight from model.bert.encoder.layer.3.attention.self.value.weight\n","\u001b[32m2024-04-24T02:24:48 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.self.value.bias from model.bert.encoder.layer.3.attention.self.value.bias\n","\u001b[32m2024-04-24T02:24:48 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.output.dense.weight from model.bert.encoder.layer.3.attention.output.dense.weight\n","\u001b[32m2024-04-24T02:24:48 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.output.dense.bias from model.bert.encoder.layer.3.attention.output.dense.bias\n","\u001b[32m2024-04-24T02:24:48 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.output.LayerNorm.weight from model.bert.encoder.layer.3.attention.output.LayerNorm.weight\n","\u001b[32m2024-04-24T02:24:48 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.output.LayerNorm.bias from model.bert.encoder.layer.3.attention.output.LayerNorm.bias\n","\u001b[32m2024-04-24T02:24:48 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.intermediate.dense.weight from model.bert.encoder.layer.3.intermediate.dense.weight\n","\u001b[32m2024-04-24T02:24:48 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.intermediate.dense.bias from model.bert.encoder.layer.3.intermediate.dense.bias\n","\u001b[32m2024-04-24T02:24:48 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.output.dense.weight from model.bert.encoder.layer.3.output.dense.weight\n","\u001b[32m2024-04-24T02:24:48 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.output.dense.bias from model.bert.encoder.layer.3.output.dense.bias\n","\u001b[32m2024-04-24T02:24:48 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.output.LayerNorm.weight from model.bert.encoder.layer.3.output.LayerNorm.weight\n","\u001b[32m2024-04-24T02:24:48 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.output.LayerNorm.bias from model.bert.encoder.layer.3.output.LayerNorm.bias\n","\u001b[32m2024-04-24T02:24:48 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.self.query.weight from model.bert.encoder.layer.4.attention.self.query.weight\n","\u001b[32m2024-04-24T02:24:48 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.self.query.bias from model.bert.encoder.layer.4.attention.self.query.bias\n","\u001b[32m2024-04-24T02:24:48 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.self.key.weight from model.bert.encoder.layer.4.attention.self.key.weight\n","\u001b[32m2024-04-24T02:24:48 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.self.key.bias from model.bert.encoder.layer.4.attention.self.key.bias\n","\u001b[32m2024-04-24T02:24:48 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.self.value.weight from model.bert.encoder.layer.4.attention.self.value.weight\n","\u001b[32m2024-04-24T02:24:48 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.self.value.bias from model.bert.encoder.layer.4.attention.self.value.bias\n","\u001b[32m2024-04-24T02:24:48 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.output.dense.weight from model.bert.encoder.layer.4.attention.output.dense.weight\n","\u001b[32m2024-04-24T02:24:48 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.output.dense.bias from model.bert.encoder.layer.4.attention.output.dense.bias\n","\u001b[32m2024-04-24T02:24:48 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.output.LayerNorm.weight from model.bert.encoder.layer.4.attention.output.LayerNorm.weight\n","\u001b[32m2024-04-24T02:24:48 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.output.LayerNorm.bias from model.bert.encoder.layer.4.attention.output.LayerNorm.bias\n","\u001b[32m2024-04-24T02:24:48 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.intermediate.dense.weight from model.bert.encoder.layer.4.intermediate.dense.weight\n","\u001b[32m2024-04-24T02:24:48 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.intermediate.dense.bias from model.bert.encoder.layer.4.intermediate.dense.bias\n","\u001b[32m2024-04-24T02:24:48 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.output.dense.weight from model.bert.encoder.layer.4.output.dense.weight\n","\u001b[32m2024-04-24T02:24:48 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.output.dense.bias from model.bert.encoder.layer.4.output.dense.bias\n","\u001b[32m2024-04-24T02:24:48 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.output.LayerNorm.weight from model.bert.encoder.layer.4.output.LayerNorm.weight\n","\u001b[32m2024-04-24T02:24:48 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.output.LayerNorm.bias from model.bert.encoder.layer.4.output.LayerNorm.bias\n","\u001b[32m2024-04-24T02:24:48 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.self.query.weight from model.bert.encoder.layer.5.attention.self.query.weight\n","\u001b[32m2024-04-24T02:24:48 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.self.query.bias from model.bert.encoder.layer.5.attention.self.query.bias\n","\u001b[32m2024-04-24T02:24:48 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.self.key.weight from model.bert.encoder.layer.5.attention.self.key.weight\n","\u001b[32m2024-04-24T02:24:48 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.self.key.bias from model.bert.encoder.layer.5.attention.self.key.bias\n","\u001b[32m2024-04-24T02:24:48 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.self.value.weight from model.bert.encoder.layer.5.attention.self.value.weight\n","\u001b[32m2024-04-24T02:24:48 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.self.value.bias from model.bert.encoder.layer.5.attention.self.value.bias\n","\u001b[32m2024-04-24T02:24:48 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.output.dense.weight from model.bert.encoder.layer.5.attention.output.dense.weight\n","\u001b[32m2024-04-24T02:24:48 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.output.dense.bias from model.bert.encoder.layer.5.attention.output.dense.bias\n","\u001b[32m2024-04-24T02:24:48 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.output.LayerNorm.weight from model.bert.encoder.layer.5.attention.output.LayerNorm.weight\n","\u001b[32m2024-04-24T02:24:48 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.output.LayerNorm.bias from model.bert.encoder.layer.5.attention.output.LayerNorm.bias\n","\u001b[32m2024-04-24T02:24:48 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.intermediate.dense.weight from model.bert.encoder.layer.5.intermediate.dense.weight\n","\u001b[32m2024-04-24T02:24:48 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.intermediate.dense.bias from model.bert.encoder.layer.5.intermediate.dense.bias\n","\u001b[32m2024-04-24T02:24:48 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.output.dense.weight from model.bert.encoder.layer.5.output.dense.weight\n","\u001b[32m2024-04-24T02:24:48 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.output.dense.bias from model.bert.encoder.layer.5.output.dense.bias\n","\u001b[32m2024-04-24T02:24:48 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.output.LayerNorm.weight from model.bert.encoder.layer.5.output.LayerNorm.weight\n","\u001b[32m2024-04-24T02:24:48 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.output.LayerNorm.bias from model.bert.encoder.layer.5.output.LayerNorm.bias\n","\u001b[32m2024-04-24T02:24:48 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.self.query.weight from model.bert.encoder.layer.6.attention.self.query.weight\n","\u001b[32m2024-04-24T02:24:48 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.self.query.bias from model.bert.encoder.layer.6.attention.self.query.bias\n","\u001b[32m2024-04-24T02:24:48 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.self.key.weight from model.bert.encoder.layer.6.attention.self.key.weight\n","\u001b[32m2024-04-24T02:24:48 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.self.key.bias from model.bert.encoder.layer.6.attention.self.key.bias\n","\u001b[32m2024-04-24T02:24:48 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.self.value.weight from model.bert.encoder.layer.6.attention.self.value.weight\n","\u001b[32m2024-04-24T02:24:48 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.self.value.bias from model.bert.encoder.layer.6.attention.self.value.bias\n","\u001b[32m2024-04-24T02:24:48 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.output.dense.weight from model.bert.encoder.layer.6.attention.output.dense.weight\n","\u001b[32m2024-04-24T02:24:48 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.output.dense.bias from model.bert.encoder.layer.6.attention.output.dense.bias\n","\u001b[32m2024-04-24T02:24:48 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.output.LayerNorm.weight from model.bert.encoder.layer.6.attention.output.LayerNorm.weight\n","\u001b[32m2024-04-24T02:24:48 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.output.LayerNorm.bias from model.bert.encoder.layer.6.attention.output.LayerNorm.bias\n","\u001b[32m2024-04-24T02:24:48 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.intermediate.dense.weight from model.bert.encoder.layer.6.intermediate.dense.weight\n","\u001b[32m2024-04-24T02:24:48 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.intermediate.dense.bias from model.bert.encoder.layer.6.intermediate.dense.bias\n","\u001b[32m2024-04-24T02:24:48 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.output.dense.weight from model.bert.encoder.layer.6.output.dense.weight\n","\u001b[32m2024-04-24T02:24:48 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.output.dense.bias from model.bert.encoder.layer.6.output.dense.bias\n","\u001b[32m2024-04-24T02:24:48 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.output.LayerNorm.weight from model.bert.encoder.layer.6.output.LayerNorm.weight\n","\u001b[32m2024-04-24T02:24:48 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.output.LayerNorm.bias from model.bert.encoder.layer.6.output.LayerNorm.bias\n","\u001b[32m2024-04-24T02:24:48 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.self.query.weight from model.bert.encoder.layer.7.attention.self.query.weight\n","\u001b[32m2024-04-24T02:24:48 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.self.query.bias from model.bert.encoder.layer.7.attention.self.query.bias\n","\u001b[32m2024-04-24T02:24:48 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.self.key.weight from model.bert.encoder.layer.7.attention.self.key.weight\n","\u001b[32m2024-04-24T02:24:48 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.self.key.bias from model.bert.encoder.layer.7.attention.self.key.bias\n","\u001b[32m2024-04-24T02:24:48 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.self.value.weight from model.bert.encoder.layer.7.attention.self.value.weight\n","\u001b[32m2024-04-24T02:24:48 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.self.value.bias from model.bert.encoder.layer.7.attention.self.value.bias\n","\u001b[32m2024-04-24T02:24:48 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.output.dense.weight from model.bert.encoder.layer.7.attention.output.dense.weight\n","\u001b[32m2024-04-24T02:24:48 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.output.dense.bias from model.bert.encoder.layer.7.attention.output.dense.bias\n","\u001b[32m2024-04-24T02:24:48 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.output.LayerNorm.weight from model.bert.encoder.layer.7.attention.output.LayerNorm.weight\n","\u001b[32m2024-04-24T02:24:48 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.output.LayerNorm.bias from model.bert.encoder.layer.7.attention.output.LayerNorm.bias\n","\u001b[32m2024-04-24T02:24:48 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.intermediate.dense.weight from model.bert.encoder.layer.7.intermediate.dense.weight\n","\u001b[32m2024-04-24T02:24:48 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.intermediate.dense.bias from model.bert.encoder.layer.7.intermediate.dense.bias\n","\u001b[32m2024-04-24T02:24:48 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.output.dense.weight from model.bert.encoder.layer.7.output.dense.weight\n","\u001b[32m2024-04-24T02:24:48 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.output.dense.bias from model.bert.encoder.layer.7.output.dense.bias\n","\u001b[32m2024-04-24T02:24:48 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.output.LayerNorm.weight from model.bert.encoder.layer.7.output.LayerNorm.weight\n","\u001b[32m2024-04-24T02:24:48 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.output.LayerNorm.bias from model.bert.encoder.layer.7.output.LayerNorm.bias\n","\u001b[32m2024-04-24T02:24:48 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.self.query.weight from model.bert.encoder.layer.8.attention.self.query.weight\n","\u001b[32m2024-04-24T02:24:48 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.self.query.bias from model.bert.encoder.layer.8.attention.self.query.bias\n","\u001b[32m2024-04-24T02:24:48 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.self.key.weight from model.bert.encoder.layer.8.attention.self.key.weight\n","\u001b[32m2024-04-24T02:24:48 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.self.key.bias from model.bert.encoder.layer.8.attention.self.key.bias\n","\u001b[32m2024-04-24T02:24:48 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.self.value.weight from model.bert.encoder.layer.8.attention.self.value.weight\n","\u001b[32m2024-04-24T02:24:48 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.self.value.bias from model.bert.encoder.layer.8.attention.self.value.bias\n","\u001b[32m2024-04-24T02:24:48 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.output.dense.weight from model.bert.encoder.layer.8.attention.output.dense.weight\n","\u001b[32m2024-04-24T02:24:48 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.output.dense.bias from model.bert.encoder.layer.8.attention.output.dense.bias\n","\u001b[32m2024-04-24T02:24:48 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.output.LayerNorm.weight from model.bert.encoder.layer.8.attention.output.LayerNorm.weight\n","\u001b[32m2024-04-24T02:24:48 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.output.LayerNorm.bias from model.bert.encoder.layer.8.attention.output.LayerNorm.bias\n","\u001b[32m2024-04-24T02:24:48 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.intermediate.dense.weight from model.bert.encoder.layer.8.intermediate.dense.weight\n","\u001b[32m2024-04-24T02:24:48 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.intermediate.dense.bias from model.bert.encoder.layer.8.intermediate.dense.bias\n","\u001b[32m2024-04-24T02:24:48 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.output.dense.weight from model.bert.encoder.layer.8.output.dense.weight\n","\u001b[32m2024-04-24T02:24:48 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.output.dense.bias from model.bert.encoder.layer.8.output.dense.bias\n","\u001b[32m2024-04-24T02:24:48 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.output.LayerNorm.weight from model.bert.encoder.layer.8.output.LayerNorm.weight\n","\u001b[32m2024-04-24T02:24:48 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.output.LayerNorm.bias from model.bert.encoder.layer.8.output.LayerNorm.bias\n","\u001b[32m2024-04-24T02:24:48 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.self.query.weight from model.bert.encoder.layer.9.attention.self.query.weight\n","\u001b[32m2024-04-24T02:24:48 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.self.query.bias from model.bert.encoder.layer.9.attention.self.query.bias\n","\u001b[32m2024-04-24T02:24:48 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.self.key.weight from model.bert.encoder.layer.9.attention.self.key.weight\n","\u001b[32m2024-04-24T02:24:48 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.self.key.bias from model.bert.encoder.layer.9.attention.self.key.bias\n","\u001b[32m2024-04-24T02:24:48 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.self.value.weight from model.bert.encoder.layer.9.attention.self.value.weight\n","\u001b[32m2024-04-24T02:24:48 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.self.value.bias from model.bert.encoder.layer.9.attention.self.value.bias\n","\u001b[32m2024-04-24T02:24:48 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.output.dense.weight from model.bert.encoder.layer.9.attention.output.dense.weight\n","\u001b[32m2024-04-24T02:24:48 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.output.dense.bias from model.bert.encoder.layer.9.attention.output.dense.bias\n","\u001b[32m2024-04-24T02:24:48 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.output.LayerNorm.weight from model.bert.encoder.layer.9.attention.output.LayerNorm.weight\n","\u001b[32m2024-04-24T02:24:48 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.output.LayerNorm.bias from model.bert.encoder.layer.9.attention.output.LayerNorm.bias\n","\u001b[32m2024-04-24T02:24:48 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.intermediate.dense.weight from model.bert.encoder.layer.9.intermediate.dense.weight\n","\u001b[32m2024-04-24T02:24:48 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.intermediate.dense.bias from model.bert.encoder.layer.9.intermediate.dense.bias\n","\u001b[32m2024-04-24T02:24:48 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.output.dense.weight from model.bert.encoder.layer.9.output.dense.weight\n","\u001b[32m2024-04-24T02:24:48 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.output.dense.bias from model.bert.encoder.layer.9.output.dense.bias\n","\u001b[32m2024-04-24T02:24:48 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.output.LayerNorm.weight from model.bert.encoder.layer.9.output.LayerNorm.weight\n","\u001b[32m2024-04-24T02:24:48 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.output.LayerNorm.bias from model.bert.encoder.layer.9.output.LayerNorm.bias\n","\u001b[32m2024-04-24T02:24:48 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.self.query.weight from model.bert.encoder.layer.10.attention.self.query.weight\n","\u001b[32m2024-04-24T02:24:48 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.self.query.bias from model.bert.encoder.layer.10.attention.self.query.bias\n","\u001b[32m2024-04-24T02:24:48 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.self.key.weight from model.bert.encoder.layer.10.attention.self.key.weight\n","\u001b[32m2024-04-24T02:24:48 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.self.key.bias from model.bert.encoder.layer.10.attention.self.key.bias\n","\u001b[32m2024-04-24T02:24:48 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.self.value.weight from model.bert.encoder.layer.10.attention.self.value.weight\n","\u001b[32m2024-04-24T02:24:48 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.self.value.bias from model.bert.encoder.layer.10.attention.self.value.bias\n","\u001b[32m2024-04-24T02:24:48 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.output.dense.weight from model.bert.encoder.layer.10.attention.output.dense.weight\n","\u001b[32m2024-04-24T02:24:48 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.output.dense.bias from model.bert.encoder.layer.10.attention.output.dense.bias\n","\u001b[32m2024-04-24T02:24:48 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.output.LayerNorm.weight from model.bert.encoder.layer.10.attention.output.LayerNorm.weight\n","\u001b[32m2024-04-24T02:24:48 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.output.LayerNorm.bias from model.bert.encoder.layer.10.attention.output.LayerNorm.bias\n","\u001b[32m2024-04-24T02:24:48 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.intermediate.dense.weight from model.bert.encoder.layer.10.intermediate.dense.weight\n","\u001b[32m2024-04-24T02:24:48 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.intermediate.dense.bias from model.bert.encoder.layer.10.intermediate.dense.bias\n","\u001b[32m2024-04-24T02:24:48 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.output.dense.weight from model.bert.encoder.layer.10.output.dense.weight\n","\u001b[32m2024-04-24T02:24:48 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.output.dense.bias from model.bert.encoder.layer.10.output.dense.bias\n","\u001b[32m2024-04-24T02:24:48 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.output.LayerNorm.weight from model.bert.encoder.layer.10.output.LayerNorm.weight\n","\u001b[32m2024-04-24T02:24:48 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.output.LayerNorm.bias from model.bert.encoder.layer.10.output.LayerNorm.bias\n","\u001b[32m2024-04-24T02:24:48 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.self.query.weight from model.bert.encoder.layer.11.attention.self.query.weight\n","\u001b[32m2024-04-24T02:24:48 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.self.query.bias from model.bert.encoder.layer.11.attention.self.query.bias\n","\u001b[32m2024-04-24T02:24:48 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.self.key.weight from model.bert.encoder.layer.11.attention.self.key.weight\n","\u001b[32m2024-04-24T02:24:48 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.self.key.bias from model.bert.encoder.layer.11.attention.self.key.bias\n","\u001b[32m2024-04-24T02:24:48 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.self.value.weight from model.bert.encoder.layer.11.attention.self.value.weight\n","\u001b[32m2024-04-24T02:24:48 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.self.value.bias from model.bert.encoder.layer.11.attention.self.value.bias\n","\u001b[32m2024-04-24T02:24:48 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.output.dense.weight from model.bert.encoder.layer.11.attention.output.dense.weight\n","\u001b[32m2024-04-24T02:24:48 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.output.dense.bias from model.bert.encoder.layer.11.attention.output.dense.bias\n","\u001b[32m2024-04-24T02:24:48 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.output.LayerNorm.weight from model.bert.encoder.layer.11.attention.output.LayerNorm.weight\n","\u001b[32m2024-04-24T02:24:48 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.output.LayerNorm.bias from model.bert.encoder.layer.11.attention.output.LayerNorm.bias\n","\u001b[32m2024-04-24T02:24:48 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.intermediate.dense.weight from model.bert.encoder.layer.11.intermediate.dense.weight\n","\u001b[32m2024-04-24T02:24:48 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.intermediate.dense.bias from model.bert.encoder.layer.11.intermediate.dense.bias\n","\u001b[32m2024-04-24T02:24:48 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.output.dense.weight from model.bert.encoder.layer.11.output.dense.weight\n","\u001b[32m2024-04-24T02:24:48 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.output.dense.bias from model.bert.encoder.layer.11.output.dense.bias\n","\u001b[32m2024-04-24T02:24:48 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.output.LayerNorm.weight from model.bert.encoder.layer.11.output.LayerNorm.weight\n","\u001b[32m2024-04-24T02:24:48 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.output.LayerNorm.bias from model.bert.encoder.layer.11.output.LayerNorm.bias\n","\u001b[32m2024-04-24T02:24:48 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.pooler.dense.weight from model.bert.pooler.dense.weight\n","\u001b[32m2024-04-24T02:24:48 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.pooler.dense.bias from model.bert.pooler.dense.bias\n","\u001b[32m2024-04-24T02:24:48 | mmf.utils.checkpoint: \u001b[0mPretrained model loaded\n","\u001b[32m2024-04-24T02:24:48 | mmf.utils.checkpoint: \u001b[0mCheckpoint loaded.\n","\u001b[32m2024-04-24T02:24:48 | mmf.utils.checkpoint: \u001b[0mCurrent num updates: 0\n","\u001b[32m2024-04-24T02:24:48 | mmf.utils.checkpoint: \u001b[0mCurrent iteration: 0\n","\u001b[32m2024-04-24T02:24:48 | mmf.utils.checkpoint: \u001b[0mCurrent epoch: 0\n","\u001b[32m2024-04-24T02:24:48 | mmf.trainers.mmf_trainer: \u001b[0m===== Model =====\n","\u001b[32m2024-04-24T02:24:48 | mmf.trainers.mmf_trainer: \u001b[0mVisualBERT(\n","  (model): VisualBERTForClassification(\n","    (bert): VisualBERTBase(\n","      (embeddings): BertVisioLinguisticEmbeddings(\n","        (word_embeddings): Embedding(30522, 768, padding_idx=0)\n","        (position_embeddings): Embedding(512, 768)\n","        (token_type_embeddings): Embedding(2, 768)\n","        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","        (token_type_embeddings_visual): Embedding(2, 768)\n","        (position_embeddings_visual): Embedding(512, 768)\n","        (projection): Linear(in_features=2048, out_features=768, bias=True)\n","      )\n","      (encoder): BertEncoderJit(\n","        (layer): ModuleList(\n","          (0-11): 12 x BertLayerJit(\n","            (attention): BertAttentionJit(\n","              (self): BertSelfAttentionJit(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","              (intermediate_act_fn): GELUActivation()\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","        )\n","      )\n","      (pooler): BertPooler(\n","        (dense): Linear(in_features=768, out_features=768, bias=True)\n","        (activation): Tanh()\n","      )\n","    )\n","    (dropout): Dropout(p=0.1, inplace=False)\n","    (classifier): Sequential(\n","      (0): BertPredictionHeadTransform(\n","        (dense): Linear(in_features=768, out_features=768, bias=True)\n","        (transform_act_fn): GELUActivation()\n","        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      )\n","      (1): Linear(in_features=768, out_features=2, bias=True)\n","    )\n","  )\n","  (losses): Losses(\n","    (losses): ModuleList(\n","      (0): MMFLoss(\n","        (loss_criterion): CrossEntropyLoss(\n","          (loss_fn): CrossEntropyLoss()\n","        )\n","      )\n","    )\n","  )\n",")\n","\u001b[32m2024-04-24T02:24:48 | mmf.utils.general: \u001b[0mTotal Parameters: 112044290. Trained Parameters: 112044290\n","\u001b[32m2024-04-24T02:24:48 | mmf.trainers.core.training_loop: \u001b[0mStarting training...\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2024-04-24T02:24:48 | py.warnings: \u001b[0m/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n","  self.pid = os.fork()\n","\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2024-04-24T02:24:48 | py.warnings: \u001b[0m/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n","  self.pid = os.fork()\n","\n","\u001b[32m2024-04-24T02:28:48 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n","\u001b[32m2024-04-24T02:28:48 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2024-04-24T02:28:51 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2024-04-24T02:28:55 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2024-04-24T02:28:56 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 200/1500, train/hateful_memes/cross_entropy: 0.6093, train/hateful_memes/cross_entropy/avg: 0.6093, train/total_loss: 0.6093, train/total_loss/avg: 0.6093, max mem: 14320.0, experiment: run, epoch: 1, num_updates: 200, iterations: 200, max_updates: 1500, lr: 0.00001, ups: 0.81, time: 04m 07s 202ms, time_since_start: 04m 12s 609ms, eta: 27m 59s 124ms\n","\u001b[32m2024-04-24T02:28:56 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n","\u001b[32m2024-04-24T02:28:56 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n","\u001b[32m2024-04-24T02:29:00 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished evaluation inference. Loaded 9\n","\u001b[32m2024-04-24T02:29:00 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n","\u001b[32m2024-04-24T02:29:00 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2024-04-24T02:29:03 | mmf.utils.checkpoint: \u001b[0mSaving best checkpoint\n","\u001b[32m2024-04-24T02:29:07 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2024-04-24T02:29:11 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2024-04-24T02:29:11 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 200/1500, val/hateful_memes/cross_entropy: 0.6819, val/total_loss: 0.6819, val/hateful_memes/accuracy: 0.6204, val/hateful_memes/binary_f1: 0.3189, val/hateful_memes/roc_auc: 0.5436, num_updates: 200, epoch: 1, iterations: 200, max_updates: 1500, val_time: 15s 199ms, best_update: 200, best_iteration: 200, best_val/hateful_memes/roc_auc: 0.543559\n","\u001b[32m2024-04-24T02:33:08 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n","\u001b[32m2024-04-24T02:33:08 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2024-04-24T02:33:12 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2024-04-24T02:33:21 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2024-04-24T02:33:21 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 400/1500, train/hateful_memes/cross_entropy: 0.5105, train/hateful_memes/cross_entropy/avg: 0.5599, train/total_loss: 0.5105, train/total_loss/avg: 0.5599, max mem: 14369.0, experiment: run, epoch: 2, num_updates: 400, iterations: 400, max_updates: 1500, lr: 0.00001, ups: 0.80, time: 04m 09s 883ms, time_since_start: 08m 37s 732ms, eta: 23m 56s 205ms\n","\u001b[32m2024-04-24T02:33:21 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n","\u001b[32m2024-04-24T02:33:21 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2024-04-24T02:33:21 | py.warnings: \u001b[0m/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n","  self.pid = os.fork()\n","\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2024-04-24T02:33:21 | py.warnings: \u001b[0m/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n","  self.pid = os.fork()\n","\n","\u001b[32m2024-04-24T02:33:25 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished evaluation inference. Loaded 9\n","\u001b[32m2024-04-24T02:33:25 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n","\u001b[32m2024-04-24T02:33:25 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2024-04-24T02:33:28 | mmf.utils.checkpoint: \u001b[0mSaving best checkpoint\n","\u001b[32m2024-04-24T02:33:32 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2024-04-24T02:33:36 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2024-04-24T02:33:36 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 400/1500, val/hateful_memes/cross_entropy: 0.7358, val/total_loss: 0.7358, val/hateful_memes/accuracy: 0.5963, val/hateful_memes/binary_f1: 0.3434, val/hateful_memes/roc_auc: 0.5572, num_updates: 400, epoch: 2, iterations: 400, max_updates: 1500, val_time: 15s 567ms, best_update: 400, best_iteration: 400, best_val/hateful_memes/roc_auc: 0.557235\n","\u001b[32m2024-04-24T02:37:34 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n","\u001b[32m2024-04-24T02:37:34 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2024-04-24T02:37:38 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2024-04-24T02:37:42 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2024-04-24T02:37:42 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 600/1500, train/hateful_memes/cross_entropy: 0.5105, train/hateful_memes/cross_entropy/avg: 0.4547, train/total_loss: 0.5105, train/total_loss/avg: 0.4547, max mem: 14369.0, experiment: run, epoch: 3, num_updates: 600, iterations: 600, max_updates: 1500, lr: 0.00002, ups: 0.81, time: 04m 06s 169ms, time_since_start: 12m 59s 472ms, eta: 19m 17s 613ms\n","\u001b[32m2024-04-24T02:37:42 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n","\u001b[32m2024-04-24T02:37:42 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2024-04-24T02:37:42 | py.warnings: \u001b[0m/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n","  self.pid = os.fork()\n","\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2024-04-24T02:37:42 | py.warnings: \u001b[0m/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n","  self.pid = os.fork()\n","\n","\u001b[32m2024-04-24T02:37:47 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished evaluation inference. Loaded 9\n","\u001b[32m2024-04-24T02:37:47 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n","\u001b[32m2024-04-24T02:37:47 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2024-04-24T02:37:52 | mmf.utils.checkpoint: \u001b[0mSaving best checkpoint\n","\u001b[32m2024-04-24T02:37:56 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2024-04-24T02:37:59 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2024-04-24T02:37:59 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 600/1500, val/hateful_memes/cross_entropy: 1.0317, val/total_loss: 1.0317, val/hateful_memes/accuracy: 0.5926, val/hateful_memes/binary_f1: 0.4330, val/hateful_memes/roc_auc: 0.5801, num_updates: 600, epoch: 3, iterations: 600, max_updates: 1500, val_time: 16s 864ms, best_update: 600, best_iteration: 600, best_val/hateful_memes/roc_auc: 0.580074\n","\u001b[32m2024-04-24T02:41:59 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n","\u001b[32m2024-04-24T02:41:59 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2024-04-24T02:42:01 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2024-04-24T02:42:06 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2024-04-24T02:42:06 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 800/1500, train/hateful_memes/cross_entropy: 0.2522, train/hateful_memes/cross_entropy/avg: 0.4041, train/total_loss: 0.2522, train/total_loss/avg: 0.4041, max mem: 14369.0, experiment: run, epoch: 3, num_updates: 800, iterations: 800, max_updates: 1500, lr: 0.00002, ups: 0.81, time: 04m 07s 116ms, time_since_start: 17m 23s 455ms, eta: 15m 03s 830ms\n","\u001b[32m2024-04-24T02:42:06 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n","\u001b[32m2024-04-24T02:42:06 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2024-04-24T02:42:06 | py.warnings: \u001b[0m/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n","  self.pid = os.fork()\n","\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2024-04-24T02:42:06 | py.warnings: \u001b[0m/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n","  self.pid = os.fork()\n","\n","\u001b[32m2024-04-24T02:42:11 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished evaluation inference. Loaded 9\n","\u001b[32m2024-04-24T02:42:11 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n","\u001b[32m2024-04-24T02:42:11 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2024-04-24T02:42:14 | mmf.utils.checkpoint: \u001b[0mSaving best checkpoint\n","\u001b[32m2024-04-24T02:42:22 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2024-04-24T02:42:27 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2024-04-24T02:42:27 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 800/1500, val/hateful_memes/cross_entropy: 1.0047, val/total_loss: 1.0047, val/hateful_memes/accuracy: 0.6537, val/hateful_memes/binary_f1: 0.4611, val/hateful_memes/roc_auc: 0.6121, num_updates: 800, epoch: 3, iterations: 800, max_updates: 1500, val_time: 20s 867ms, best_update: 800, best_iteration: 800, best_val/hateful_memes/roc_auc: 0.612103\n","\u001b[32m2024-04-24T02:46:27 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n","\u001b[32m2024-04-24T02:46:27 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2024-04-24T02:46:31 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2024-04-24T02:46:34 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2024-04-24T02:46:34 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1000/1500, train/hateful_memes/cross_entropy: 0.2522, train/hateful_memes/cross_entropy/avg: 0.3474, train/total_loss: 0.2522, train/total_loss/avg: 0.3474, max mem: 14369.0, experiment: run, epoch: 4, num_updates: 1000, iterations: 1000, max_updates: 1500, lr: 0.00003, ups: 0.81, time: 04m 07s 092ms, time_since_start: 21m 51s 417ms, eta: 10m 45s 528ms\n","\u001b[32m2024-04-24T02:46:34 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n","\u001b[32m2024-04-24T02:46:34 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2024-04-24T02:46:34 | py.warnings: \u001b[0m/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n","  self.pid = os.fork()\n","\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2024-04-24T02:46:34 | py.warnings: \u001b[0m/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n","  self.pid = os.fork()\n","\n","\u001b[32m2024-04-24T02:46:39 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished evaluation inference. Loaded 9\n","\u001b[32m2024-04-24T02:46:39 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n","\u001b[32m2024-04-24T02:46:39 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2024-04-24T02:46:42 | mmf.utils.checkpoint: \u001b[0mSaving best checkpoint\n","\u001b[32m2024-04-24T02:46:51 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2024-04-24T02:46:56 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2024-04-24T02:46:56 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1000/1500, val/hateful_memes/cross_entropy: 1.4216, val/total_loss: 1.4216, val/hateful_memes/accuracy: 0.6352, val/hateful_memes/binary_f1: 0.4543, val/hateful_memes/roc_auc: 0.6184, num_updates: 1000, epoch: 4, iterations: 1000, max_updates: 1500, val_time: 21s 294ms, best_update: 1000, best_iteration: 1000, best_val/hateful_memes/roc_auc: 0.618412\n","\u001b[32m2024-04-24T02:50:55 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n","\u001b[32m2024-04-24T02:50:55 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2024-04-24T02:50:58 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2024-04-24T02:51:02 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2024-04-24T02:51:02 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1200/1500, train/hateful_memes/cross_entropy: 0.2442, train/hateful_memes/cross_entropy/avg: 0.3248, train/total_loss: 0.2442, train/total_loss/avg: 0.3248, max mem: 14369.0, experiment: run, epoch: 5, num_updates: 1200, iterations: 1200, max_updates: 1500, lr: 0.00003, ups: 0.81, time: 04m 06s 553ms, time_since_start: 26m 19s 267ms, eta: 06m 26s 472ms\n","\u001b[32m2024-04-24T02:51:02 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n","\u001b[32m2024-04-24T02:51:02 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2024-04-24T02:51:02 | py.warnings: \u001b[0m/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n","  self.pid = os.fork()\n","\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2024-04-24T02:51:02 | py.warnings: \u001b[0m/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n","  self.pid = os.fork()\n","\n","\u001b[32m2024-04-24T02:51:06 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished evaluation inference. Loaded 9\n","\u001b[32m2024-04-24T02:51:07 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n","\u001b[32m2024-04-24T02:51:07 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2024-04-24T02:51:10 | mmf.utils.checkpoint: \u001b[0mSaving best checkpoint\n","\u001b[32m2024-04-24T02:51:18 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2024-04-24T02:51:23 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2024-04-24T02:51:23 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1200/1500, val/hateful_memes/cross_entropy: 1.2661, val/total_loss: 1.2661, val/hateful_memes/accuracy: 0.6537, val/hateful_memes/binary_f1: 0.3746, val/hateful_memes/roc_auc: 0.6457, num_updates: 1200, epoch: 5, iterations: 1200, max_updates: 1500, val_time: 20s 741ms, best_update: 1200, best_iteration: 1200, best_val/hateful_memes/roc_auc: 0.645691\n","\u001b[32m2024-04-24T02:55:22 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n","\u001b[32m2024-04-24T02:55:22 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2024-04-24T02:55:25 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2024-04-24T02:55:29 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2024-04-24T02:55:29 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1400/1500, train/hateful_memes/cross_entropy: 0.2442, train/hateful_memes/cross_entropy/avg: 0.2989, train/total_loss: 0.2442, train/total_loss/avg: 0.2989, max mem: 14369.0, experiment: run, epoch: 6, num_updates: 1400, iterations: 1400, max_updates: 1500, lr: 0.00003, ups: 0.81, time: 04m 06s 356ms, time_since_start: 30m 46s 367ms, eta: 02m 08s 721ms\n","\u001b[32m2024-04-24T02:55:29 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n","\u001b[32m2024-04-24T02:55:29 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2024-04-24T02:55:29 | py.warnings: \u001b[0m/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n","  self.pid = os.fork()\n","\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2024-04-24T02:55:29 | py.warnings: \u001b[0m/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n","  self.pid = os.fork()\n","\n","\u001b[32m2024-04-24T02:55:34 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished evaluation inference. Loaded 9\n","\u001b[32m2024-04-24T02:55:34 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n","\u001b[32m2024-04-24T02:55:34 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2024-04-24T02:55:38 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2024-04-24T02:55:45 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2024-04-24T02:55:45 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1400/1500, val/hateful_memes/cross_entropy: 1.4059, val/total_loss: 1.4059, val/hateful_memes/accuracy: 0.6241, val/hateful_memes/binary_f1: 0.4938, val/hateful_memes/roc_auc: 0.6400, num_updates: 1400, epoch: 6, iterations: 1400, max_updates: 1500, val_time: 15s 879ms, best_update: 1200, best_iteration: 1200, best_val/hateful_memes/roc_auc: 0.645691\n","\u001b[32m2024-04-24T02:57:45 | mmf.trainers.core.training_loop: \u001b[0mStepping into final validation check\n","\u001b[32m2024-04-24T02:57:45 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2024-04-24T02:57:45 | py.warnings: \u001b[0m/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n","  self.pid = os.fork()\n","\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2024-04-24T02:57:45 | py.warnings: \u001b[0m/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n","  self.pid = os.fork()\n","\n","\u001b[32m2024-04-24T02:57:49 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished evaluation inference. Loaded 9\n","\u001b[32m2024-04-24T02:57:49 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n","\u001b[32m2024-04-24T02:57:50 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1500/1500, val/hateful_memes/cross_entropy: 1.5660, val/total_loss: 1.5660, val/hateful_memes/accuracy: 0.6407, val/hateful_memes/binary_f1: 0.3782, val/hateful_memes/roc_auc: 0.6209, num_updates: 1500, epoch: 6, iterations: 1500, max_updates: 1500, val_time: 02m 20s 510ms, best_update: 1200, best_iteration: 1200, best_val/hateful_memes/roc_auc: 0.645691\n","\u001b[32m2024-04-24T02:57:50 | mmf.utils.checkpoint: \u001b[0mRestoring checkpoint\n","\u001b[32m2024-04-24T02:57:50 | mmf.utils.checkpoint: \u001b[0mLoading checkpoint\n","\u001b[32m2024-04-24T02:57:50 | mmf.utils.checkpoint: \u001b[0mCheckpoint loaded.\n","\u001b[32m2024-04-24T02:57:50 | mmf.utils.checkpoint: \u001b[0mCurrent num updates: 1200\n","\u001b[32m2024-04-24T02:57:50 | mmf.utils.checkpoint: \u001b[0mCurrent iteration: 1200\n","\u001b[32m2024-04-24T02:57:50 | mmf.utils.checkpoint: \u001b[0mCurrent epoch: 5\n","\u001b[32m2024-04-24T02:57:51 | mmf.trainers.mmf_trainer: \u001b[0mStarting inference on val set\n","\u001b[32m2024-04-24T02:57:51 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n","  0% 0/9 [00:00<?, ?it/s]\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2024-04-24T02:57:51 | py.warnings: \u001b[0m/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n","  self.pid = os.fork()\n","\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2024-04-24T02:57:51 | py.warnings: \u001b[0m/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n","  self.pid = os.fork()\n","\n","100% 9/9 [00:04<00:00,  2.12it/s]\n","\u001b[32m2024-04-24T02:57:56 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished evaluation inference. Loaded 9\n","\u001b[32m2024-04-24T02:57:56 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n","\u001b[32m2024-04-24T02:57:56 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1200/1500, val/hateful_memes/cross_entropy: 1.2661, val/total_loss: 1.2661, val/hateful_memes/accuracy: 0.6537, val/hateful_memes/binary_f1: 0.3746, val/hateful_memes/roc_auc: 0.6457\n","\u001b[32m2024-04-24T02:57:56 | mmf.trainers.callbacks.logistics: \u001b[0mFinished run in 33m 13s 040ms\n"]}]},{"cell_type":"markdown","source":["### Dataset: Augmented with complete transformations"],"metadata":{"id":"WBYt1CShv1WC"}},{"cell_type":"code","source":["!mmf_run config=\"./configs/defaults.yaml\" \\\n","  model=visual_bert \\\n","  dataset=hateful_memes \\\n","  run_type=train_val \\\n","  training.log_interval=200 \\\n","  training.batch_size=64 \\\n","  training.evaluation_interval=200 \\\n","  training.tensorboard=True \\\n","  training.checkpoint_interval=200 \\\n","  checkpoint.resume_pretrained=True \\\n","  checkpoint.resume_file=\"/content/drive/MyDrive/hateful-memes/Deep Learning Group Project/visualBERT_vizwiz_04212024_pretrained.ckpt\" \\\n","  dataset_config.hateful_memes.annotations.train[0]=\"hateful_memes/defaults/annotations/train_dev_transform_all.jsonl\" \\\n","  dataset_config.hateful_memes.annotations.val[0]=\"hateful_memes/defaults/annotations/dev_unseen.jsonl\" \\\n","  dataset_config.hateful_memes.annotations.test[0]=\"hateful_memes/defaults/annotations/test_unseen.jsonl\""],"metadata":{"id":"oVPSfiU6v7FA","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1713985336751,"user_tz":420,"elapsed":2023229,"user":{"displayName":"Aneri Shah","userId":"14009776940163552827"}},"outputId":"d69010f9-366e-4ef5-8369-0ad18bb4c71e"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["2024-04-24 18:28:36.744362: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","2024-04-24 18:28:36.744436: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","2024-04-24 18:28:36.746058: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","2024-04-24 18:28:37.854011: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","2024-04-24 18:28:42.621793: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:47] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n","/usr/local/lib/python3.10/dist-packages/transformers/deepspeed.py:23: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/omegaconf/grammar_visitor.py:253: UserWarning: In the sequence `MMF_USER_DIR,` some elements are missing: please replace them with empty quoted strings. See https://github.com/omry/omegaconf/issues/572 for details.\n","  warnings.warn(\n","\u001b[32m2024-04-24T18:28:47 | mmf.utils.configuration: \u001b[0mOverriding option config to ./configs/defaults.yaml\n","\u001b[32m2024-04-24T18:28:47 | mmf.utils.configuration: \u001b[0mOverriding option model to visual_bert\n","\u001b[32m2024-04-24T18:28:47 | mmf.utils.configuration: \u001b[0mOverriding option datasets to hateful_memes\n","\u001b[32m2024-04-24T18:28:47 | mmf.utils.configuration: \u001b[0mOverriding option run_type to train_val\n","\u001b[32m2024-04-24T18:28:47 | mmf.utils.configuration: \u001b[0mOverriding option training.log_interval to 200\n","\u001b[32m2024-04-24T18:28:47 | mmf.utils.configuration: \u001b[0mOverriding option training.batch_size to 64\n","\u001b[32m2024-04-24T18:28:47 | mmf.utils.configuration: \u001b[0mOverriding option training.evaluation_interval to 200\n","\u001b[32m2024-04-24T18:28:47 | mmf.utils.configuration: \u001b[0mOverriding option training.tensorboard to True\n","\u001b[32m2024-04-24T18:28:47 | mmf.utils.configuration: \u001b[0mOverriding option training.checkpoint_interval to 200\n","\u001b[32m2024-04-24T18:28:47 | mmf.utils.configuration: \u001b[0mOverriding option checkpoint.resume_pretrained to True\n","\u001b[32m2024-04-24T18:28:47 | mmf.utils.configuration: \u001b[0mOverriding option checkpoint.resume_file to /content/drive/MyDrive/hateful-memes/Deep Learning Group Project/visualBERT_vizwiz_04212024_pretrained.ckpt\n","\u001b[32m2024-04-24T18:28:47 | mmf.utils.configuration: \u001b[0mOverriding option dataset_config.hateful_memes.annotations.train[0] to hateful_memes/defaults/annotations/train_dev_transform_all.jsonl\n","\u001b[32m2024-04-24T18:28:47 | mmf.utils.configuration: \u001b[0mOverriding option dataset_config.hateful_memes.annotations.val[0] to hateful_memes/defaults/annotations/dev_unseen.jsonl\n","\u001b[32m2024-04-24T18:28:47 | mmf.utils.configuration: \u001b[0mOverriding option dataset_config.hateful_memes.annotations.test[0] to hateful_memes/defaults/annotations/test_unseen.jsonl\n","/usr/local/lib/python3.10/dist-packages/omegaconf/grammar_visitor.py:253: UserWarning: In the sequence `MMF_LOG_DIR,` some elements are missing: please replace them with empty quoted strings. See https://github.com/omry/omegaconf/issues/572 for details.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/omegaconf/grammar_visitor.py:253: UserWarning: In the sequence `MMF_REPORT_DIR,` some elements are missing: please replace them with empty quoted strings. See https://github.com/omry/omegaconf/issues/572 for details.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/omegaconf/grammar_visitor.py:253: UserWarning: In the sequence `MMF_TENSORBOARD_LOGDIR,` some elements are missing: please replace them with empty quoted strings. See https://github.com/omry/omegaconf/issues/572 for details.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/omegaconf/grammar_visitor.py:253: UserWarning: In the sequence `MMF_WANDB_LOGDIR,` some elements are missing: please replace them with empty quoted strings. See https://github.com/omry/omegaconf/issues/572 for details.\n","  warnings.warn(\n","\u001b[32m2024-04-24T18:28:47 | mmf: \u001b[0mLogging to: ./save/train.log\n","\u001b[32m2024-04-24T18:28:47 | mmf_cli.run: \u001b[0mNamespace(config_override=None, local_rank=None, opts=['config=./configs/defaults.yaml', 'model=visual_bert', 'dataset=hateful_memes', 'run_type=train_val', 'training.log_interval=200', 'training.batch_size=64', 'training.evaluation_interval=200', 'training.tensorboard=True', 'training.checkpoint_interval=200', 'checkpoint.resume_pretrained=True', 'checkpoint.resume_file=/content/drive/MyDrive/hateful-memes/Deep Learning Group Project/visualBERT_vizwiz_04212024_pretrained.ckpt', 'dataset_config.hateful_memes.annotations.train[0]=hateful_memes/defaults/annotations/train_dev_transform_all.jsonl', 'dataset_config.hateful_memes.annotations.val[0]=hateful_memes/defaults/annotations/dev_unseen.jsonl', 'dataset_config.hateful_memes.annotations.test[0]=hateful_memes/defaults/annotations/test_unseen.jsonl'])\n","\u001b[32m2024-04-24T18:28:47 | mmf_cli.run: \u001b[0mTorch version: 2.2.0+cu121\n","\u001b[32m2024-04-24T18:28:47 | mmf.utils.general: \u001b[0mCUDA Device 0 is: NVIDIA L4\n","\u001b[32m2024-04-24T18:28:47 | mmf_cli.run: \u001b[0mUsing seed 47814321\n","\u001b[32m2024-04-24T18:28:47 | mmf.trainers.mmf_trainer: \u001b[0mLoading datasets\n","loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--bert-base-uncased/snapshots/86b5e0934494bd15c9632b12f734a8a67f723594/config.json\n","Model config BertConfig {\n","  \"_name_or_path\": \"bert-base-uncased\",\n","  \"architectures\": [\n","    \"BertForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.39.2\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n","loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--bert-base-uncased/snapshots/86b5e0934494bd15c9632b12f734a8a67f723594/vocab.txt\n","loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--bert-base-uncased/snapshots/86b5e0934494bd15c9632b12f734a8a67f723594/tokenizer.json\n","loading file added_tokens.json from cache at None\n","loading file special_tokens_map.json from cache at None\n","loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--bert-base-uncased/snapshots/86b5e0934494bd15c9632b12f734a8a67f723594/tokenizer_config.json\n","loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--bert-base-uncased/snapshots/86b5e0934494bd15c9632b12f734a8a67f723594/config.json\n","Model config BertConfig {\n","  \"_name_or_path\": \"bert-base-uncased\",\n","  \"architectures\": [\n","    \"BertForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.39.2\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n","\u001b[32m2024-04-24T18:28:48 | mmf.datasets.multi_datamodule: \u001b[0mMultitasking disabled by default for single dataset training\n","\u001b[32m2024-04-24T18:28:48 | mmf.datasets.multi_datamodule: \u001b[0mMultitasking disabled by default for single dataset training\n","\u001b[32m2024-04-24T18:28:48 | mmf.datasets.multi_datamodule: \u001b[0mMultitasking disabled by default for single dataset training\n","\u001b[32m2024-04-24T18:28:48 | mmf.trainers.mmf_trainer: \u001b[0mLoading model\n","Model config BertConfig {\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bert_model_name\": \"bert-base-uncased\",\n","  \"bypass_transformer\": false,\n","  \"classifier_dropout\": null,\n","  \"embedding_strategy\": \"plain\",\n","  \"finetune_lr_multiplier\": 1,\n","  \"freeze_base\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"losses\": [\n","    \"cross_entropy\"\n","  ],\n","  \"max_position_embeddings\": 512,\n","  \"model\": \"visual_bert\",\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"pooler_strategy\": \"default\",\n","  \"position_embedding_type\": \"absolute\",\n","  \"random_initialize\": false,\n","  \"special_visual_initialize\": true,\n","  \"training_head_type\": \"classification\",\n","  \"transformers_version\": \"4.39.2\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"visual_embedding_dim\": 2048,\n","  \"vocab_size\": 30522,\n","  \"zerobias\": false\n","}\n","\n","loading weights file model.safetensors from cache at /root/.cache/torch/mmf/distributed_-1/models--bert-base-uncased/snapshots/86b5e0934494bd15c9632b12f734a8a67f723594/model.safetensors\n","Some weights of the model checkpoint at bert-base-uncased were not used when initializing VisualBERTBase: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n","- This IS expected if you are initializing VisualBERTBase from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing VisualBERTBase from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of VisualBERTBase were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['bert.embeddings.position_embeddings_visual.weight', 'bert.embeddings.projection.bias', 'bert.embeddings.projection.weight', 'bert.embeddings.token_type_embeddings_visual.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","\u001b[32m2024-04-24T18:28:49 | mmf.trainers.mmf_trainer: \u001b[0mLoading optimizer\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2024-04-24T18:28:49 | py.warnings: \u001b[0m/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:457: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n","\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2024-04-24T18:28:49 | py.warnings: \u001b[0m/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:457: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n","\n","\u001b[32m2024-04-24T18:28:49 | mmf.trainers.mmf_trainer: \u001b[0mLoading metrics\n","\u001b[32m2024-04-24T18:28:50 | mmf.utils.checkpoint: \u001b[0mLoading checkpoint\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2024-04-24T18:29:09 | mmf: \u001b[0mKey data_parallel is not present in registry, returning default value of None\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2024-04-24T18:29:09 | mmf: \u001b[0mKey distributed is not present in registry, returning default value of None\n","\u001b[32m2024-04-24T18:29:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.word_embeddings.weight from model.bert.embeddings.word_embeddings.weight\n","\u001b[32m2024-04-24T18:29:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.position_embeddings.weight from model.bert.embeddings.position_embeddings.weight\n","\u001b[32m2024-04-24T18:29:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.token_type_embeddings.weight from model.bert.embeddings.token_type_embeddings.weight\n","\u001b[32m2024-04-24T18:29:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.LayerNorm.weight from model.bert.embeddings.LayerNorm.weight\n","\u001b[32m2024-04-24T18:29:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.LayerNorm.bias from model.bert.embeddings.LayerNorm.bias\n","\u001b[32m2024-04-24T18:29:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.token_type_embeddings_visual.weight from model.bert.embeddings.token_type_embeddings_visual.weight\n","\u001b[32m2024-04-24T18:29:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.position_embeddings_visual.weight from model.bert.embeddings.position_embeddings_visual.weight\n","\u001b[32m2024-04-24T18:29:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.projection.weight from model.bert.embeddings.projection.weight\n","\u001b[32m2024-04-24T18:29:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.projection.bias from model.bert.embeddings.projection.bias\n","\u001b[32m2024-04-24T18:29:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.self.query.weight from model.bert.encoder.layer.0.attention.self.query.weight\n","\u001b[32m2024-04-24T18:29:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.self.query.bias from model.bert.encoder.layer.0.attention.self.query.bias\n","\u001b[32m2024-04-24T18:29:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.self.key.weight from model.bert.encoder.layer.0.attention.self.key.weight\n","\u001b[32m2024-04-24T18:29:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.self.key.bias from model.bert.encoder.layer.0.attention.self.key.bias\n","\u001b[32m2024-04-24T18:29:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.self.value.weight from model.bert.encoder.layer.0.attention.self.value.weight\n","\u001b[32m2024-04-24T18:29:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.self.value.bias from model.bert.encoder.layer.0.attention.self.value.bias\n","\u001b[32m2024-04-24T18:29:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.output.dense.weight from model.bert.encoder.layer.0.attention.output.dense.weight\n","\u001b[32m2024-04-24T18:29:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.output.dense.bias from model.bert.encoder.layer.0.attention.output.dense.bias\n","\u001b[32m2024-04-24T18:29:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.output.LayerNorm.weight from model.bert.encoder.layer.0.attention.output.LayerNorm.weight\n","\u001b[32m2024-04-24T18:29:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.output.LayerNorm.bias from model.bert.encoder.layer.0.attention.output.LayerNorm.bias\n","\u001b[32m2024-04-24T18:29:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.intermediate.dense.weight from model.bert.encoder.layer.0.intermediate.dense.weight\n","\u001b[32m2024-04-24T18:29:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.intermediate.dense.bias from model.bert.encoder.layer.0.intermediate.dense.bias\n","\u001b[32m2024-04-24T18:29:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.output.dense.weight from model.bert.encoder.layer.0.output.dense.weight\n","\u001b[32m2024-04-24T18:29:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.output.dense.bias from model.bert.encoder.layer.0.output.dense.bias\n","\u001b[32m2024-04-24T18:29:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.output.LayerNorm.weight from model.bert.encoder.layer.0.output.LayerNorm.weight\n","\u001b[32m2024-04-24T18:29:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.output.LayerNorm.bias from model.bert.encoder.layer.0.output.LayerNorm.bias\n","\u001b[32m2024-04-24T18:29:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.self.query.weight from model.bert.encoder.layer.1.attention.self.query.weight\n","\u001b[32m2024-04-24T18:29:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.self.query.bias from model.bert.encoder.layer.1.attention.self.query.bias\n","\u001b[32m2024-04-24T18:29:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.self.key.weight from model.bert.encoder.layer.1.attention.self.key.weight\n","\u001b[32m2024-04-24T18:29:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.self.key.bias from model.bert.encoder.layer.1.attention.self.key.bias\n","\u001b[32m2024-04-24T18:29:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.self.value.weight from model.bert.encoder.layer.1.attention.self.value.weight\n","\u001b[32m2024-04-24T18:29:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.self.value.bias from model.bert.encoder.layer.1.attention.self.value.bias\n","\u001b[32m2024-04-24T18:29:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.output.dense.weight from model.bert.encoder.layer.1.attention.output.dense.weight\n","\u001b[32m2024-04-24T18:29:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.output.dense.bias from model.bert.encoder.layer.1.attention.output.dense.bias\n","\u001b[32m2024-04-24T18:29:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.output.LayerNorm.weight from model.bert.encoder.layer.1.attention.output.LayerNorm.weight\n","\u001b[32m2024-04-24T18:29:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.output.LayerNorm.bias from model.bert.encoder.layer.1.attention.output.LayerNorm.bias\n","\u001b[32m2024-04-24T18:29:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.intermediate.dense.weight from model.bert.encoder.layer.1.intermediate.dense.weight\n","\u001b[32m2024-04-24T18:29:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.intermediate.dense.bias from model.bert.encoder.layer.1.intermediate.dense.bias\n","\u001b[32m2024-04-24T18:29:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.output.dense.weight from model.bert.encoder.layer.1.output.dense.weight\n","\u001b[32m2024-04-24T18:29:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.output.dense.bias from model.bert.encoder.layer.1.output.dense.bias\n","\u001b[32m2024-04-24T18:29:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.output.LayerNorm.weight from model.bert.encoder.layer.1.output.LayerNorm.weight\n","\u001b[32m2024-04-24T18:29:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.output.LayerNorm.bias from model.bert.encoder.layer.1.output.LayerNorm.bias\n","\u001b[32m2024-04-24T18:29:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.self.query.weight from model.bert.encoder.layer.2.attention.self.query.weight\n","\u001b[32m2024-04-24T18:29:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.self.query.bias from model.bert.encoder.layer.2.attention.self.query.bias\n","\u001b[32m2024-04-24T18:29:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.self.key.weight from model.bert.encoder.layer.2.attention.self.key.weight\n","\u001b[32m2024-04-24T18:29:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.self.key.bias from model.bert.encoder.layer.2.attention.self.key.bias\n","\u001b[32m2024-04-24T18:29:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.self.value.weight from model.bert.encoder.layer.2.attention.self.value.weight\n","\u001b[32m2024-04-24T18:29:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.self.value.bias from model.bert.encoder.layer.2.attention.self.value.bias\n","\u001b[32m2024-04-24T18:29:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.output.dense.weight from model.bert.encoder.layer.2.attention.output.dense.weight\n","\u001b[32m2024-04-24T18:29:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.output.dense.bias from model.bert.encoder.layer.2.attention.output.dense.bias\n","\u001b[32m2024-04-24T18:29:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.output.LayerNorm.weight from model.bert.encoder.layer.2.attention.output.LayerNorm.weight\n","\u001b[32m2024-04-24T18:29:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.output.LayerNorm.bias from model.bert.encoder.layer.2.attention.output.LayerNorm.bias\n","\u001b[32m2024-04-24T18:29:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.intermediate.dense.weight from model.bert.encoder.layer.2.intermediate.dense.weight\n","\u001b[32m2024-04-24T18:29:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.intermediate.dense.bias from model.bert.encoder.layer.2.intermediate.dense.bias\n","\u001b[32m2024-04-24T18:29:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.output.dense.weight from model.bert.encoder.layer.2.output.dense.weight\n","\u001b[32m2024-04-24T18:29:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.output.dense.bias from model.bert.encoder.layer.2.output.dense.bias\n","\u001b[32m2024-04-24T18:29:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.output.LayerNorm.weight from model.bert.encoder.layer.2.output.LayerNorm.weight\n","\u001b[32m2024-04-24T18:29:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.output.LayerNorm.bias from model.bert.encoder.layer.2.output.LayerNorm.bias\n","\u001b[32m2024-04-24T18:29:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.self.query.weight from model.bert.encoder.layer.3.attention.self.query.weight\n","\u001b[32m2024-04-24T18:29:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.self.query.bias from model.bert.encoder.layer.3.attention.self.query.bias\n","\u001b[32m2024-04-24T18:29:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.self.key.weight from model.bert.encoder.layer.3.attention.self.key.weight\n","\u001b[32m2024-04-24T18:29:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.self.key.bias from model.bert.encoder.layer.3.attention.self.key.bias\n","\u001b[32m2024-04-24T18:29:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.self.value.weight from model.bert.encoder.layer.3.attention.self.value.weight\n","\u001b[32m2024-04-24T18:29:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.self.value.bias from model.bert.encoder.layer.3.attention.self.value.bias\n","\u001b[32m2024-04-24T18:29:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.output.dense.weight from model.bert.encoder.layer.3.attention.output.dense.weight\n","\u001b[32m2024-04-24T18:29:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.output.dense.bias from model.bert.encoder.layer.3.attention.output.dense.bias\n","\u001b[32m2024-04-24T18:29:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.output.LayerNorm.weight from model.bert.encoder.layer.3.attention.output.LayerNorm.weight\n","\u001b[32m2024-04-24T18:29:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.output.LayerNorm.bias from model.bert.encoder.layer.3.attention.output.LayerNorm.bias\n","\u001b[32m2024-04-24T18:29:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.intermediate.dense.weight from model.bert.encoder.layer.3.intermediate.dense.weight\n","\u001b[32m2024-04-24T18:29:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.intermediate.dense.bias from model.bert.encoder.layer.3.intermediate.dense.bias\n","\u001b[32m2024-04-24T18:29:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.output.dense.weight from model.bert.encoder.layer.3.output.dense.weight\n","\u001b[32m2024-04-24T18:29:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.output.dense.bias from model.bert.encoder.layer.3.output.dense.bias\n","\u001b[32m2024-04-24T18:29:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.output.LayerNorm.weight from model.bert.encoder.layer.3.output.LayerNorm.weight\n","\u001b[32m2024-04-24T18:29:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.output.LayerNorm.bias from model.bert.encoder.layer.3.output.LayerNorm.bias\n","\u001b[32m2024-04-24T18:29:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.self.query.weight from model.bert.encoder.layer.4.attention.self.query.weight\n","\u001b[32m2024-04-24T18:29:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.self.query.bias from model.bert.encoder.layer.4.attention.self.query.bias\n","\u001b[32m2024-04-24T18:29:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.self.key.weight from model.bert.encoder.layer.4.attention.self.key.weight\n","\u001b[32m2024-04-24T18:29:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.self.key.bias from model.bert.encoder.layer.4.attention.self.key.bias\n","\u001b[32m2024-04-24T18:29:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.self.value.weight from model.bert.encoder.layer.4.attention.self.value.weight\n","\u001b[32m2024-04-24T18:29:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.self.value.bias from model.bert.encoder.layer.4.attention.self.value.bias\n","\u001b[32m2024-04-24T18:29:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.output.dense.weight from model.bert.encoder.layer.4.attention.output.dense.weight\n","\u001b[32m2024-04-24T18:29:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.output.dense.bias from model.bert.encoder.layer.4.attention.output.dense.bias\n","\u001b[32m2024-04-24T18:29:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.output.LayerNorm.weight from model.bert.encoder.layer.4.attention.output.LayerNorm.weight\n","\u001b[32m2024-04-24T18:29:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.output.LayerNorm.bias from model.bert.encoder.layer.4.attention.output.LayerNorm.bias\n","\u001b[32m2024-04-24T18:29:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.intermediate.dense.weight from model.bert.encoder.layer.4.intermediate.dense.weight\n","\u001b[32m2024-04-24T18:29:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.intermediate.dense.bias from model.bert.encoder.layer.4.intermediate.dense.bias\n","\u001b[32m2024-04-24T18:29:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.output.dense.weight from model.bert.encoder.layer.4.output.dense.weight\n","\u001b[32m2024-04-24T18:29:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.output.dense.bias from model.bert.encoder.layer.4.output.dense.bias\n","\u001b[32m2024-04-24T18:29:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.output.LayerNorm.weight from model.bert.encoder.layer.4.output.LayerNorm.weight\n","\u001b[32m2024-04-24T18:29:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.output.LayerNorm.bias from model.bert.encoder.layer.4.output.LayerNorm.bias\n","\u001b[32m2024-04-24T18:29:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.self.query.weight from model.bert.encoder.layer.5.attention.self.query.weight\n","\u001b[32m2024-04-24T18:29:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.self.query.bias from model.bert.encoder.layer.5.attention.self.query.bias\n","\u001b[32m2024-04-24T18:29:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.self.key.weight from model.bert.encoder.layer.5.attention.self.key.weight\n","\u001b[32m2024-04-24T18:29:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.self.key.bias from model.bert.encoder.layer.5.attention.self.key.bias\n","\u001b[32m2024-04-24T18:29:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.self.value.weight from model.bert.encoder.layer.5.attention.self.value.weight\n","\u001b[32m2024-04-24T18:29:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.self.value.bias from model.bert.encoder.layer.5.attention.self.value.bias\n","\u001b[32m2024-04-24T18:29:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.output.dense.weight from model.bert.encoder.layer.5.attention.output.dense.weight\n","\u001b[32m2024-04-24T18:29:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.output.dense.bias from model.bert.encoder.layer.5.attention.output.dense.bias\n","\u001b[32m2024-04-24T18:29:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.output.LayerNorm.weight from model.bert.encoder.layer.5.attention.output.LayerNorm.weight\n","\u001b[32m2024-04-24T18:29:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.output.LayerNorm.bias from model.bert.encoder.layer.5.attention.output.LayerNorm.bias\n","\u001b[32m2024-04-24T18:29:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.intermediate.dense.weight from model.bert.encoder.layer.5.intermediate.dense.weight\n","\u001b[32m2024-04-24T18:29:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.intermediate.dense.bias from model.bert.encoder.layer.5.intermediate.dense.bias\n","\u001b[32m2024-04-24T18:29:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.output.dense.weight from model.bert.encoder.layer.5.output.dense.weight\n","\u001b[32m2024-04-24T18:29:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.output.dense.bias from model.bert.encoder.layer.5.output.dense.bias\n","\u001b[32m2024-04-24T18:29:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.output.LayerNorm.weight from model.bert.encoder.layer.5.output.LayerNorm.weight\n","\u001b[32m2024-04-24T18:29:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.output.LayerNorm.bias from model.bert.encoder.layer.5.output.LayerNorm.bias\n","\u001b[32m2024-04-24T18:29:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.self.query.weight from model.bert.encoder.layer.6.attention.self.query.weight\n","\u001b[32m2024-04-24T18:29:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.self.query.bias from model.bert.encoder.layer.6.attention.self.query.bias\n","\u001b[32m2024-04-24T18:29:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.self.key.weight from model.bert.encoder.layer.6.attention.self.key.weight\n","\u001b[32m2024-04-24T18:29:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.self.key.bias from model.bert.encoder.layer.6.attention.self.key.bias\n","\u001b[32m2024-04-24T18:29:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.self.value.weight from model.bert.encoder.layer.6.attention.self.value.weight\n","\u001b[32m2024-04-24T18:29:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.self.value.bias from model.bert.encoder.layer.6.attention.self.value.bias\n","\u001b[32m2024-04-24T18:29:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.output.dense.weight from model.bert.encoder.layer.6.attention.output.dense.weight\n","\u001b[32m2024-04-24T18:29:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.output.dense.bias from model.bert.encoder.layer.6.attention.output.dense.bias\n","\u001b[32m2024-04-24T18:29:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.output.LayerNorm.weight from model.bert.encoder.layer.6.attention.output.LayerNorm.weight\n","\u001b[32m2024-04-24T18:29:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.output.LayerNorm.bias from model.bert.encoder.layer.6.attention.output.LayerNorm.bias\n","\u001b[32m2024-04-24T18:29:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.intermediate.dense.weight from model.bert.encoder.layer.6.intermediate.dense.weight\n","\u001b[32m2024-04-24T18:29:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.intermediate.dense.bias from model.bert.encoder.layer.6.intermediate.dense.bias\n","\u001b[32m2024-04-24T18:29:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.output.dense.weight from model.bert.encoder.layer.6.output.dense.weight\n","\u001b[32m2024-04-24T18:29:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.output.dense.bias from model.bert.encoder.layer.6.output.dense.bias\n","\u001b[32m2024-04-24T18:29:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.output.LayerNorm.weight from model.bert.encoder.layer.6.output.LayerNorm.weight\n","\u001b[32m2024-04-24T18:29:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.output.LayerNorm.bias from model.bert.encoder.layer.6.output.LayerNorm.bias\n","\u001b[32m2024-04-24T18:29:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.self.query.weight from model.bert.encoder.layer.7.attention.self.query.weight\n","\u001b[32m2024-04-24T18:29:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.self.query.bias from model.bert.encoder.layer.7.attention.self.query.bias\n","\u001b[32m2024-04-24T18:29:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.self.key.weight from model.bert.encoder.layer.7.attention.self.key.weight\n","\u001b[32m2024-04-24T18:29:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.self.key.bias from model.bert.encoder.layer.7.attention.self.key.bias\n","\u001b[32m2024-04-24T18:29:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.self.value.weight from model.bert.encoder.layer.7.attention.self.value.weight\n","\u001b[32m2024-04-24T18:29:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.self.value.bias from model.bert.encoder.layer.7.attention.self.value.bias\n","\u001b[32m2024-04-24T18:29:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.output.dense.weight from model.bert.encoder.layer.7.attention.output.dense.weight\n","\u001b[32m2024-04-24T18:29:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.output.dense.bias from model.bert.encoder.layer.7.attention.output.dense.bias\n","\u001b[32m2024-04-24T18:29:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.output.LayerNorm.weight from model.bert.encoder.layer.7.attention.output.LayerNorm.weight\n","\u001b[32m2024-04-24T18:29:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.output.LayerNorm.bias from model.bert.encoder.layer.7.attention.output.LayerNorm.bias\n","\u001b[32m2024-04-24T18:29:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.intermediate.dense.weight from model.bert.encoder.layer.7.intermediate.dense.weight\n","\u001b[32m2024-04-24T18:29:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.intermediate.dense.bias from model.bert.encoder.layer.7.intermediate.dense.bias\n","\u001b[32m2024-04-24T18:29:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.output.dense.weight from model.bert.encoder.layer.7.output.dense.weight\n","\u001b[32m2024-04-24T18:29:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.output.dense.bias from model.bert.encoder.layer.7.output.dense.bias\n","\u001b[32m2024-04-24T18:29:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.output.LayerNorm.weight from model.bert.encoder.layer.7.output.LayerNorm.weight\n","\u001b[32m2024-04-24T18:29:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.output.LayerNorm.bias from model.bert.encoder.layer.7.output.LayerNorm.bias\n","\u001b[32m2024-04-24T18:29:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.self.query.weight from model.bert.encoder.layer.8.attention.self.query.weight\n","\u001b[32m2024-04-24T18:29:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.self.query.bias from model.bert.encoder.layer.8.attention.self.query.bias\n","\u001b[32m2024-04-24T18:29:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.self.key.weight from model.bert.encoder.layer.8.attention.self.key.weight\n","\u001b[32m2024-04-24T18:29:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.self.key.bias from model.bert.encoder.layer.8.attention.self.key.bias\n","\u001b[32m2024-04-24T18:29:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.self.value.weight from model.bert.encoder.layer.8.attention.self.value.weight\n","\u001b[32m2024-04-24T18:29:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.self.value.bias from model.bert.encoder.layer.8.attention.self.value.bias\n","\u001b[32m2024-04-24T18:29:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.output.dense.weight from model.bert.encoder.layer.8.attention.output.dense.weight\n","\u001b[32m2024-04-24T18:29:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.output.dense.bias from model.bert.encoder.layer.8.attention.output.dense.bias\n","\u001b[32m2024-04-24T18:29:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.output.LayerNorm.weight from model.bert.encoder.layer.8.attention.output.LayerNorm.weight\n","\u001b[32m2024-04-24T18:29:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.output.LayerNorm.bias from model.bert.encoder.layer.8.attention.output.LayerNorm.bias\n","\u001b[32m2024-04-24T18:29:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.intermediate.dense.weight from model.bert.encoder.layer.8.intermediate.dense.weight\n","\u001b[32m2024-04-24T18:29:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.intermediate.dense.bias from model.bert.encoder.layer.8.intermediate.dense.bias\n","\u001b[32m2024-04-24T18:29:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.output.dense.weight from model.bert.encoder.layer.8.output.dense.weight\n","\u001b[32m2024-04-24T18:29:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.output.dense.bias from model.bert.encoder.layer.8.output.dense.bias\n","\u001b[32m2024-04-24T18:29:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.output.LayerNorm.weight from model.bert.encoder.layer.8.output.LayerNorm.weight\n","\u001b[32m2024-04-24T18:29:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.output.LayerNorm.bias from model.bert.encoder.layer.8.output.LayerNorm.bias\n","\u001b[32m2024-04-24T18:29:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.self.query.weight from model.bert.encoder.layer.9.attention.self.query.weight\n","\u001b[32m2024-04-24T18:29:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.self.query.bias from model.bert.encoder.layer.9.attention.self.query.bias\n","\u001b[32m2024-04-24T18:29:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.self.key.weight from model.bert.encoder.layer.9.attention.self.key.weight\n","\u001b[32m2024-04-24T18:29:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.self.key.bias from model.bert.encoder.layer.9.attention.self.key.bias\n","\u001b[32m2024-04-24T18:29:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.self.value.weight from model.bert.encoder.layer.9.attention.self.value.weight\n","\u001b[32m2024-04-24T18:29:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.self.value.bias from model.bert.encoder.layer.9.attention.self.value.bias\n","\u001b[32m2024-04-24T18:29:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.output.dense.weight from model.bert.encoder.layer.9.attention.output.dense.weight\n","\u001b[32m2024-04-24T18:29:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.output.dense.bias from model.bert.encoder.layer.9.attention.output.dense.bias\n","\u001b[32m2024-04-24T18:29:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.output.LayerNorm.weight from model.bert.encoder.layer.9.attention.output.LayerNorm.weight\n","\u001b[32m2024-04-24T18:29:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.output.LayerNorm.bias from model.bert.encoder.layer.9.attention.output.LayerNorm.bias\n","\u001b[32m2024-04-24T18:29:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.intermediate.dense.weight from model.bert.encoder.layer.9.intermediate.dense.weight\n","\u001b[32m2024-04-24T18:29:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.intermediate.dense.bias from model.bert.encoder.layer.9.intermediate.dense.bias\n","\u001b[32m2024-04-24T18:29:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.output.dense.weight from model.bert.encoder.layer.9.output.dense.weight\n","\u001b[32m2024-04-24T18:29:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.output.dense.bias from model.bert.encoder.layer.9.output.dense.bias\n","\u001b[32m2024-04-24T18:29:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.output.LayerNorm.weight from model.bert.encoder.layer.9.output.LayerNorm.weight\n","\u001b[32m2024-04-24T18:29:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.output.LayerNorm.bias from model.bert.encoder.layer.9.output.LayerNorm.bias\n","\u001b[32m2024-04-24T18:29:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.self.query.weight from model.bert.encoder.layer.10.attention.self.query.weight\n","\u001b[32m2024-04-24T18:29:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.self.query.bias from model.bert.encoder.layer.10.attention.self.query.bias\n","\u001b[32m2024-04-24T18:29:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.self.key.weight from model.bert.encoder.layer.10.attention.self.key.weight\n","\u001b[32m2024-04-24T18:29:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.self.key.bias from model.bert.encoder.layer.10.attention.self.key.bias\n","\u001b[32m2024-04-24T18:29:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.self.value.weight from model.bert.encoder.layer.10.attention.self.value.weight\n","\u001b[32m2024-04-24T18:29:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.self.value.bias from model.bert.encoder.layer.10.attention.self.value.bias\n","\u001b[32m2024-04-24T18:29:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.output.dense.weight from model.bert.encoder.layer.10.attention.output.dense.weight\n","\u001b[32m2024-04-24T18:29:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.output.dense.bias from model.bert.encoder.layer.10.attention.output.dense.bias\n","\u001b[32m2024-04-24T18:29:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.output.LayerNorm.weight from model.bert.encoder.layer.10.attention.output.LayerNorm.weight\n","\u001b[32m2024-04-24T18:29:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.output.LayerNorm.bias from model.bert.encoder.layer.10.attention.output.LayerNorm.bias\n","\u001b[32m2024-04-24T18:29:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.intermediate.dense.weight from model.bert.encoder.layer.10.intermediate.dense.weight\n","\u001b[32m2024-04-24T18:29:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.intermediate.dense.bias from model.bert.encoder.layer.10.intermediate.dense.bias\n","\u001b[32m2024-04-24T18:29:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.output.dense.weight from model.bert.encoder.layer.10.output.dense.weight\n","\u001b[32m2024-04-24T18:29:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.output.dense.bias from model.bert.encoder.layer.10.output.dense.bias\n","\u001b[32m2024-04-24T18:29:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.output.LayerNorm.weight from model.bert.encoder.layer.10.output.LayerNorm.weight\n","\u001b[32m2024-04-24T18:29:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.output.LayerNorm.bias from model.bert.encoder.layer.10.output.LayerNorm.bias\n","\u001b[32m2024-04-24T18:29:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.self.query.weight from model.bert.encoder.layer.11.attention.self.query.weight\n","\u001b[32m2024-04-24T18:29:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.self.query.bias from model.bert.encoder.layer.11.attention.self.query.bias\n","\u001b[32m2024-04-24T18:29:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.self.key.weight from model.bert.encoder.layer.11.attention.self.key.weight\n","\u001b[32m2024-04-24T18:29:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.self.key.bias from model.bert.encoder.layer.11.attention.self.key.bias\n","\u001b[32m2024-04-24T18:29:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.self.value.weight from model.bert.encoder.layer.11.attention.self.value.weight\n","\u001b[32m2024-04-24T18:29:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.self.value.bias from model.bert.encoder.layer.11.attention.self.value.bias\n","\u001b[32m2024-04-24T18:29:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.output.dense.weight from model.bert.encoder.layer.11.attention.output.dense.weight\n","\u001b[32m2024-04-24T18:29:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.output.dense.bias from model.bert.encoder.layer.11.attention.output.dense.bias\n","\u001b[32m2024-04-24T18:29:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.output.LayerNorm.weight from model.bert.encoder.layer.11.attention.output.LayerNorm.weight\n","\u001b[32m2024-04-24T18:29:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.output.LayerNorm.bias from model.bert.encoder.layer.11.attention.output.LayerNorm.bias\n","\u001b[32m2024-04-24T18:29:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.intermediate.dense.weight from model.bert.encoder.layer.11.intermediate.dense.weight\n","\u001b[32m2024-04-24T18:29:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.intermediate.dense.bias from model.bert.encoder.layer.11.intermediate.dense.bias\n","\u001b[32m2024-04-24T18:29:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.output.dense.weight from model.bert.encoder.layer.11.output.dense.weight\n","\u001b[32m2024-04-24T18:29:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.output.dense.bias from model.bert.encoder.layer.11.output.dense.bias\n","\u001b[32m2024-04-24T18:29:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.output.LayerNorm.weight from model.bert.encoder.layer.11.output.LayerNorm.weight\n","\u001b[32m2024-04-24T18:29:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.output.LayerNorm.bias from model.bert.encoder.layer.11.output.LayerNorm.bias\n","\u001b[32m2024-04-24T18:29:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.pooler.dense.weight from model.bert.pooler.dense.weight\n","\u001b[32m2024-04-24T18:29:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.pooler.dense.bias from model.bert.pooler.dense.bias\n","\u001b[32m2024-04-24T18:29:09 | mmf.utils.checkpoint: \u001b[0mPretrained model loaded\n","\u001b[32m2024-04-24T18:29:09 | mmf.utils.checkpoint: \u001b[0mCheckpoint loaded.\n","\u001b[32m2024-04-24T18:29:09 | mmf.utils.checkpoint: \u001b[0mCurrent num updates: 0\n","\u001b[32m2024-04-24T18:29:09 | mmf.utils.checkpoint: \u001b[0mCurrent iteration: 0\n","\u001b[32m2024-04-24T18:29:09 | mmf.utils.checkpoint: \u001b[0mCurrent epoch: 0\n","\u001b[32m2024-04-24T18:29:09 | mmf.trainers.mmf_trainer: \u001b[0m===== Model =====\n","\u001b[32m2024-04-24T18:29:09 | mmf.trainers.mmf_trainer: \u001b[0mVisualBERT(\n","  (model): VisualBERTForClassification(\n","    (bert): VisualBERTBase(\n","      (embeddings): BertVisioLinguisticEmbeddings(\n","        (word_embeddings): Embedding(30522, 768, padding_idx=0)\n","        (position_embeddings): Embedding(512, 768)\n","        (token_type_embeddings): Embedding(2, 768)\n","        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","        (token_type_embeddings_visual): Embedding(2, 768)\n","        (position_embeddings_visual): Embedding(512, 768)\n","        (projection): Linear(in_features=2048, out_features=768, bias=True)\n","      )\n","      (encoder): BertEncoderJit(\n","        (layer): ModuleList(\n","          (0-11): 12 x BertLayerJit(\n","            (attention): BertAttentionJit(\n","              (self): BertSelfAttentionJit(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","              (intermediate_act_fn): GELUActivation()\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","        )\n","      )\n","      (pooler): BertPooler(\n","        (dense): Linear(in_features=768, out_features=768, bias=True)\n","        (activation): Tanh()\n","      )\n","    )\n","    (dropout): Dropout(p=0.1, inplace=False)\n","    (classifier): Sequential(\n","      (0): BertPredictionHeadTransform(\n","        (dense): Linear(in_features=768, out_features=768, bias=True)\n","        (transform_act_fn): GELUActivation()\n","        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      )\n","      (1): Linear(in_features=768, out_features=2, bias=True)\n","    )\n","  )\n","  (losses): Losses(\n","    (losses): ModuleList(\n","      (0): MMFLoss(\n","        (loss_criterion): CrossEntropyLoss(\n","          (loss_fn): CrossEntropyLoss()\n","        )\n","      )\n","    )\n","  )\n",")\n","\u001b[32m2024-04-24T18:29:09 | mmf.utils.general: \u001b[0mTotal Parameters: 112044290. Trained Parameters: 112044290\n","\u001b[32m2024-04-24T18:29:09 | mmf.trainers.core.training_loop: \u001b[0mStarting training...\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2024-04-24T18:29:09 | py.warnings: \u001b[0m/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n","  self.pid = os.fork()\n","\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2024-04-24T18:29:09 | py.warnings: \u001b[0m/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n","  self.pid = os.fork()\n","\n","\u001b[32m2024-04-24T18:33:11 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n","\u001b[32m2024-04-24T18:33:11 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2024-04-24T18:33:13 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2024-04-24T18:33:18 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2024-04-24T18:33:18 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 200/1500, train/hateful_memes/cross_entropy: 0.6298, train/hateful_memes/cross_entropy/avg: 0.6298, train/total_loss: 0.6298, train/total_loss/avg: 0.6298, max mem: 14320.0, experiment: run, epoch: 1, num_updates: 200, iterations: 200, max_updates: 1500, lr: 0.00001, ups: 0.80, time: 04m 09s 155ms, time_since_start: 04m 28s 894ms, eta: 28m 12s 391ms\n","\u001b[32m2024-04-24T18:33:18 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n","\u001b[32m2024-04-24T18:33:18 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n","\u001b[32m2024-04-24T18:33:23 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished evaluation inference. Loaded 9\n","\u001b[32m2024-04-24T18:33:23 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n","\u001b[32m2024-04-24T18:33:23 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2024-04-24T18:33:25 | mmf.utils.checkpoint: \u001b[0mSaving best checkpoint\n","\u001b[32m2024-04-24T18:33:31 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2024-04-24T18:33:36 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2024-04-24T18:33:36 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 200/1500, val/hateful_memes/cross_entropy: 0.6720, val/total_loss: 0.6720, val/hateful_memes/accuracy: 0.6407, val/hateful_memes/binary_f1: 0.1709, val/hateful_memes/roc_auc: 0.5684, num_updates: 200, epoch: 1, iterations: 200, max_updates: 1500, val_time: 17s 199ms, best_update: 200, best_iteration: 200, best_val/hateful_memes/roc_auc: 0.568353\n","\u001b[32m2024-04-24T18:37:40 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n","\u001b[32m2024-04-24T18:37:40 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2024-04-24T18:37:42 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2024-04-24T18:37:47 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2024-04-24T18:37:47 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 400/1500, train/hateful_memes/cross_entropy: 0.6298, train/hateful_memes/cross_entropy/avg: 0.6367, train/total_loss: 0.6298, train/total_loss/avg: 0.6367, max mem: 14369.0, experiment: run, epoch: 1, num_updates: 400, iterations: 400, max_updates: 1500, lr: 0.00001, ups: 0.80, time: 04m 11s 861ms, time_since_start: 08m 57s 984ms, eta: 24m 07s 573ms\n","\u001b[32m2024-04-24T18:37:47 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n","\u001b[32m2024-04-24T18:37:47 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2024-04-24T18:37:47 | py.warnings: \u001b[0m/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n","  self.pid = os.fork()\n","\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2024-04-24T18:37:47 | py.warnings: \u001b[0m/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n","  self.pid = os.fork()\n","\n","\u001b[32m2024-04-24T18:37:52 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished evaluation inference. Loaded 9\n","\u001b[32m2024-04-24T18:37:52 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n","\u001b[32m2024-04-24T18:37:52 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2024-04-24T18:37:54 | mmf.utils.checkpoint: \u001b[0mSaving best checkpoint\n","\u001b[32m2024-04-24T18:38:00 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2024-04-24T18:38:05 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2024-04-24T18:38:05 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 400/1500, val/hateful_memes/cross_entropy: 0.7456, val/total_loss: 0.7456, val/hateful_memes/accuracy: 0.5759, val/hateful_memes/binary_f1: 0.4687, val/hateful_memes/roc_auc: 0.5908, num_updates: 400, epoch: 1, iterations: 400, max_updates: 1500, val_time: 17s 120ms, best_update: 400, best_iteration: 400, best_val/hateful_memes/roc_auc: 0.590824\n","\u001b[32m2024-04-24T18:42:05 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n","\u001b[32m2024-04-24T18:42:05 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2024-04-24T18:42:08 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2024-04-24T18:42:11 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2024-04-24T18:42:11 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 600/1500, train/hateful_memes/cross_entropy: 0.6298, train/hateful_memes/cross_entropy/avg: 0.5306, train/total_loss: 0.6298, train/total_loss/avg: 0.5306, max mem: 14369.0, experiment: run, epoch: 2, num_updates: 600, iterations: 600, max_updates: 1500, lr: 0.00002, ups: 0.81, time: 04m 06s 023ms, time_since_start: 13m 21s 130ms, eta: 19m 16s 924ms\n","\u001b[32m2024-04-24T18:42:11 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n","\u001b[32m2024-04-24T18:42:11 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2024-04-24T18:42:11 | py.warnings: \u001b[0m/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n","  self.pid = os.fork()\n","\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2024-04-24T18:42:11 | py.warnings: \u001b[0m/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n","  self.pid = os.fork()\n","\n","\u001b[32m2024-04-24T18:42:15 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished evaluation inference. Loaded 9\n","\u001b[32m2024-04-24T18:42:15 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n","\u001b[32m2024-04-24T18:42:15 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2024-04-24T18:42:18 | mmf.utils.checkpoint: \u001b[0mSaving best checkpoint\n","\u001b[32m2024-04-24T18:42:23 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2024-04-24T18:42:28 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2024-04-24T18:42:28 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 600/1500, val/hateful_memes/cross_entropy: 0.8993, val/total_loss: 0.8993, val/hateful_memes/accuracy: 0.6000, val/hateful_memes/binary_f1: 0.4033, val/hateful_memes/roc_auc: 0.6093, num_updates: 600, epoch: 2, iterations: 600, max_updates: 1500, val_time: 17s 414ms, best_update: 600, best_iteration: 600, best_val/hateful_memes/roc_auc: 0.609279\n","\u001b[32m2024-04-24T18:46:29 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n","\u001b[32m2024-04-24T18:46:29 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2024-04-24T18:46:31 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2024-04-24T18:46:37 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2024-04-24T18:46:37 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 800/1500, train/hateful_memes/cross_entropy: 0.3462, train/hateful_memes/cross_entropy/avg: 0.4845, train/total_loss: 0.3462, train/total_loss/avg: 0.4845, max mem: 14369.0, experiment: run, epoch: 2, num_updates: 800, iterations: 800, max_updates: 1500, lr: 0.00002, ups: 0.80, time: 04m 09s 318ms, time_since_start: 17m 47s 865ms, eta: 15m 11s 882ms\n","\u001b[32m2024-04-24T18:46:37 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n","\u001b[32m2024-04-24T18:46:37 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2024-04-24T18:46:37 | py.warnings: \u001b[0m/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n","  self.pid = os.fork()\n","\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2024-04-24T18:46:37 | py.warnings: \u001b[0m/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n","  self.pid = os.fork()\n","\n","\u001b[32m2024-04-24T18:46:42 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished evaluation inference. Loaded 9\n","\u001b[32m2024-04-24T18:46:42 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n","\u001b[32m2024-04-24T18:46:42 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2024-04-24T18:46:44 | mmf.utils.checkpoint: \u001b[0mSaving best checkpoint\n","\u001b[32m2024-04-24T18:46:48 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2024-04-24T18:46:55 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2024-04-24T18:46:55 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 800/1500, val/hateful_memes/cross_entropy: 0.9305, val/total_loss: 0.9305, val/hateful_memes/accuracy: 0.6537, val/hateful_memes/binary_f1: 0.4246, val/hateful_memes/roc_auc: 0.6383, num_updates: 800, epoch: 2, iterations: 800, max_updates: 1500, val_time: 17s 993ms, best_update: 800, best_iteration: 800, best_val/hateful_memes/roc_auc: 0.638309\n","\u001b[32m2024-04-24T18:50:57 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n","\u001b[32m2024-04-24T18:50:57 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2024-04-24T18:50:59 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2024-04-24T18:51:02 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2024-04-24T18:51:02 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1000/1500, train/hateful_memes/cross_entropy: 0.3462, train/hateful_memes/cross_entropy/avg: 0.4069, train/total_loss: 0.3462, train/total_loss/avg: 0.4069, max mem: 14369.0, experiment: run, epoch: 3, num_updates: 1000, iterations: 1000, max_updates: 1500, lr: 0.00003, ups: 0.81, time: 04m 07s 088ms, time_since_start: 22m 12s 950ms, eta: 10m 45s 519ms\n","\u001b[32m2024-04-24T18:51:02 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n","\u001b[32m2024-04-24T18:51:02 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2024-04-24T18:51:02 | py.warnings: \u001b[0m/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n","  self.pid = os.fork()\n","\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2024-04-24T18:51:02 | py.warnings: \u001b[0m/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n","  self.pid = os.fork()\n","\n","\u001b[32m2024-04-24T18:51:07 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished evaluation inference. Loaded 9\n","\u001b[32m2024-04-24T18:51:07 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n","\u001b[32m2024-04-24T18:51:07 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2024-04-24T18:51:09 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2024-04-24T18:51:14 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2024-04-24T18:51:14 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1000/1500, val/hateful_memes/cross_entropy: 1.2333, val/total_loss: 1.2333, val/hateful_memes/accuracy: 0.6222, val/hateful_memes/binary_f1: 0.4104, val/hateful_memes/roc_auc: 0.6158, num_updates: 1000, epoch: 3, iterations: 1000, max_updates: 1500, val_time: 11s 319ms, best_update: 800, best_iteration: 800, best_val/hateful_memes/roc_auc: 0.638309\n","\u001b[32m2024-04-24T18:55:16 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n","\u001b[32m2024-04-24T18:55:16 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2024-04-24T18:55:18 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2024-04-24T18:55:24 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2024-04-24T18:55:24 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1200/1500, train/hateful_memes/cross_entropy: 0.3183, train/hateful_memes/cross_entropy/avg: 0.3436, train/total_loss: 0.3183, train/total_loss/avg: 0.3436, max mem: 14369.0, experiment: run, epoch: 3, num_updates: 1200, iterations: 1200, max_updates: 1500, lr: 0.00003, ups: 0.80, time: 04m 09s 867ms, time_since_start: 26m 34s 139ms, eta: 06m 31s 666ms\n","\u001b[32m2024-04-24T18:55:24 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n","\u001b[32m2024-04-24T18:55:24 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2024-04-24T18:55:24 | py.warnings: \u001b[0m/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n","  self.pid = os.fork()\n","\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2024-04-24T18:55:24 | py.warnings: \u001b[0m/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n","  self.pid = os.fork()\n","\n","\u001b[32m2024-04-24T18:55:28 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished evaluation inference. Loaded 9\n","\u001b[32m2024-04-24T18:55:28 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n","\u001b[32m2024-04-24T18:55:28 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2024-04-24T18:55:31 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2024-04-24T18:55:36 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2024-04-24T18:55:36 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1200/1500, val/hateful_memes/cross_entropy: 1.4639, val/total_loss: 1.4639, val/hateful_memes/accuracy: 0.6185, val/hateful_memes/binary_f1: 0.5142, val/hateful_memes/roc_auc: 0.6367, num_updates: 1200, epoch: 3, iterations: 1200, max_updates: 1500, val_time: 12s 611ms, best_update: 800, best_iteration: 800, best_val/hateful_memes/roc_auc: 0.638309\n","\u001b[32m2024-04-24T18:59:37 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n","\u001b[32m2024-04-24T18:59:37 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2024-04-24T18:59:40 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2024-04-24T18:59:44 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2024-04-24T18:59:44 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1400/1500, train/hateful_memes/cross_entropy: 0.3183, train/hateful_memes/cross_entropy/avg: 0.3035, train/total_loss: 0.3183, train/total_loss/avg: 0.3035, max mem: 14369.0, experiment: run, epoch: 4, num_updates: 1400, iterations: 1400, max_updates: 1500, lr: 0.00003, ups: 0.81, time: 04m 07s 538ms, time_since_start: 30m 54s 291ms, eta: 02m 09s 338ms\n","\u001b[32m2024-04-24T18:59:44 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n","\u001b[32m2024-04-24T18:59:44 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2024-04-24T18:59:44 | py.warnings: \u001b[0m/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n","  self.pid = os.fork()\n","\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2024-04-24T18:59:44 | py.warnings: \u001b[0m/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n","  self.pid = os.fork()\n","\n","\u001b[32m2024-04-24T18:59:48 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished evaluation inference. Loaded 9\n","\u001b[32m2024-04-24T18:59:48 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n","\u001b[32m2024-04-24T18:59:48 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2024-04-24T18:59:51 | mmf.utils.checkpoint: \u001b[0mSaving best checkpoint\n","\u001b[32m2024-04-24T18:59:56 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2024-04-24T19:00:01 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2024-04-24T19:00:01 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1400/1500, val/hateful_memes/cross_entropy: 1.4819, val/total_loss: 1.4819, val/hateful_memes/accuracy: 0.6648, val/hateful_memes/binary_f1: 0.4361, val/hateful_memes/roc_auc: 0.6624, num_updates: 1400, epoch: 4, iterations: 1400, max_updates: 1500, val_time: 17s 142ms, best_update: 1400, best_iteration: 1400, best_val/hateful_memes/roc_auc: 0.662397\n","\u001b[32m2024-04-24T19:02:02 | mmf.trainers.core.training_loop: \u001b[0mStepping into final validation check\n","\u001b[32m2024-04-24T19:02:02 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2024-04-24T19:02:02 | py.warnings: \u001b[0m/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n","  self.pid = os.fork()\n","\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2024-04-24T19:02:02 | py.warnings: \u001b[0m/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n","  self.pid = os.fork()\n","\n","\u001b[32m2024-04-24T19:02:07 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished evaluation inference. Loaded 9\n","\u001b[32m2024-04-24T19:02:07 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n","\u001b[32m2024-04-24T19:02:07 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1500/1500, val/hateful_memes/cross_entropy: 1.7199, val/total_loss: 1.7199, val/hateful_memes/accuracy: 0.6500, val/hateful_memes/binary_f1: 0.3549, val/hateful_memes/roc_auc: 0.6278, num_updates: 1500, epoch: 4, iterations: 1500, max_updates: 1500, val_time: 02m 23s 046ms, best_update: 1400, best_iteration: 1400, best_val/hateful_memes/roc_auc: 0.662397\n","\u001b[32m2024-04-24T19:02:07 | mmf.utils.checkpoint: \u001b[0mRestoring checkpoint\n","\u001b[32m2024-04-24T19:02:07 | mmf.utils.checkpoint: \u001b[0mLoading checkpoint\n","\u001b[32m2024-04-24T19:02:08 | mmf.utils.checkpoint: \u001b[0mCheckpoint loaded.\n","\u001b[32m2024-04-24T19:02:08 | mmf.utils.checkpoint: \u001b[0mCurrent num updates: 1400\n","\u001b[32m2024-04-24T19:02:08 | mmf.utils.checkpoint: \u001b[0mCurrent iteration: 1400\n","\u001b[32m2024-04-24T19:02:08 | mmf.utils.checkpoint: \u001b[0mCurrent epoch: 4\n","\u001b[32m2024-04-24T19:02:09 | mmf.trainers.mmf_trainer: \u001b[0mStarting inference on val set\n","\u001b[32m2024-04-24T19:02:09 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n","  0% 0/9 [00:00<?, ?it/s]\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2024-04-24T19:02:09 | py.warnings: \u001b[0m/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n","  self.pid = os.fork()\n","\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2024-04-24T19:02:09 | py.warnings: \u001b[0m/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n","  self.pid = os.fork()\n","\n","100% 9/9 [00:04<00:00,  2.07it/s]\n","\u001b[32m2024-04-24T19:02:13 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished evaluation inference. Loaded 9\n","\u001b[32m2024-04-24T19:02:13 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n","\u001b[32m2024-04-24T19:02:13 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1400/1500, val/hateful_memes/cross_entropy: 1.4819, val/total_loss: 1.4819, val/hateful_memes/accuracy: 0.6648, val/hateful_memes/binary_f1: 0.4361, val/hateful_memes/roc_auc: 0.6624\n","\u001b[32m2024-04-24T19:02:13 | mmf.trainers.callbacks.logistics: \u001b[0mFinished run in 33m 23s 772ms\n"]}]},{"cell_type":"markdown","source":["### Dataset: Augmented with combined transformations"],"metadata":{"id":"C_msVMRfv9w3"}},{"cell_type":"code","source":["!mmf_run config=\"./configs/defaults.yaml\" \\\n","  model=visual_bert \\\n","  dataset=hateful_memes \\\n","  run_type=train_val \\\n","  training.log_interval=200 \\\n","  training.batch_size=64 \\\n","  training.evaluation_interval=200 \\\n","  training.tensorboard=True \\\n","  training.checkpoint_interval=200 \\\n","  checkpoint.resume_pretrained=True \\\n","  checkpoint.resume_file=\"/content/drive/MyDrive/hateful-memes/Deep Learning Group Project/visualBERT_vizwiz_04212024_pretrained.ckpt\" \\\n","  dataset_config.hateful_memes.annotations.train[0]=\"hateful_memes/defaults/annotations/train_dev_transform_all_combined.jsonl\" \\\n","  dataset_config.hateful_memes.annotations.val[0]=\"hateful_memes/defaults/annotations/dev_unseen.jsonl\" \\\n","  dataset_config.hateful_memes.annotations.test[0]=\"hateful_memes/defaults/annotations/test_unseen.jsonl\""],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qm8yfIwawA1U","executionInfo":{"status":"ok","timestamp":1713929476447,"user_tz":420,"elapsed":1860556,"user":{"displayName":"Aneri Shah","userId":"14009776940163552827"}},"outputId":"ad396e89-216f-4364-96b5-d1d6b3692923"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["2024-04-24 02:58:01.974874: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","2024-04-24 02:58:01.974946: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","2024-04-24 02:58:01.976370: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","2024-04-24 02:58:03.029554: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","2024-04-24 02:58:07.448445: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:47] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n","/usr/local/lib/python3.10/dist-packages/transformers/deepspeed.py:23: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/omegaconf/grammar_visitor.py:253: UserWarning: In the sequence `MMF_USER_DIR,` some elements are missing: please replace them with empty quoted strings. See https://github.com/omry/omegaconf/issues/572 for details.\n","  warnings.warn(\n","\u001b[32m2024-04-24T02:58:12 | mmf.utils.configuration: \u001b[0mOverriding option config to ./configs/defaults.yaml\n","\u001b[32m2024-04-24T02:58:12 | mmf.utils.configuration: \u001b[0mOverriding option model to visual_bert\n","\u001b[32m2024-04-24T02:58:12 | mmf.utils.configuration: \u001b[0mOverriding option datasets to hateful_memes\n","\u001b[32m2024-04-24T02:58:12 | mmf.utils.configuration: \u001b[0mOverriding option run_type to train_val\n","\u001b[32m2024-04-24T02:58:12 | mmf.utils.configuration: \u001b[0mOverriding option training.log_interval to 200\n","\u001b[32m2024-04-24T02:58:12 | mmf.utils.configuration: \u001b[0mOverriding option training.batch_size to 64\n","\u001b[32m2024-04-24T02:58:12 | mmf.utils.configuration: \u001b[0mOverriding option training.evaluation_interval to 200\n","\u001b[32m2024-04-24T02:58:12 | mmf.utils.configuration: \u001b[0mOverriding option training.tensorboard to True\n","\u001b[32m2024-04-24T02:58:12 | mmf.utils.configuration: \u001b[0mOverriding option training.checkpoint_interval to 200\n","\u001b[32m2024-04-24T02:58:12 | mmf.utils.configuration: \u001b[0mOverriding option checkpoint.resume_pretrained to True\n","\u001b[32m2024-04-24T02:58:12 | mmf.utils.configuration: \u001b[0mOverriding option checkpoint.resume_file to /content/drive/MyDrive/hateful-memes/Deep Learning Group Project/visualBERT_vizwiz_04212024_pretrained.ckpt\n","\u001b[32m2024-04-24T02:58:12 | mmf.utils.configuration: \u001b[0mOverriding option dataset_config.hateful_memes.annotations.train[0] to hateful_memes/defaults/annotations/train_dev_transform_all_combined.jsonl\n","\u001b[32m2024-04-24T02:58:12 | mmf.utils.configuration: \u001b[0mOverriding option dataset_config.hateful_memes.annotations.val[0] to hateful_memes/defaults/annotations/dev_unseen.jsonl\n","\u001b[32m2024-04-24T02:58:12 | mmf.utils.configuration: \u001b[0mOverriding option dataset_config.hateful_memes.annotations.test[0] to hateful_memes/defaults/annotations/test_unseen.jsonl\n","/usr/local/lib/python3.10/dist-packages/omegaconf/grammar_visitor.py:253: UserWarning: In the sequence `MMF_LOG_DIR,` some elements are missing: please replace them with empty quoted strings. See https://github.com/omry/omegaconf/issues/572 for details.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/omegaconf/grammar_visitor.py:253: UserWarning: In the sequence `MMF_REPORT_DIR,` some elements are missing: please replace them with empty quoted strings. See https://github.com/omry/omegaconf/issues/572 for details.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/omegaconf/grammar_visitor.py:253: UserWarning: In the sequence `MMF_TENSORBOARD_LOGDIR,` some elements are missing: please replace them with empty quoted strings. See https://github.com/omry/omegaconf/issues/572 for details.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/omegaconf/grammar_visitor.py:253: UserWarning: In the sequence `MMF_WANDB_LOGDIR,` some elements are missing: please replace them with empty quoted strings. See https://github.com/omry/omegaconf/issues/572 for details.\n","  warnings.warn(\n","\u001b[32m2024-04-24T02:58:12 | mmf: \u001b[0mLogging to: ./save/train.log\n","\u001b[32m2024-04-24T02:58:12 | mmf_cli.run: \u001b[0mNamespace(config_override=None, local_rank=None, opts=['config=./configs/defaults.yaml', 'model=visual_bert', 'dataset=hateful_memes', 'run_type=train_val', 'training.log_interval=200', 'training.batch_size=64', 'training.evaluation_interval=200', 'training.tensorboard=True', 'training.checkpoint_interval=200', 'checkpoint.resume_pretrained=True', 'checkpoint.resume_file=/content/drive/MyDrive/hateful-memes/Deep Learning Group Project/visualBERT_vizwiz_04212024_pretrained.ckpt', 'dataset_config.hateful_memes.annotations.train[0]=hateful_memes/defaults/annotations/train_dev_transform_all_combined.jsonl', 'dataset_config.hateful_memes.annotations.val[0]=hateful_memes/defaults/annotations/dev_unseen.jsonl', 'dataset_config.hateful_memes.annotations.test[0]=hateful_memes/defaults/annotations/test_unseen.jsonl'])\n","\u001b[32m2024-04-24T02:58:12 | mmf_cli.run: \u001b[0mTorch version: 2.2.0+cu121\n","\u001b[32m2024-04-24T02:58:12 | mmf.utils.general: \u001b[0mCUDA Device 0 is: NVIDIA L4\n","\u001b[32m2024-04-24T02:58:12 | mmf_cli.run: \u001b[0mUsing seed 12441111\n","\u001b[32m2024-04-24T02:58:12 | mmf.trainers.mmf_trainer: \u001b[0mLoading datasets\n","loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--bert-base-uncased/snapshots/86b5e0934494bd15c9632b12f734a8a67f723594/config.json\n","Model config BertConfig {\n","  \"_name_or_path\": \"bert-base-uncased\",\n","  \"architectures\": [\n","    \"BertForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.39.2\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n","loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--bert-base-uncased/snapshots/86b5e0934494bd15c9632b12f734a8a67f723594/vocab.txt\n","loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--bert-base-uncased/snapshots/86b5e0934494bd15c9632b12f734a8a67f723594/tokenizer.json\n","loading file added_tokens.json from cache at None\n","loading file special_tokens_map.json from cache at None\n","loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--bert-base-uncased/snapshots/86b5e0934494bd15c9632b12f734a8a67f723594/tokenizer_config.json\n","loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--bert-base-uncased/snapshots/86b5e0934494bd15c9632b12f734a8a67f723594/config.json\n","Model config BertConfig {\n","  \"_name_or_path\": \"bert-base-uncased\",\n","  \"architectures\": [\n","    \"BertForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.39.2\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n","\u001b[32m2024-04-24T02:58:12 | mmf.datasets.multi_datamodule: \u001b[0mMultitasking disabled by default for single dataset training\n","\u001b[32m2024-04-24T02:58:12 | mmf.datasets.multi_datamodule: \u001b[0mMultitasking disabled by default for single dataset training\n","\u001b[32m2024-04-24T02:58:12 | mmf.datasets.multi_datamodule: \u001b[0mMultitasking disabled by default for single dataset training\n","\u001b[32m2024-04-24T02:58:12 | mmf.trainers.mmf_trainer: \u001b[0mLoading model\n","Model config BertConfig {\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bert_model_name\": \"bert-base-uncased\",\n","  \"bypass_transformer\": false,\n","  \"classifier_dropout\": null,\n","  \"embedding_strategy\": \"plain\",\n","  \"finetune_lr_multiplier\": 1,\n","  \"freeze_base\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"losses\": [\n","    \"cross_entropy\"\n","  ],\n","  \"max_position_embeddings\": 512,\n","  \"model\": \"visual_bert\",\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"pooler_strategy\": \"default\",\n","  \"position_embedding_type\": \"absolute\",\n","  \"random_initialize\": false,\n","  \"special_visual_initialize\": true,\n","  \"training_head_type\": \"classification\",\n","  \"transformers_version\": \"4.39.2\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"visual_embedding_dim\": 2048,\n","  \"vocab_size\": 30522,\n","  \"zerobias\": false\n","}\n","\n","loading weights file model.safetensors from cache at /root/.cache/torch/mmf/distributed_-1/models--bert-base-uncased/snapshots/86b5e0934494bd15c9632b12f734a8a67f723594/model.safetensors\n","Some weights of the model checkpoint at bert-base-uncased were not used when initializing VisualBERTBase: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n","- This IS expected if you are initializing VisualBERTBase from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing VisualBERTBase from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of VisualBERTBase were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['bert.embeddings.position_embeddings_visual.weight', 'bert.embeddings.projection.bias', 'bert.embeddings.projection.weight', 'bert.embeddings.token_type_embeddings_visual.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","\u001b[32m2024-04-24T02:58:13 | mmf.trainers.mmf_trainer: \u001b[0mLoading optimizer\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2024-04-24T02:58:13 | py.warnings: \u001b[0m/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:457: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n","\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2024-04-24T02:58:13 | py.warnings: \u001b[0m/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:457: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n","\n","\u001b[32m2024-04-24T02:58:13 | mmf.trainers.mmf_trainer: \u001b[0mLoading metrics\n","\u001b[32m2024-04-24T02:58:13 | mmf.utils.checkpoint: \u001b[0mLoading checkpoint\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2024-04-24T02:58:15 | mmf: \u001b[0mKey data_parallel is not present in registry, returning default value of None\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2024-04-24T02:58:15 | mmf: \u001b[0mKey distributed is not present in registry, returning default value of None\n","\u001b[32m2024-04-24T02:58:15 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.word_embeddings.weight from model.bert.embeddings.word_embeddings.weight\n","\u001b[32m2024-04-24T02:58:15 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.position_embeddings.weight from model.bert.embeddings.position_embeddings.weight\n","\u001b[32m2024-04-24T02:58:15 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.token_type_embeddings.weight from model.bert.embeddings.token_type_embeddings.weight\n","\u001b[32m2024-04-24T02:58:15 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.LayerNorm.weight from model.bert.embeddings.LayerNorm.weight\n","\u001b[32m2024-04-24T02:58:15 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.LayerNorm.bias from model.bert.embeddings.LayerNorm.bias\n","\u001b[32m2024-04-24T02:58:15 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.token_type_embeddings_visual.weight from model.bert.embeddings.token_type_embeddings_visual.weight\n","\u001b[32m2024-04-24T02:58:15 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.position_embeddings_visual.weight from model.bert.embeddings.position_embeddings_visual.weight\n","\u001b[32m2024-04-24T02:58:15 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.projection.weight from model.bert.embeddings.projection.weight\n","\u001b[32m2024-04-24T02:58:15 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.projection.bias from model.bert.embeddings.projection.bias\n","\u001b[32m2024-04-24T02:58:15 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.self.query.weight from model.bert.encoder.layer.0.attention.self.query.weight\n","\u001b[32m2024-04-24T02:58:15 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.self.query.bias from model.bert.encoder.layer.0.attention.self.query.bias\n","\u001b[32m2024-04-24T02:58:15 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.self.key.weight from model.bert.encoder.layer.0.attention.self.key.weight\n","\u001b[32m2024-04-24T02:58:15 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.self.key.bias from model.bert.encoder.layer.0.attention.self.key.bias\n","\u001b[32m2024-04-24T02:58:15 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.self.value.weight from model.bert.encoder.layer.0.attention.self.value.weight\n","\u001b[32m2024-04-24T02:58:15 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.self.value.bias from model.bert.encoder.layer.0.attention.self.value.bias\n","\u001b[32m2024-04-24T02:58:15 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.output.dense.weight from model.bert.encoder.layer.0.attention.output.dense.weight\n","\u001b[32m2024-04-24T02:58:15 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.output.dense.bias from model.bert.encoder.layer.0.attention.output.dense.bias\n","\u001b[32m2024-04-24T02:58:15 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.output.LayerNorm.weight from model.bert.encoder.layer.0.attention.output.LayerNorm.weight\n","\u001b[32m2024-04-24T02:58:15 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.output.LayerNorm.bias from model.bert.encoder.layer.0.attention.output.LayerNorm.bias\n","\u001b[32m2024-04-24T02:58:15 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.intermediate.dense.weight from model.bert.encoder.layer.0.intermediate.dense.weight\n","\u001b[32m2024-04-24T02:58:15 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.intermediate.dense.bias from model.bert.encoder.layer.0.intermediate.dense.bias\n","\u001b[32m2024-04-24T02:58:15 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.output.dense.weight from model.bert.encoder.layer.0.output.dense.weight\n","\u001b[32m2024-04-24T02:58:15 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.output.dense.bias from model.bert.encoder.layer.0.output.dense.bias\n","\u001b[32m2024-04-24T02:58:15 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.output.LayerNorm.weight from model.bert.encoder.layer.0.output.LayerNorm.weight\n","\u001b[32m2024-04-24T02:58:15 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.output.LayerNorm.bias from model.bert.encoder.layer.0.output.LayerNorm.bias\n","\u001b[32m2024-04-24T02:58:15 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.self.query.weight from model.bert.encoder.layer.1.attention.self.query.weight\n","\u001b[32m2024-04-24T02:58:15 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.self.query.bias from model.bert.encoder.layer.1.attention.self.query.bias\n","\u001b[32m2024-04-24T02:58:15 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.self.key.weight from model.bert.encoder.layer.1.attention.self.key.weight\n","\u001b[32m2024-04-24T02:58:15 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.self.key.bias from model.bert.encoder.layer.1.attention.self.key.bias\n","\u001b[32m2024-04-24T02:58:15 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.self.value.weight from model.bert.encoder.layer.1.attention.self.value.weight\n","\u001b[32m2024-04-24T02:58:15 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.self.value.bias from model.bert.encoder.layer.1.attention.self.value.bias\n","\u001b[32m2024-04-24T02:58:15 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.output.dense.weight from model.bert.encoder.layer.1.attention.output.dense.weight\n","\u001b[32m2024-04-24T02:58:15 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.output.dense.bias from model.bert.encoder.layer.1.attention.output.dense.bias\n","\u001b[32m2024-04-24T02:58:15 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.output.LayerNorm.weight from model.bert.encoder.layer.1.attention.output.LayerNorm.weight\n","\u001b[32m2024-04-24T02:58:15 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.output.LayerNorm.bias from model.bert.encoder.layer.1.attention.output.LayerNorm.bias\n","\u001b[32m2024-04-24T02:58:15 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.intermediate.dense.weight from model.bert.encoder.layer.1.intermediate.dense.weight\n","\u001b[32m2024-04-24T02:58:15 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.intermediate.dense.bias from model.bert.encoder.layer.1.intermediate.dense.bias\n","\u001b[32m2024-04-24T02:58:15 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.output.dense.weight from model.bert.encoder.layer.1.output.dense.weight\n","\u001b[32m2024-04-24T02:58:15 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.output.dense.bias from model.bert.encoder.layer.1.output.dense.bias\n","\u001b[32m2024-04-24T02:58:15 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.output.LayerNorm.weight from model.bert.encoder.layer.1.output.LayerNorm.weight\n","\u001b[32m2024-04-24T02:58:15 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.output.LayerNorm.bias from model.bert.encoder.layer.1.output.LayerNorm.bias\n","\u001b[32m2024-04-24T02:58:15 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.self.query.weight from model.bert.encoder.layer.2.attention.self.query.weight\n","\u001b[32m2024-04-24T02:58:15 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.self.query.bias from model.bert.encoder.layer.2.attention.self.query.bias\n","\u001b[32m2024-04-24T02:58:15 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.self.key.weight from model.bert.encoder.layer.2.attention.self.key.weight\n","\u001b[32m2024-04-24T02:58:15 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.self.key.bias from model.bert.encoder.layer.2.attention.self.key.bias\n","\u001b[32m2024-04-24T02:58:15 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.self.value.weight from model.bert.encoder.layer.2.attention.self.value.weight\n","\u001b[32m2024-04-24T02:58:15 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.self.value.bias from model.bert.encoder.layer.2.attention.self.value.bias\n","\u001b[32m2024-04-24T02:58:15 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.output.dense.weight from model.bert.encoder.layer.2.attention.output.dense.weight\n","\u001b[32m2024-04-24T02:58:15 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.output.dense.bias from model.bert.encoder.layer.2.attention.output.dense.bias\n","\u001b[32m2024-04-24T02:58:15 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.output.LayerNorm.weight from model.bert.encoder.layer.2.attention.output.LayerNorm.weight\n","\u001b[32m2024-04-24T02:58:15 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.output.LayerNorm.bias from model.bert.encoder.layer.2.attention.output.LayerNorm.bias\n","\u001b[32m2024-04-24T02:58:15 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.intermediate.dense.weight from model.bert.encoder.layer.2.intermediate.dense.weight\n","\u001b[32m2024-04-24T02:58:15 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.intermediate.dense.bias from model.bert.encoder.layer.2.intermediate.dense.bias\n","\u001b[32m2024-04-24T02:58:15 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.output.dense.weight from model.bert.encoder.layer.2.output.dense.weight\n","\u001b[32m2024-04-24T02:58:15 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.output.dense.bias from model.bert.encoder.layer.2.output.dense.bias\n","\u001b[32m2024-04-24T02:58:15 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.output.LayerNorm.weight from model.bert.encoder.layer.2.output.LayerNorm.weight\n","\u001b[32m2024-04-24T02:58:15 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.output.LayerNorm.bias from model.bert.encoder.layer.2.output.LayerNorm.bias\n","\u001b[32m2024-04-24T02:58:15 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.self.query.weight from model.bert.encoder.layer.3.attention.self.query.weight\n","\u001b[32m2024-04-24T02:58:15 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.self.query.bias from model.bert.encoder.layer.3.attention.self.query.bias\n","\u001b[32m2024-04-24T02:58:15 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.self.key.weight from model.bert.encoder.layer.3.attention.self.key.weight\n","\u001b[32m2024-04-24T02:58:15 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.self.key.bias from model.bert.encoder.layer.3.attention.self.key.bias\n","\u001b[32m2024-04-24T02:58:15 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.self.value.weight from model.bert.encoder.layer.3.attention.self.value.weight\n","\u001b[32m2024-04-24T02:58:15 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.self.value.bias from model.bert.encoder.layer.3.attention.self.value.bias\n","\u001b[32m2024-04-24T02:58:15 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.output.dense.weight from model.bert.encoder.layer.3.attention.output.dense.weight\n","\u001b[32m2024-04-24T02:58:15 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.output.dense.bias from model.bert.encoder.layer.3.attention.output.dense.bias\n","\u001b[32m2024-04-24T02:58:15 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.output.LayerNorm.weight from model.bert.encoder.layer.3.attention.output.LayerNorm.weight\n","\u001b[32m2024-04-24T02:58:15 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.output.LayerNorm.bias from model.bert.encoder.layer.3.attention.output.LayerNorm.bias\n","\u001b[32m2024-04-24T02:58:15 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.intermediate.dense.weight from model.bert.encoder.layer.3.intermediate.dense.weight\n","\u001b[32m2024-04-24T02:58:15 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.intermediate.dense.bias from model.bert.encoder.layer.3.intermediate.dense.bias\n","\u001b[32m2024-04-24T02:58:15 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.output.dense.weight from model.bert.encoder.layer.3.output.dense.weight\n","\u001b[32m2024-04-24T02:58:15 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.output.dense.bias from model.bert.encoder.layer.3.output.dense.bias\n","\u001b[32m2024-04-24T02:58:15 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.output.LayerNorm.weight from model.bert.encoder.layer.3.output.LayerNorm.weight\n","\u001b[32m2024-04-24T02:58:15 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.output.LayerNorm.bias from model.bert.encoder.layer.3.output.LayerNorm.bias\n","\u001b[32m2024-04-24T02:58:15 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.self.query.weight from model.bert.encoder.layer.4.attention.self.query.weight\n","\u001b[32m2024-04-24T02:58:15 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.self.query.bias from model.bert.encoder.layer.4.attention.self.query.bias\n","\u001b[32m2024-04-24T02:58:15 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.self.key.weight from model.bert.encoder.layer.4.attention.self.key.weight\n","\u001b[32m2024-04-24T02:58:15 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.self.key.bias from model.bert.encoder.layer.4.attention.self.key.bias\n","\u001b[32m2024-04-24T02:58:15 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.self.value.weight from model.bert.encoder.layer.4.attention.self.value.weight\n","\u001b[32m2024-04-24T02:58:15 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.self.value.bias from model.bert.encoder.layer.4.attention.self.value.bias\n","\u001b[32m2024-04-24T02:58:15 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.output.dense.weight from model.bert.encoder.layer.4.attention.output.dense.weight\n","\u001b[32m2024-04-24T02:58:15 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.output.dense.bias from model.bert.encoder.layer.4.attention.output.dense.bias\n","\u001b[32m2024-04-24T02:58:15 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.output.LayerNorm.weight from model.bert.encoder.layer.4.attention.output.LayerNorm.weight\n","\u001b[32m2024-04-24T02:58:15 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.output.LayerNorm.bias from model.bert.encoder.layer.4.attention.output.LayerNorm.bias\n","\u001b[32m2024-04-24T02:58:15 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.intermediate.dense.weight from model.bert.encoder.layer.4.intermediate.dense.weight\n","\u001b[32m2024-04-24T02:58:15 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.intermediate.dense.bias from model.bert.encoder.layer.4.intermediate.dense.bias\n","\u001b[32m2024-04-24T02:58:15 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.output.dense.weight from model.bert.encoder.layer.4.output.dense.weight\n","\u001b[32m2024-04-24T02:58:15 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.output.dense.bias from model.bert.encoder.layer.4.output.dense.bias\n","\u001b[32m2024-04-24T02:58:15 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.output.LayerNorm.weight from model.bert.encoder.layer.4.output.LayerNorm.weight\n","\u001b[32m2024-04-24T02:58:15 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.output.LayerNorm.bias from model.bert.encoder.layer.4.output.LayerNorm.bias\n","\u001b[32m2024-04-24T02:58:15 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.self.query.weight from model.bert.encoder.layer.5.attention.self.query.weight\n","\u001b[32m2024-04-24T02:58:15 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.self.query.bias from model.bert.encoder.layer.5.attention.self.query.bias\n","\u001b[32m2024-04-24T02:58:15 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.self.key.weight from model.bert.encoder.layer.5.attention.self.key.weight\n","\u001b[32m2024-04-24T02:58:15 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.self.key.bias from model.bert.encoder.layer.5.attention.self.key.bias\n","\u001b[32m2024-04-24T02:58:15 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.self.value.weight from model.bert.encoder.layer.5.attention.self.value.weight\n","\u001b[32m2024-04-24T02:58:15 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.self.value.bias from model.bert.encoder.layer.5.attention.self.value.bias\n","\u001b[32m2024-04-24T02:58:15 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.output.dense.weight from model.bert.encoder.layer.5.attention.output.dense.weight\n","\u001b[32m2024-04-24T02:58:15 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.output.dense.bias from model.bert.encoder.layer.5.attention.output.dense.bias\n","\u001b[32m2024-04-24T02:58:15 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.output.LayerNorm.weight from model.bert.encoder.layer.5.attention.output.LayerNorm.weight\n","\u001b[32m2024-04-24T02:58:15 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.output.LayerNorm.bias from model.bert.encoder.layer.5.attention.output.LayerNorm.bias\n","\u001b[32m2024-04-24T02:58:15 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.intermediate.dense.weight from model.bert.encoder.layer.5.intermediate.dense.weight\n","\u001b[32m2024-04-24T02:58:15 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.intermediate.dense.bias from model.bert.encoder.layer.5.intermediate.dense.bias\n","\u001b[32m2024-04-24T02:58:15 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.output.dense.weight from model.bert.encoder.layer.5.output.dense.weight\n","\u001b[32m2024-04-24T02:58:15 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.output.dense.bias from model.bert.encoder.layer.5.output.dense.bias\n","\u001b[32m2024-04-24T02:58:15 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.output.LayerNorm.weight from model.bert.encoder.layer.5.output.LayerNorm.weight\n","\u001b[32m2024-04-24T02:58:15 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.output.LayerNorm.bias from model.bert.encoder.layer.5.output.LayerNorm.bias\n","\u001b[32m2024-04-24T02:58:15 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.self.query.weight from model.bert.encoder.layer.6.attention.self.query.weight\n","\u001b[32m2024-04-24T02:58:15 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.self.query.bias from model.bert.encoder.layer.6.attention.self.query.bias\n","\u001b[32m2024-04-24T02:58:15 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.self.key.weight from model.bert.encoder.layer.6.attention.self.key.weight\n","\u001b[32m2024-04-24T02:58:15 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.self.key.bias from model.bert.encoder.layer.6.attention.self.key.bias\n","\u001b[32m2024-04-24T02:58:15 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.self.value.weight from model.bert.encoder.layer.6.attention.self.value.weight\n","\u001b[32m2024-04-24T02:58:15 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.self.value.bias from model.bert.encoder.layer.6.attention.self.value.bias\n","\u001b[32m2024-04-24T02:58:15 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.output.dense.weight from model.bert.encoder.layer.6.attention.output.dense.weight\n","\u001b[32m2024-04-24T02:58:15 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.output.dense.bias from model.bert.encoder.layer.6.attention.output.dense.bias\n","\u001b[32m2024-04-24T02:58:15 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.output.LayerNorm.weight from model.bert.encoder.layer.6.attention.output.LayerNorm.weight\n","\u001b[32m2024-04-24T02:58:15 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.output.LayerNorm.bias from model.bert.encoder.layer.6.attention.output.LayerNorm.bias\n","\u001b[32m2024-04-24T02:58:15 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.intermediate.dense.weight from model.bert.encoder.layer.6.intermediate.dense.weight\n","\u001b[32m2024-04-24T02:58:15 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.intermediate.dense.bias from model.bert.encoder.layer.6.intermediate.dense.bias\n","\u001b[32m2024-04-24T02:58:15 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.output.dense.weight from model.bert.encoder.layer.6.output.dense.weight\n","\u001b[32m2024-04-24T02:58:15 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.output.dense.bias from model.bert.encoder.layer.6.output.dense.bias\n","\u001b[32m2024-04-24T02:58:15 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.output.LayerNorm.weight from model.bert.encoder.layer.6.output.LayerNorm.weight\n","\u001b[32m2024-04-24T02:58:15 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.output.LayerNorm.bias from model.bert.encoder.layer.6.output.LayerNorm.bias\n","\u001b[32m2024-04-24T02:58:15 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.self.query.weight from model.bert.encoder.layer.7.attention.self.query.weight\n","\u001b[32m2024-04-24T02:58:15 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.self.query.bias from model.bert.encoder.layer.7.attention.self.query.bias\n","\u001b[32m2024-04-24T02:58:15 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.self.key.weight from model.bert.encoder.layer.7.attention.self.key.weight\n","\u001b[32m2024-04-24T02:58:15 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.self.key.bias from model.bert.encoder.layer.7.attention.self.key.bias\n","\u001b[32m2024-04-24T02:58:15 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.self.value.weight from model.bert.encoder.layer.7.attention.self.value.weight\n","\u001b[32m2024-04-24T02:58:15 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.self.value.bias from model.bert.encoder.layer.7.attention.self.value.bias\n","\u001b[32m2024-04-24T02:58:15 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.output.dense.weight from model.bert.encoder.layer.7.attention.output.dense.weight\n","\u001b[32m2024-04-24T02:58:15 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.output.dense.bias from model.bert.encoder.layer.7.attention.output.dense.bias\n","\u001b[32m2024-04-24T02:58:15 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.output.LayerNorm.weight from model.bert.encoder.layer.7.attention.output.LayerNorm.weight\n","\u001b[32m2024-04-24T02:58:15 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.output.LayerNorm.bias from model.bert.encoder.layer.7.attention.output.LayerNorm.bias\n","\u001b[32m2024-04-24T02:58:15 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.intermediate.dense.weight from model.bert.encoder.layer.7.intermediate.dense.weight\n","\u001b[32m2024-04-24T02:58:15 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.intermediate.dense.bias from model.bert.encoder.layer.7.intermediate.dense.bias\n","\u001b[32m2024-04-24T02:58:15 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.output.dense.weight from model.bert.encoder.layer.7.output.dense.weight\n","\u001b[32m2024-04-24T02:58:15 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.output.dense.bias from model.bert.encoder.layer.7.output.dense.bias\n","\u001b[32m2024-04-24T02:58:15 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.output.LayerNorm.weight from model.bert.encoder.layer.7.output.LayerNorm.weight\n","\u001b[32m2024-04-24T02:58:15 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.output.LayerNorm.bias from model.bert.encoder.layer.7.output.LayerNorm.bias\n","\u001b[32m2024-04-24T02:58:15 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.self.query.weight from model.bert.encoder.layer.8.attention.self.query.weight\n","\u001b[32m2024-04-24T02:58:15 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.self.query.bias from model.bert.encoder.layer.8.attention.self.query.bias\n","\u001b[32m2024-04-24T02:58:15 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.self.key.weight from model.bert.encoder.layer.8.attention.self.key.weight\n","\u001b[32m2024-04-24T02:58:15 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.self.key.bias from model.bert.encoder.layer.8.attention.self.key.bias\n","\u001b[32m2024-04-24T02:58:15 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.self.value.weight from model.bert.encoder.layer.8.attention.self.value.weight\n","\u001b[32m2024-04-24T02:58:15 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.self.value.bias from model.bert.encoder.layer.8.attention.self.value.bias\n","\u001b[32m2024-04-24T02:58:15 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.output.dense.weight from model.bert.encoder.layer.8.attention.output.dense.weight\n","\u001b[32m2024-04-24T02:58:15 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.output.dense.bias from model.bert.encoder.layer.8.attention.output.dense.bias\n","\u001b[32m2024-04-24T02:58:15 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.output.LayerNorm.weight from model.bert.encoder.layer.8.attention.output.LayerNorm.weight\n","\u001b[32m2024-04-24T02:58:15 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.output.LayerNorm.bias from model.bert.encoder.layer.8.attention.output.LayerNorm.bias\n","\u001b[32m2024-04-24T02:58:15 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.intermediate.dense.weight from model.bert.encoder.layer.8.intermediate.dense.weight\n","\u001b[32m2024-04-24T02:58:15 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.intermediate.dense.bias from model.bert.encoder.layer.8.intermediate.dense.bias\n","\u001b[32m2024-04-24T02:58:15 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.output.dense.weight from model.bert.encoder.layer.8.output.dense.weight\n","\u001b[32m2024-04-24T02:58:15 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.output.dense.bias from model.bert.encoder.layer.8.output.dense.bias\n","\u001b[32m2024-04-24T02:58:15 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.output.LayerNorm.weight from model.bert.encoder.layer.8.output.LayerNorm.weight\n","\u001b[32m2024-04-24T02:58:15 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.output.LayerNorm.bias from model.bert.encoder.layer.8.output.LayerNorm.bias\n","\u001b[32m2024-04-24T02:58:15 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.self.query.weight from model.bert.encoder.layer.9.attention.self.query.weight\n","\u001b[32m2024-04-24T02:58:15 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.self.query.bias from model.bert.encoder.layer.9.attention.self.query.bias\n","\u001b[32m2024-04-24T02:58:15 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.self.key.weight from model.bert.encoder.layer.9.attention.self.key.weight\n","\u001b[32m2024-04-24T02:58:15 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.self.key.bias from model.bert.encoder.layer.9.attention.self.key.bias\n","\u001b[32m2024-04-24T02:58:15 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.self.value.weight from model.bert.encoder.layer.9.attention.self.value.weight\n","\u001b[32m2024-04-24T02:58:15 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.self.value.bias from model.bert.encoder.layer.9.attention.self.value.bias\n","\u001b[32m2024-04-24T02:58:15 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.output.dense.weight from model.bert.encoder.layer.9.attention.output.dense.weight\n","\u001b[32m2024-04-24T02:58:15 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.output.dense.bias from model.bert.encoder.layer.9.attention.output.dense.bias\n","\u001b[32m2024-04-24T02:58:15 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.output.LayerNorm.weight from model.bert.encoder.layer.9.attention.output.LayerNorm.weight\n","\u001b[32m2024-04-24T02:58:15 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.output.LayerNorm.bias from model.bert.encoder.layer.9.attention.output.LayerNorm.bias\n","\u001b[32m2024-04-24T02:58:15 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.intermediate.dense.weight from model.bert.encoder.layer.9.intermediate.dense.weight\n","\u001b[32m2024-04-24T02:58:15 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.intermediate.dense.bias from model.bert.encoder.layer.9.intermediate.dense.bias\n","\u001b[32m2024-04-24T02:58:15 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.output.dense.weight from model.bert.encoder.layer.9.output.dense.weight\n","\u001b[32m2024-04-24T02:58:15 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.output.dense.bias from model.bert.encoder.layer.9.output.dense.bias\n","\u001b[32m2024-04-24T02:58:15 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.output.LayerNorm.weight from model.bert.encoder.layer.9.output.LayerNorm.weight\n","\u001b[32m2024-04-24T02:58:15 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.output.LayerNorm.bias from model.bert.encoder.layer.9.output.LayerNorm.bias\n","\u001b[32m2024-04-24T02:58:15 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.self.query.weight from model.bert.encoder.layer.10.attention.self.query.weight\n","\u001b[32m2024-04-24T02:58:15 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.self.query.bias from model.bert.encoder.layer.10.attention.self.query.bias\n","\u001b[32m2024-04-24T02:58:15 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.self.key.weight from model.bert.encoder.layer.10.attention.self.key.weight\n","\u001b[32m2024-04-24T02:58:15 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.self.key.bias from model.bert.encoder.layer.10.attention.self.key.bias\n","\u001b[32m2024-04-24T02:58:15 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.self.value.weight from model.bert.encoder.layer.10.attention.self.value.weight\n","\u001b[32m2024-04-24T02:58:15 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.self.value.bias from model.bert.encoder.layer.10.attention.self.value.bias\n","\u001b[32m2024-04-24T02:58:15 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.output.dense.weight from model.bert.encoder.layer.10.attention.output.dense.weight\n","\u001b[32m2024-04-24T02:58:15 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.output.dense.bias from model.bert.encoder.layer.10.attention.output.dense.bias\n","\u001b[32m2024-04-24T02:58:15 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.output.LayerNorm.weight from model.bert.encoder.layer.10.attention.output.LayerNorm.weight\n","\u001b[32m2024-04-24T02:58:15 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.output.LayerNorm.bias from model.bert.encoder.layer.10.attention.output.LayerNorm.bias\n","\u001b[32m2024-04-24T02:58:15 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.intermediate.dense.weight from model.bert.encoder.layer.10.intermediate.dense.weight\n","\u001b[32m2024-04-24T02:58:15 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.intermediate.dense.bias from model.bert.encoder.layer.10.intermediate.dense.bias\n","\u001b[32m2024-04-24T02:58:15 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.output.dense.weight from model.bert.encoder.layer.10.output.dense.weight\n","\u001b[32m2024-04-24T02:58:15 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.output.dense.bias from model.bert.encoder.layer.10.output.dense.bias\n","\u001b[32m2024-04-24T02:58:15 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.output.LayerNorm.weight from model.bert.encoder.layer.10.output.LayerNorm.weight\n","\u001b[32m2024-04-24T02:58:15 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.output.LayerNorm.bias from model.bert.encoder.layer.10.output.LayerNorm.bias\n","\u001b[32m2024-04-24T02:58:15 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.self.query.weight from model.bert.encoder.layer.11.attention.self.query.weight\n","\u001b[32m2024-04-24T02:58:15 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.self.query.bias from model.bert.encoder.layer.11.attention.self.query.bias\n","\u001b[32m2024-04-24T02:58:15 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.self.key.weight from model.bert.encoder.layer.11.attention.self.key.weight\n","\u001b[32m2024-04-24T02:58:15 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.self.key.bias from model.bert.encoder.layer.11.attention.self.key.bias\n","\u001b[32m2024-04-24T02:58:15 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.self.value.weight from model.bert.encoder.layer.11.attention.self.value.weight\n","\u001b[32m2024-04-24T02:58:15 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.self.value.bias from model.bert.encoder.layer.11.attention.self.value.bias\n","\u001b[32m2024-04-24T02:58:15 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.output.dense.weight from model.bert.encoder.layer.11.attention.output.dense.weight\n","\u001b[32m2024-04-24T02:58:15 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.output.dense.bias from model.bert.encoder.layer.11.attention.output.dense.bias\n","\u001b[32m2024-04-24T02:58:15 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.output.LayerNorm.weight from model.bert.encoder.layer.11.attention.output.LayerNorm.weight\n","\u001b[32m2024-04-24T02:58:15 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.output.LayerNorm.bias from model.bert.encoder.layer.11.attention.output.LayerNorm.bias\n","\u001b[32m2024-04-24T02:58:15 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.intermediate.dense.weight from model.bert.encoder.layer.11.intermediate.dense.weight\n","\u001b[32m2024-04-24T02:58:15 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.intermediate.dense.bias from model.bert.encoder.layer.11.intermediate.dense.bias\n","\u001b[32m2024-04-24T02:58:15 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.output.dense.weight from model.bert.encoder.layer.11.output.dense.weight\n","\u001b[32m2024-04-24T02:58:15 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.output.dense.bias from model.bert.encoder.layer.11.output.dense.bias\n","\u001b[32m2024-04-24T02:58:15 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.output.LayerNorm.weight from model.bert.encoder.layer.11.output.LayerNorm.weight\n","\u001b[32m2024-04-24T02:58:15 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.output.LayerNorm.bias from model.bert.encoder.layer.11.output.LayerNorm.bias\n","\u001b[32m2024-04-24T02:58:15 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.pooler.dense.weight from model.bert.pooler.dense.weight\n","\u001b[32m2024-04-24T02:58:15 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.pooler.dense.bias from model.bert.pooler.dense.bias\n","\u001b[32m2024-04-24T02:58:15 | mmf.utils.checkpoint: \u001b[0mPretrained model loaded\n","\u001b[32m2024-04-24T02:58:15 | mmf.utils.checkpoint: \u001b[0mCheckpoint loaded.\n","\u001b[32m2024-04-24T02:58:15 | mmf.utils.checkpoint: \u001b[0mCurrent num updates: 0\n","\u001b[32m2024-04-24T02:58:15 | mmf.utils.checkpoint: \u001b[0mCurrent iteration: 0\n","\u001b[32m2024-04-24T02:58:15 | mmf.utils.checkpoint: \u001b[0mCurrent epoch: 0\n","\u001b[32m2024-04-24T02:58:15 | mmf.trainers.mmf_trainer: \u001b[0m===== Model =====\n","\u001b[32m2024-04-24T02:58:15 | mmf.trainers.mmf_trainer: \u001b[0mVisualBERT(\n","  (model): VisualBERTForClassification(\n","    (bert): VisualBERTBase(\n","      (embeddings): BertVisioLinguisticEmbeddings(\n","        (word_embeddings): Embedding(30522, 768, padding_idx=0)\n","        (position_embeddings): Embedding(512, 768)\n","        (token_type_embeddings): Embedding(2, 768)\n","        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","        (token_type_embeddings_visual): Embedding(2, 768)\n","        (position_embeddings_visual): Embedding(512, 768)\n","        (projection): Linear(in_features=2048, out_features=768, bias=True)\n","      )\n","      (encoder): BertEncoderJit(\n","        (layer): ModuleList(\n","          (0-11): 12 x BertLayerJit(\n","            (attention): BertAttentionJit(\n","              (self): BertSelfAttentionJit(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","              (intermediate_act_fn): GELUActivation()\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","        )\n","      )\n","      (pooler): BertPooler(\n","        (dense): Linear(in_features=768, out_features=768, bias=True)\n","        (activation): Tanh()\n","      )\n","    )\n","    (dropout): Dropout(p=0.1, inplace=False)\n","    (classifier): Sequential(\n","      (0): BertPredictionHeadTransform(\n","        (dense): Linear(in_features=768, out_features=768, bias=True)\n","        (transform_act_fn): GELUActivation()\n","        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      )\n","      (1): Linear(in_features=768, out_features=2, bias=True)\n","    )\n","  )\n","  (losses): Losses(\n","    (losses): ModuleList(\n","      (0): MMFLoss(\n","        (loss_criterion): CrossEntropyLoss(\n","          (loss_fn): CrossEntropyLoss()\n","        )\n","      )\n","    )\n","  )\n",")\n","\u001b[32m2024-04-24T02:58:15 | mmf.utils.general: \u001b[0mTotal Parameters: 112044290. Trained Parameters: 112044290\n","\u001b[32m2024-04-24T02:58:15 | mmf.trainers.core.training_loop: \u001b[0mStarting training...\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2024-04-24T02:58:15 | py.warnings: \u001b[0m/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n","  self.pid = os.fork()\n","\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2024-04-24T02:58:15 | py.warnings: \u001b[0m/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n","  self.pid = os.fork()\n","\n","\u001b[32m2024-04-24T03:02:13 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n","\u001b[32m2024-04-24T03:02:13 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2024-04-24T03:02:18 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2024-04-24T03:02:23 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2024-04-24T03:02:23 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 200/1500, train/hateful_memes/cross_entropy: 0.6107, train/hateful_memes/cross_entropy/avg: 0.6107, train/total_loss: 0.6107, train/total_loss/avg: 0.6107, max mem: 14320.0, experiment: run, epoch: 1, num_updates: 200, iterations: 200, max_updates: 1500, lr: 0.00001, ups: 0.81, time: 04m 08s 226ms, time_since_start: 04m 09s 956ms, eta: 28m 06s 077ms\n","\u001b[32m2024-04-24T03:02:23 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n","\u001b[32m2024-04-24T03:02:23 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n","\u001b[32m2024-04-24T03:02:27 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished evaluation inference. Loaded 9\n","\u001b[32m2024-04-24T03:02:27 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n","\u001b[32m2024-04-24T03:02:27 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2024-04-24T03:02:30 | mmf.utils.checkpoint: \u001b[0mSaving best checkpoint\n","\u001b[32m2024-04-24T03:02:35 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2024-04-24T03:02:38 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2024-04-24T03:02:38 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 200/1500, val/hateful_memes/cross_entropy: 0.6815, val/total_loss: 0.6815, val/hateful_memes/accuracy: 0.6352, val/hateful_memes/binary_f1: 0.2335, val/hateful_memes/roc_auc: 0.5490, num_updates: 200, epoch: 1, iterations: 200, max_updates: 1500, val_time: 15s 323ms, best_update: 200, best_iteration: 200, best_val/hateful_memes/roc_auc: 0.548956\n","\u001b[32m2024-04-24T03:06:36 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n","\u001b[32m2024-04-24T03:06:36 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2024-04-24T03:06:39 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2024-04-24T03:06:43 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2024-04-24T03:06:43 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 400/1500, train/hateful_memes/cross_entropy: 0.4877, train/hateful_memes/cross_entropy/avg: 0.5492, train/total_loss: 0.4877, train/total_loss/avg: 0.5492, max mem: 14369.0, experiment: run, epoch: 2, num_updates: 400, iterations: 400, max_updates: 1500, lr: 0.00001, ups: 0.82, time: 04m 04s 958ms, time_since_start: 08m 30s 242ms, eta: 23m 27s 896ms\n","\u001b[32m2024-04-24T03:06:43 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n","\u001b[32m2024-04-24T03:06:43 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2024-04-24T03:06:43 | py.warnings: \u001b[0m/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n","  self.pid = os.fork()\n","\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2024-04-24T03:06:43 | py.warnings: \u001b[0m/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n","  self.pid = os.fork()\n","\n","\u001b[32m2024-04-24T03:06:47 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished evaluation inference. Loaded 9\n","\u001b[32m2024-04-24T03:06:47 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n","\u001b[32m2024-04-24T03:06:48 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2024-04-24T03:06:51 | mmf.utils.checkpoint: \u001b[0mSaving best checkpoint\n","\u001b[32m2024-04-24T03:06:58 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2024-04-24T03:07:01 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2024-04-24T03:07:01 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 400/1500, val/hateful_memes/cross_entropy: 0.7311, val/total_loss: 0.7311, val/hateful_memes/accuracy: 0.6241, val/hateful_memes/binary_f1: 0.2454, val/hateful_memes/roc_auc: 0.6031, num_updates: 400, epoch: 2, iterations: 400, max_updates: 1500, val_time: 18s 213ms, best_update: 400, best_iteration: 400, best_val/hateful_memes/roc_auc: 0.603118\n","\u001b[32m2024-04-24T03:10:58 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n","\u001b[32m2024-04-24T03:10:58 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2024-04-24T03:11:01 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2024-04-24T03:11:06 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2024-04-24T03:11:06 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 600/1500, train/hateful_memes/cross_entropy: 0.4877, train/hateful_memes/cross_entropy/avg: 0.4618, train/total_loss: 0.4877, train/total_loss/avg: 0.4618, max mem: 14369.0, experiment: run, epoch: 3, num_updates: 600, iterations: 600, max_updates: 1500, lr: 0.00002, ups: 0.82, time: 04m 04s 352ms, time_since_start: 12m 52s 810ms, eta: 19m 09s 068ms\n","\u001b[32m2024-04-24T03:11:06 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n","\u001b[32m2024-04-24T03:11:06 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2024-04-24T03:11:06 | py.warnings: \u001b[0m/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n","  self.pid = os.fork()\n","\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2024-04-24T03:11:06 | py.warnings: \u001b[0m/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n","  self.pid = os.fork()\n","\n","\u001b[32m2024-04-24T03:11:10 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished evaluation inference. Loaded 9\n","\u001b[32m2024-04-24T03:11:10 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n","\u001b[32m2024-04-24T03:11:10 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2024-04-24T03:11:13 | mmf.utils.checkpoint: \u001b[0mSaving best checkpoint\n","\u001b[32m2024-04-24T03:11:17 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2024-04-24T03:11:21 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2024-04-24T03:11:21 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 600/1500, val/hateful_memes/cross_entropy: 0.9132, val/total_loss: 0.9132, val/hateful_memes/accuracy: 0.6500, val/hateful_memes/binary_f1: 0.3368, val/hateful_memes/roc_auc: 0.6193, num_updates: 600, epoch: 3, iterations: 600, max_updates: 1500, val_time: 15s 431ms, best_update: 600, best_iteration: 600, best_val/hateful_memes/roc_auc: 0.619338\n","\u001b[32m2024-04-24T03:15:17 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n","\u001b[32m2024-04-24T03:15:17 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2024-04-24T03:15:20 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2024-04-24T03:15:24 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2024-04-24T03:15:24 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 800/1500, train/hateful_memes/cross_entropy: 0.2869, train/hateful_memes/cross_entropy/avg: 0.4073, train/total_loss: 0.2869, train/total_loss/avg: 0.4073, max mem: 14369.0, experiment: run, epoch: 3, num_updates: 800, iterations: 800, max_updates: 1500, lr: 0.00002, ups: 0.82, time: 04m 03s 118ms, time_since_start: 17m 11s 362ms, eta: 14m 49s 205ms\n","\u001b[32m2024-04-24T03:15:24 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n","\u001b[32m2024-04-24T03:15:24 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2024-04-24T03:15:24 | py.warnings: \u001b[0m/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n","  self.pid = os.fork()\n","\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2024-04-24T03:15:24 | py.warnings: \u001b[0m/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n","  self.pid = os.fork()\n","\n","\u001b[32m2024-04-24T03:15:28 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished evaluation inference. Loaded 9\n","\u001b[32m2024-04-24T03:15:28 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n","\u001b[32m2024-04-24T03:15:29 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2024-04-24T03:15:37 | mmf.utils.checkpoint: \u001b[0mSaving best checkpoint\n","\u001b[32m2024-04-24T03:15:46 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2024-04-24T03:15:54 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2024-04-24T03:15:54 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 800/1500, val/hateful_memes/cross_entropy: 1.0229, val/total_loss: 1.0229, val/hateful_memes/accuracy: 0.6130, val/hateful_memes/binary_f1: 0.3148, val/hateful_memes/roc_auc: 0.6311, num_updates: 800, epoch: 3, iterations: 800, max_updates: 1500, val_time: 29s 426ms, best_update: 800, best_iteration: 800, best_val/hateful_memes/roc_auc: 0.631059\n","\u001b[32m2024-04-24T03:19:51 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n","\u001b[32m2024-04-24T03:19:51 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2024-04-24T03:19:54 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2024-04-24T03:19:59 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2024-04-24T03:19:59 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1000/1500, train/hateful_memes/cross_entropy: 0.2869, train/hateful_memes/cross_entropy/avg: 0.3768, train/total_loss: 0.2869, train/total_loss/avg: 0.3768, max mem: 14369.0, experiment: run, epoch: 4, num_updates: 1000, iterations: 1000, max_updates: 1500, lr: 0.00003, ups: 0.82, time: 04m 04s 844ms, time_since_start: 21m 45s 635ms, eta: 10m 39s 656ms\n","\u001b[32m2024-04-24T03:19:59 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n","\u001b[32m2024-04-24T03:19:59 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2024-04-24T03:19:59 | py.warnings: \u001b[0m/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n","  self.pid = os.fork()\n","\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2024-04-24T03:19:59 | py.warnings: \u001b[0m/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n","  self.pid = os.fork()\n","\n","\u001b[32m2024-04-24T03:20:03 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished evaluation inference. Loaded 9\n","\u001b[32m2024-04-24T03:20:03 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n","\u001b[32m2024-04-24T03:20:03 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2024-04-24T03:20:07 | mmf.utils.checkpoint: \u001b[0mSaving best checkpoint\n","\u001b[32m2024-04-24T03:20:10 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2024-04-24T03:20:20 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2024-04-24T03:20:20 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1000/1500, val/hateful_memes/cross_entropy: 1.3573, val/total_loss: 1.3573, val/hateful_memes/accuracy: 0.6278, val/hateful_memes/binary_f1: 0.3777, val/hateful_memes/roc_auc: 0.6359, num_updates: 1000, epoch: 4, iterations: 1000, max_updates: 1500, val_time: 21s 265ms, best_update: 1000, best_iteration: 1000, best_val/hateful_memes/roc_auc: 0.635853\n","\u001b[32m2024-04-24T03:24:18 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n","\u001b[32m2024-04-24T03:24:18 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2024-04-24T03:24:21 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2024-04-24T03:24:25 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2024-04-24T03:24:25 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1200/1500, train/hateful_memes/cross_entropy: 0.2548, train/hateful_memes/cross_entropy/avg: 0.3374, train/total_loss: 0.2548, train/total_loss/avg: 0.3374, max mem: 14369.0, experiment: run, epoch: 5, num_updates: 1200, iterations: 1200, max_updates: 1500, lr: 0.00003, ups: 0.82, time: 04m 04s 999ms, time_since_start: 26m 11s 903ms, eta: 06m 24s 037ms\n","\u001b[32m2024-04-24T03:24:25 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n","\u001b[32m2024-04-24T03:24:25 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2024-04-24T03:24:25 | py.warnings: \u001b[0m/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n","  self.pid = os.fork()\n","\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2024-04-24T03:24:25 | py.warnings: \u001b[0m/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n","  self.pid = os.fork()\n","\n","\u001b[32m2024-04-24T03:24:29 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished evaluation inference. Loaded 9\n","\u001b[32m2024-04-24T03:24:29 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n","\u001b[32m2024-04-24T03:24:29 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2024-04-24T03:24:32 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2024-04-24T03:24:36 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2024-04-24T03:24:36 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1200/1500, val/hateful_memes/cross_entropy: 1.4839, val/total_loss: 1.4839, val/hateful_memes/accuracy: 0.6241, val/hateful_memes/binary_f1: 0.4314, val/hateful_memes/roc_auc: 0.6272, num_updates: 1200, epoch: 5, iterations: 1200, max_updates: 1500, val_time: 11s 532ms, best_update: 1000, best_iteration: 1000, best_val/hateful_memes/roc_auc: 0.635853\n","\u001b[32m2024-04-24T03:28:34 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n","\u001b[32m2024-04-24T03:28:34 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2024-04-24T03:28:38 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2024-04-24T03:28:47 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2024-04-24T03:28:47 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1400/1500, train/hateful_memes/cross_entropy: 0.2548, train/hateful_memes/cross_entropy/avg: 0.2935, train/total_loss: 0.2548, train/total_loss/avg: 0.2935, max mem: 14369.0, experiment: run, epoch: 6, num_updates: 1400, iterations: 1400, max_updates: 1500, lr: 0.00003, ups: 0.80, time: 04m 10s 291ms, time_since_start: 30m 33s 730ms, eta: 02m 10s 777ms\n","\u001b[32m2024-04-24T03:28:47 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n","\u001b[32m2024-04-24T03:28:47 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2024-04-24T03:28:47 | py.warnings: \u001b[0m/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n","  self.pid = os.fork()\n","\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2024-04-24T03:28:47 | py.warnings: \u001b[0m/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n","  self.pid = os.fork()\n","\n","\u001b[32m2024-04-24T03:28:51 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished evaluation inference. Loaded 9\n","\u001b[32m2024-04-24T03:28:51 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n","\u001b[32m2024-04-24T03:28:51 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2024-04-24T03:28:54 | mmf.utils.checkpoint: \u001b[0mSaving best checkpoint\n","\u001b[32m2024-04-24T03:28:58 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2024-04-24T03:29:02 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2024-04-24T03:29:02 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1400/1500, val/hateful_memes/cross_entropy: 1.6492, val/total_loss: 1.6492, val/hateful_memes/accuracy: 0.6389, val/hateful_memes/binary_f1: 0.4380, val/hateful_memes/roc_auc: 0.6431, num_updates: 1400, epoch: 6, iterations: 1400, max_updates: 1500, val_time: 15s 255ms, best_update: 1400, best_iteration: 1400, best_val/hateful_memes/roc_auc: 0.643070\n","\u001b[32m2024-04-24T03:31:02 | mmf.trainers.core.training_loop: \u001b[0mStepping into final validation check\n","\u001b[32m2024-04-24T03:31:02 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2024-04-24T03:31:02 | py.warnings: \u001b[0m/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n","  self.pid = os.fork()\n","\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2024-04-24T03:31:02 | py.warnings: \u001b[0m/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n","  self.pid = os.fork()\n","\n","\u001b[32m2024-04-24T03:31:06 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished evaluation inference. Loaded 9\n","\u001b[32m2024-04-24T03:31:06 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n","\u001b[32m2024-04-24T03:31:06 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1500/1500, val/hateful_memes/cross_entropy: 1.4570, val/total_loss: 1.4570, val/hateful_memes/accuracy: 0.6574, val/hateful_memes/binary_f1: 0.4729, val/hateful_memes/roc_auc: 0.6408, num_updates: 1500, epoch: 6, iterations: 1500, max_updates: 1500, val_time: 02m 19s 638ms, best_update: 1400, best_iteration: 1400, best_val/hateful_memes/roc_auc: 0.643070\n","\u001b[32m2024-04-24T03:31:06 | mmf.utils.checkpoint: \u001b[0mRestoring checkpoint\n","\u001b[32m2024-04-24T03:31:06 | mmf.utils.checkpoint: \u001b[0mLoading checkpoint\n","\u001b[32m2024-04-24T03:31:07 | mmf.utils.checkpoint: \u001b[0mCheckpoint loaded.\n","\u001b[32m2024-04-24T03:31:07 | mmf.utils.checkpoint: \u001b[0mCurrent num updates: 1400\n","\u001b[32m2024-04-24T03:31:07 | mmf.utils.checkpoint: \u001b[0mCurrent iteration: 1400\n","\u001b[32m2024-04-24T03:31:07 | mmf.utils.checkpoint: \u001b[0mCurrent epoch: 6\n","\u001b[32m2024-04-24T03:31:08 | mmf.trainers.mmf_trainer: \u001b[0mStarting inference on val set\n","\u001b[32m2024-04-24T03:31:08 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n","  0% 0/9 [00:00<?, ?it/s]\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2024-04-24T03:31:08 | py.warnings: \u001b[0m/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n","  self.pid = os.fork()\n","\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2024-04-24T03:31:08 | py.warnings: \u001b[0m/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n","  self.pid = os.fork()\n","\n","100% 9/9 [00:04<00:00,  2.08it/s]\n","\u001b[32m2024-04-24T03:31:13 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished evaluation inference. Loaded 9\n","\u001b[32m2024-04-24T03:31:13 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n","\u001b[32m2024-04-24T03:31:13 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1400/1500, val/hateful_memes/cross_entropy: 1.6492, val/total_loss: 1.6492, val/hateful_memes/accuracy: 0.6389, val/hateful_memes/binary_f1: 0.4380, val/hateful_memes/roc_auc: 0.6431\n","\u001b[32m2024-04-24T03:31:13 | mmf.trainers.callbacks.logistics: \u001b[0mFinished run in 32m 59s 896ms\n"]}]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"L4","machine_shape":"hm","provenance":[],"collapsed_sections":["w_ydFyhUYFwz","J3fUue7SNwen"],"authorship_tag":"ABX9TyMoc+bWyf3Bnvut1hem2O4f"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}